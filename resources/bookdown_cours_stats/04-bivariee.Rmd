# Analyse Bivariée {#bivariee}


L'analyse bivariée , comme son nom l'indique, a pour objectif d'analyser le lien qui peut exister entre **deux** variables.


En guise de rappel sur les types de variables (cf section \@ref(vocabulaire)) , donnons des exemples: 

Pour deux variables quantitatives , on pourrait analyser le lien entre : 

- le nombre d'habitants et nombre de lignes de bus des départements français
- le nombre de lignes de bus en Isère en 1998 et en 2018  


Pour deux variables qualitatives, on pourrait analyser le lien entre : 

- la couleur des yeux  et le fait de porter des lunettes
- les catégories de séries télé et la plate-forme  où ils sont disponibles 


Enfin, pour une variable quantitative et une variable qualitative :

- le lien entre la taille et la couleur des yeux, 
- le lien entre  le nombre d'aces et le côté du cours de tennis



## Corrélation n'implique pas causalité 

<center>
 <span style="color:red; font-size:1.5em"> &#9888; Une liaison, même très forte, entre deux variables, n'indique pas la causalité! &#9888;</span>
</center>


<br>
Cette erreur est très courante, très tentante. 


De nombreux contre-exemples sont heureusement disponibles pour finir de se convaincre que la **corrélation n'implqiue pas la causalité**: 


<img src="chart.svg"/>

© TylerVigen http://tylervigen.com/spurious-correlations
D'autres sont disponibles à cette adresse.



## Analyse bivariée avec des données spatiales


### Données spatiales
  
L'analyse bivariée que nous allons aborder dans ce chapitre concerne des variables traditionnelles, i.e. **pas des variables de localisation** . Si elles proviennent de données spatiales, ce sera : 

- soit des individus restreints spatialement (sélection spatiale)
- soit des variables "géographiques" (e.g. lieu de résidence) renseignées pour les individus

La localisation **en tant que variable** n'interviendra **pas** directement dans nos analyses.
  

### Ressources pour données localisées et distances 

Il existe des outils statistiques pour analyser le lien qui existe entre des variables localisées dans l'espace *et leur localisation elle-même*. Ces techniques **ne sont pas au programme de ce cours**, je les mentionne pour les curieux. 


On peut ainsi mesurer l' **auto-corrélation spatiale** , qui indique si les valeurs proches sont regroupées ou au contraire disséminées dans l'espace, à l'aide de l'[indice de Moran  global](https://en.wikipedia.org/wiki/Moran%27s_I), et identifier des clusters de valeurs plus fortes ou plus faibles que la normale à l'aide de l'[indice de Moran local](https://geodacenter.github.io/workbook/6a_local_auto/lab6a.html).


On peut également effectuer des  régressions qui tiennent  compte de la localisation des observations dans l'espace, qu'on appelle GWR pour **Geographicaly Weighted Regression** (plus de détails [sur wikipedia](https://fr.wikipedia.org/wiki/R%C3%A9gression_g%C3%A9ographiquement_pond%C3%A9r%C3%A9e) et la [fiche de l'INSEE](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjxtrK85pvuAhX2DmMBHdDaCTwQFjAAegQIAxAC&url=https%3A%2F%2Fwww.insee.fr%2Fen%2Fstatistiques%2Ffichier%2F3635545%2Fimet131-m-chapitre-9.pdf&usg=AOvVaw0Mli34vfm-BI1PymvssD5Y)) 


Enfin , si on désire prendre en compte les **distances** dans les flux entre unités spatiales (par exemple pour expliquer les déplacements domicile-travail dans une région) on peut se tourner vers les [modèles gravitaires](https://www.hypergeo.eu/spip.php?article76) 




## Régression linéaire 



### Avant toute chose  


Vous devriez commencer à avoir l'habitude de ce mantra, encore plus valable dans le cas d'une analyse bivariée : 

<span style="color:red; font-size: 1.5em;">Toujours en premier: Regarder l'aspect des données avec des graphiques </span> 



### Diverses formes de dépendances 


Ce qu'on appelle **lien** ou **liaison** ou encore **dépendance** entre les variables expriment le fait que les valeurs de deux variables n'évoluent pas indépendamment mais au contraire présentent une certaine **forme** une certaine **régularité**.

Ces «régularités» peuvent être de plusieurs formes, en voici quelques unes : 


```{r formes, echo=FALSE, fig.width= 8, fig.height=5}
x <- runif(100, min = 0, max=10)
y1 <- 8.5*x - 23 + rnorm(100,0, 12)
y2 <- -16*x +12 + rnorm(100,0,10)
y3 <- sqrt(exp(x)) + rnorm(100,0,8)
y4 <-  50*sin(x) + rnorm(100,0,10)
y5 <-  rnorm(100, 0, 60)
y6 <- 21.345
data2 <-  data.frame(x, y1, y2, y3, y4, y5, y6)
library(reshape2)
library(ggplot2)
data2 <- melt(data2, idvars=c("x"), measure.vars=c("y1","y2","y3","y4", "y5", "y6"))
plo2 <-  ggplot(data2, aes(x=x, y=value))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  facet_wrap(~variable)+
  xlab("value of x variable")+
  ylab("value of yi variable")
  
plo2
```

Cette matrice de graphes, de gauche à droite et de haut en bas montre :

- une dépendance linéaire positive
- une dépendance linéaire négative
- une dépendance non-linéaire , peut-être exponentielle
- une dépendance périodique, sinusoïdale
- une absence de dépendance, les deux variables sont indépendantes
- une absence de dépendance, la variable de l'axe des $y$ est constante


En pratique les formes sont beaucoup moins régulières que ces exemples très «mathématiquement» simples


### Les étapes de l'analyse bivariée 

1. Tracer le nuage de points
2. Existe-t-il une relation ? 
3. Est-elle de forme linéaire ?  De quel sens ?
4. Si la liaison est de forme linéaire $\rightarrow$ faire une **régression** 
5. Si la liaison est non linéaire, est-elle monotone ?  De forme connue ?$\rightarrow$  proposer un **modèle**, i.e. une équation qui décrive la forme de la dépendance entre les deux variables.
6. (5bis)Réaliser un modèle **LOESS** avec prudence (uniquement descriptif , aucun pouvoir de généralisation) 
cf le blog de Lise Vaudor [http://perso.ens-lyon.fr/lise.vaudor/regression-loess/]


Voici un arbre de décision plus précis : 


<img src="decisionTreeCorr.svg"/>





### Régression linéaire 

#### Principe
 
Si la forme du nuage de points s'y prête, c'est-à-dire bien allongée, rectiligne , on peut entreprendre une **régression linéaire** (aussi appelé  **ajustement** linéaire).
 
Cela consiste à trouver la droite qui passe «**au mieux**» dans le nuage de points de deux variables quantitatives $V_1$ et $V_2$.

«**au mieux**» est ici employé au sens des **moindres carrés**, c'est-à-dire que parmi toutes les droites qui peuvent passer au milieu du nuage de points, on va choisir celle pour laquelle l'erreur commise est la plus faible. 

L'erreur commise par la droite, c'est la somme des écarts entre les points du nuage et la droite, élevés au carré pour que les erreurs positives et négatives ne se compensent pas.  

On cherche la droite (en fait on cherche les deux coefficients $a$ et $b$ de son équation $y=ax+b$) qui minimise la somme des carrés des écarts, d'où son nom de MCO (Moindres Carrés Ordinaires) ou OLS (Ordinary Least Squares in english).


Si le nuage de points n'est pas amorphe, sans forme, et que les deux variables ne sont pas totalement indépendantes, ou constantes, alors cette droite est unique.  


#### La droite de régression

Cette droite, qu'on appelle **droite de régression** permet de visualiser: 

* l'**intensité** de la dépendance, suivant que les points sont proches de la droite ou non
* la **forme** de la dépendance, suivant que le nuage soit bien de forme linéaire
* le **sens** de la dépendance : nulle, positive ou négative 


```{r regline, echo=FALSE, fig.width=6, fig.height=4, message=F}
library(ggplot2)
data <-  data.frame(X=c(1,2,3,5,6,7,12,14,22, 2,3 ,10,10,9,7),Y=c(15,16,13, 9,9,12,3,2,5,11,12,7,8,9,8))
plo <-  ggplot(data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  xlab("Var1")+
  ylab("Var2")
plo
```

 
 
### Le modèle linéaire 
 

L'**équation** de la droite  est un **modèle linéaire** de la relation statistique qui lie $V_1$ et $V_2$; 

Ici le modèle est :  $\hat{V_2}=aV_1+b$



Si la régression linéaire est réussie, alors pour un individu $i$ dont on connait $V1_i$, on infère la valeur $V2_i$ par le modèle :    $\hat{V_{2i}} = aV_{1i} +b$
 
On dit aussi que $V_1$ **explique** $V_2$ , ou que le modèle **prédit** $V_2$ à partir de $V_1$ (on note les valeurs prédites $\hat{V_2}$)
 
###  "validité" du modèle linaire 

En pratique, il faut réunir deux critères : 

- des coefficients avec des **p-values** associées **faibles** (e.g. <0.05)  $\leftrightarrow$ "on a peu de chances de se tromper"
- un $R^2$ élevé  $\leftrightarrow$ "le modèle prédit bien les observations"
  
 
## le $R^2$
 
 $R^2 \in [0;1]$ , c'est le  **coefficient de détermination linéaire**.

Donne la **qualité de prédiction** de la régression. 

"Proche de 1"" $\equiv$ "très bonne qualité"
 
C'est le pourcentage de "variation" de $V_2$ due à la "variation" de $V_1$

## le $R^2$

défini par : 
 $R^2  = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$


si on note $\hat{V_2}$ les valeurs de $V_2$ prédite par le modèle linéaire, alors:

 $R^2=cor(\hat{V_2},V_2)^2$   

(au sens de la corrélation de Pearson)


## la p-value

Elle peut s'interpréter comme «la probabilité d'avoir un résultat de regression identique avec deux variables véritablement indépendantes» 

<br>
<br>

La p-value est associée à la notion d'**hypothèses nulle**. 
Ici , l'hypothèse nulle est "les deux séries sont indépendantes". 


## la p-value 
 
Plus grossièrement : la p-value est le  **pourcentage de chances de se tromper** en rejetant l'hypothèse nulle, 

<br> c'est à dire <br>

se tromper en considérant que  les deux séries ne sont pas indépendantes et qu'il existe une relation entre les deux (ici, linéaire car nous testons un modèle linéaire).


## l'Hypothèse nulle 

$H_0$ : «les deux variables sont indépendantes»


  - **conserver** $H_0$ : considérer les deux variables comme **indépendantes**
  - **rejeter** $H_0$ : considérer les deux variables comme **dépendantes** = ayant une relation statistique, un lien.



## Format des resultats donnés par avec R

```{r reslm,fig.width=6, fig.height=4}
regression <- lm(iris$Petal.Length~iris$Petal.Width)
summary(regression)
```

Distribution des résidus, coefficients du modèle ajusté et leur **p-value** associée (ici sur un test de Student, notée `Pr(>|t|)` et  $R^2$) 




## Bonus: Critères de significativité du lien linéaire 

Les **résidus** $\epsilon_i$ (écart entre valeur observée et valeur prédite ($V_2 - \hat{V_2}$) par le modèle pour l'individu $i$) doivent:

- être indépendants  : covariance nulle ou très faible $cov(x_i, \epsilon_i) = 0$
- être distribués selon un loi normale de moyenne nulle $\epsilon \sim \mathscr{N}(0,\sigma_{\epsilon})$
- être distribués de façon homogène (homoscédasticité), i.e. de variance constante $var(\epsilon_i)=\sigma_{\epsilon}^2$ , indépendante de l'observation



## Bonus: Evaluation de l'indépendance des résidus avec R 

Graphique de la fonction `acf`:  

Si une barre exceptée la première dépasse  la ligne en pointillés, on peut remettre en cause l'indépendance des résidus. Ici, c'est le cas.

```{r residualSigni3,fig.width=6, fig.height=4}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
acf(residuals(modele1))
```



 


## Bonus:Les 4 graphiques résultats de la fonction `lm`


La fonction `lm` de R et ses résultats permettent de tracer 4 graphiques pour évaluer certains des critères de significativité.


```{r residualSignif,fig.width=6, fig.height=4, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
par(mfrow=c(2,2)) # pour avoir une matrice de graphes
plot(modele1)
```



## Bonus: Évaluer l'homogénéité des résidus avec R

Premier graphique : pour vérifier que le nuage de points est homogène (e.g. pas de relation non-linéaire entre résidus et valeurs prédites)

Une éventuelle relation non-linéaire pourrait se retrouver dans les résidus(Ici : légère structure parabolique )

```{r residualSignif1,fig.width=6, fig.height=3, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,1)
```


## Bonus: Evaluer la normalité de la distribution des résidus avec R

Le deuxième graphique "Q-Q plot" : pour vérifier l'**hypothèse de normalité des résidus**, les points doivent être proches de la bissectrice  


```{r residualSignif2,fig.width=6, fig.height=4, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,2)
```




## Bonus: Evaluer l'homoscédasticité des résidus 

<span style="font-size:18px;"> Le troisème graphique "Scale location" : si les résidus sont distribués de façon homogène suivant les valeurs "fittées",  alors la droite est plutôt horizontale et les points sont disposés de façon homogène autour.</span>

```{r residualSignif3,fig.width=6, fig.height=2.5, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,3)
```

<span style="font-size:18px;">Ici: légère pente mais les points sont distribués de façon relativement homogène autour de la droite.</span>



# Corrélation de deux variables quantitatives 

## Corrélation (linéaire)

Dans le cas d'une liaison statistique linéaire entre deux variables, on peut calculer l'«intensité» de ce lien sans nécessairement trouver les coefficients modèle linéaire : c'est la  **corrélation**

$cor(x,y) \in [-1;1]$ entre deux variables $x$ et $y$ .

- +1 : les deux variables croissent ou décroissent conjointement
- -1 : quand l'une des variables croît, l'autre décroît.
- 0 : pas de relation **linéaire** entre les deux variables
 
 <font size="3">R donne le coefficient de Pearson par defaut, l'argument `method` de la fonction `cor()` permet de spécifier deux autres coefficients : Kendall et Spearman.</font>


## Test de corrélation entre deux variables avec R

Version plus complète : c'est un **test**, on a plusieurs indicateurs statistiques sur ce test, notamment la **p-value** et **l'intervalle de confiance**

```{r cortestIris}
cor.test(iris$Petal.Length, iris$Petal.Width)
```
Rappel : la p-value quantifie la **significativité** du test.
En général, on considère le test significatif si elle est en dessous de 5%, soit 0.05.  


## Calcul direct du coefficient de corrélation

Soient deux variables $V_1$ et $V_2$

Le coefficient de corrélation $r$ de $V_1$ et $V_2$ est la normalisation de la covariance par le produit des écart-types des variables 

$r= \frac{cov(V_1,V_2)}{\sigma_{V_1}\sigma_{V_2}}$ 




La covariance est la **moyenne du produit des écarts à la moyenne**

$cov(V_1,V_2)= E[(V_1-E[V_1])(V_2-E[V_2])]$





## Matrice de corrélations


```{r corMatIris}
cor(iris[,1:4])
```

Présentation des corrélations entre les variables quantitatives d'un tableau, pour tous les couples de variables.

La matrice de corrélation est symétrique, et sa diagonale est constituée de 1.




## Sensibilité aux 'outliers'

```{r sensitiveOut1}
X <-  c(3,2,3,4,1,2,3,4,5,2,3,4,3)
Y <-  c(1,2,2,2,3,3,3,3,3,4,4,4,5)
plot(X, Y, xlim = c(0,16), ylim= c(0,16))
cor.test(X,Y)$estimate
```


## Sensibilité aux 'outliers'

```{r sensitiveOut2}
X <-  c(3,2,3,4,1,2,3,4,5,2,3,4,3,15)
Y <-  c(1,2,2,2,3,3,3,3,3,4,4,4,5,15)
plot(X, Y, xlim = c(0,16), ylim= c(0,16))
cor.test(X,Y)$estimate
```

## Sensibilité aux 'outliers'

Outlier : observation "*anormale*",  par sa valeur extrème , comparée aux autres.


La corrélation et la régression linéaire sont très sensibles aux outliers.

$\rightarrow$ s'interroger sur la nécessité de nettoyer/filter les données et des conséquences 



# Régression linéaire avec R

## Regression linéaire avec R

Fonction  `lm()` , modèle de la forme `Variable_a_expliquer ~ Variable_explicative` 

```{r regress, echo=TRUE }
my_model <- lm(Petal.Width~Petal.Length, data=iris)
summary(my_model)
```


## Que faire lorsque la relation n'est pas linéaire ?

Quand les deux variables sembles corrélées , de façon **monotone** mais **non linéaire**,

$\rightarrow$ Coefficient de **Spearman**, basé sur le **rang** des individus.

<center>
$\rho = 1 - \frac{6\sum_{i=1}^{n}(rg(X_i)-rg(Y_i))^2 }{n^3 -n}$
</center>


avec :

$rg(X_i)$ le *rang* de $X_i$  (le classement de sa valeur) dans la distribution de $X$

$n$ le nombre d'individus

## Obtenir le coefficient  de Spearman avec R


```{r rho}
cor.test(iris$Sepal.Length, iris$Sepal.Width, method="spearman", exact = FALSE)
```


l'argument `exact` doit être précisé en cas de valeurs ex aequo dans les données.  


## Utilisation conjointe des coefficients de Pearson et Spearman 

$r$ (Pearson) et $\rho$ (Spearman) sont deux moyens d'estimer la corrélation: lequel choisir ?

si $r = \rho$: on garde $r$ (plus simple à interpréter)

si $r < \rho$: la relation est non-linéaire : prendre  $\rho$

si $r > \rho$: il y a un biais, prendre $\rho$ (plus robuste)

... et toujours **tracer le nuage de points** pour examiner  la nature de la relation.


# "trucs" pour linéariser des relations non-linéaires


## Relation log-linéaire 

```{r loglin, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- 1.5*x^2.2  + rnorm(1000,10, 12)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```


Relation de type $y=ax^b$ 

se linéarise par $ln(y)=aln(x) + ln(b)$



## Relation géométrique (exponentielle) 

```{r expolin, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- exp(1.5*x+ 0.2)  + 10000*rnorm(1000,10, 12)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```


Relation de type $y=e^{ax+b}$ 

se linéarise par $ln(y)=ax + ln(b)$

## Relation logarithmique

```{r logar, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- 1.5*log(x)+ 0.2  + 0.22*rnorm(1000,1, 2)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```

Relation de type $y=a*ln(x)+b$ 

$\rightarrow$ changement de variable 

## Relation logistique

```{r logistic, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
ymax <-  10
ymin <- 0
y1 <- ymin+ ((ymax -ymin) /(1 + exp(-1.5*x+6.8))) + 0.5*rnorm(1000,1,1)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```

Relation de type $y= y_{min} * \frac{y_{max}-y_{min}}{1+e^{ax+b}}$

se linéarise par $ln\bigg(\frac{y_{max}-y}{y-y_{min}}\bigg)=ax+b$

# Lien entre deux variables qualitatives

## Représentation graphique

Pour deux variables qualitatives, on ne peut pas produire de nuages de points, ni de droite de régression.

$\rightarrow$ on peut représenter la table de contingence (cf. fonction `mosaicplot` de R).

```{r mosaic, echo=FALSE, fig.height=3}
library(RColorBrewer)
mypalette <-  brewer.pal(12,"Set3")
par(mar=c(0,0,0,0))
mosaicplot(~Class+Survived, data=Titanic, color=mypalette,main = "")
``` 


## Test statistique dit du "Chi 2" ou "Chi carré"

Le test du $\chi ^2$ est un test d'indépendance, il mesure l'**écart**, la différence,  entre deux distributions de **variables qualitatives**  

Il répond à la question : "Existe-t-il un lien statistique entre deux séries de valeurs qualitatives"  

(La réponse est de type  OUI/NON , le $\chi^2$ ne donne pas l'**intensité** du lien)


## Test statistique dit du "Chi 2" ou "Chi carré"

- Hypothèse nulle $H_0$ : les deux distributions sont indépendantes.
- «faire le test» permet de conserver ou de rejeter cette hypothèse

## Principe du Chi 2

- On génère une **population théorique** à laquelle on va comparer la **population observée** en considérant leurs **distribution**.
- Cette distribution théorique reflête ce qui se passerait si on suppose que $H_0$ est vraie 
- Avec cette comparaison,  on pourra rejeter ou conserver l'hypothèse nulle.

La construction de cette distribution se fait à partir du **tableau de contingence**


##  Tableau de contingence

C'est un tableau à double entrée qui croise deux **variables qualitatives**. 


Dans une case on trouve l'**effectif** (= le nombre) des individus caractérisés par la conjonction des modalités en ligne et en colonnes.


Exemple sur des formes géométriques de couleurs :

$$\begin{array}{c|c|c}
   & blanc & noir \\ 
   \hline
carré &  22 &   12 \\ 
  \hline
  rond &   10 &  30 \\ 
    \hline
  triangle &  26 &   5 \\ 
\end{array}$$


Dans R : fonction `table()`

## Construction de la distribution théorique.

On commence par sommer les effectifs selon les modalités (en ligne et en colonne)

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  22 &   12 & 34\\ 
  \hline
  rond &   10 &  30 &  40  \\ 
    \hline
  triangle &  26 &   5 &  31\\
  \hline
  \texttt{total} & 58 & 47 & 105
  \end{array}$$

On appelle les sommes en lignes et en colonnes **sommes marginales**, elles sont mises dans les "marges" du tableau.



## Construction de la distribution théorique

En divisant par la taille de la population, on obtient les **fréquences observées**.

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.20952381&  0.11428571 &  0.3238095\\ 
  \hline
  rond &    0.09523810 & 0.28571429 & 0.3809524  \\ 
    \hline
  triangle & 0.24761905 & 0.04761905 &   0.2952381\\
  \hline
  \texttt{total} &  0.552381 & 0.447619 & 1
  \end{array}$$

<font size="4">
On obtient les **pourcentages de l'effectif** dans les cases du tableau.


C'est également la **probabilité** , qu'un individu de la **population observée** soit caractérisé par les modalités en ligne et en colonne.
</font>

## Construction de la distribution théorique

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.20952381&  0.11428571 &  \textbf{0.3238095}\\ 
  \hline
  rond &    0.09523810 & 0.28571429 & \textbf{0.3809524}  \\ 
    \hline
  triangle & 0.24761905 & 0.04761905 &   \textbf{0.2952381}\\
  \hline
  \texttt{total} &  \textbf{0.552381} & \textbf{0.447619} & 1
  \end{array}$$


De la même façon, les **fréquences marginales** (marges divisées par la taille de la pop.), donnent la **probabilité** d'observer un individu de la modalité correspondant à la ligne ou à la colonne considérée.

<font size="4">
Exemple : dans cette population , j'ai 29.5% de chances de tirer un triangle, et 55% de chances de tirer une pièce blanche.
</font>

## Construction de la distribution théorique

Rappel : Probabilité conjointe de deux évènements $A$ et $B$ **indépendants**  
$P(A \cap B) = P(A) \times P(B)$

À partir des **fréquences marginales** précédentes, on obtient pour chaque couple de modalités, la probabilité **théorique**,  celle qui suppose $H_0$, par un simple **produit**.

<font size="4">
Exemple :  Si $H_0$ est vraie, la probabilité d'observer un triangle noir est donnée par: 

$P(triangle \cap noir) = P(triangle) \times P(noir)$

$P(triangle \cap noir) =0.447619 \times 0.2952381  = 0.1321542$

La probabilité théorique d'observer un triangle noir est de 13,2%  
</font>

## Construction de la distribution théorique

On crée un second tableau, dont chaque case vaut le produit des fréquences marginales calculées sur le tableau des observations.

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.1788662 &   0.1449433 &  \textbf{0.3238095}\\ 
  \hline
  rond &    0.2104309 &0.1705215 & \textbf{0.3809524}  \\ 
    \hline
  triangle &  0.1630839 & 0.1321542 &   \textbf{0.2952381}\\
  \hline
  \texttt{total} &  \textbf{0.552381} & \textbf{0.447619} & 1
  \end{array}$$

C'est le tableau des **fréquences théoriques**.


## Tableau des effectifs théoriques

On l'obtient en multipliant les fréquences théoriques par la taille de la population observée (ici 105)


$$\begin{array}{c|c|c}
   & blanc & noir \\ 
   \hline
carré &  18.78095 &  15.21905 \\ 
  \hline
  rond &   22.09524 & 17.90476    \\ 
    \hline
  triangle & 17.12381 & 13.87619   \\
  \end{array}$$

<font size="4">
N.B. Il n'est pas nécessaire d'arrondir les effectifs théoriques
</font>


## Calcul du Chi 2


C'est la somme, pour chaque case du tableau de contingence (i.e. pour chaque couple de modalités),  des écarts carrés entre effectif observé et effectif théorique**, divisés par l'effectif théorique.

Soient $T^{obs}$ le tableau des effectifs observés, $T^{theo}$ le tableau des effectifs théoriques, 


<center>
$\chi^2 =  \sum_{i,j}  \frac{( T^{obs}_{i,j} -  T^{theo}_{i,j})^2}{T^{obs}_{i,j}}$       
</center>

ici : $\chi^2 = 26.30329$


## Interprétation du Chi 2 

Il faut comparé la valeur du $\chi^2$ calculée avec la **valeur critique** qu'on trouve   dans une **table de loi de Student** (ou table de loi du chi 2).

C'est un tableau à double entrée : une **valeur de quantile**, et un **degré de liberté**.

On peut considérer que la valeur de quantile est le pourcentage d'erreur qu'on s'autorise de faire.  On prend souvent **5%** :  la colonne 1-0.05 =  0.95

Le degré de liberté est obtenu en calculant la valeur $(nb\_lignes - 1)*(nb\_colonnes -1)$.

Dans notre exemple , le degré de liberté est 2*1 = 2


## Table de loi de Student 

<center>
<img src="table.loi.de.student.png", width=100%></img>
</center>


## Interprétation du Chi 2 


D'après le tableau de la loi de Student , la valeur critique pour un test avec 5% de chances de se tromper est un degré de liberté de 2 vaut 4.303.

Si la valeur calculée du $\chi^2$ est **supérieure** à la valeur critique, on **rejette** $H_0$.



Pour notre exemple: On rejette $H_0$, i.e. les deux variables sont **dépendantes**,  car $\chi ^2 \approx 26 > 4.303$

Interprétation: «la forme est liée à la couleur dans cette population, nous pouvons l'affirmer avec un risque d'erreur d'au moins  5%»


## Les étapes du $\chi ^2$

- Tableau de contingence
- Sommes marginales 
    - Calcul des fréquences observées 
    - Calcul des fréquences théoriques
- Tableau d'effectifs théoriques
- Calcul de la valeur du test 
- Comparaison avec les valeurs de la table de Student 



# Lien entre une variable qualitative et une variable quantitative.


## Représentation graphique.

Pas de moyen **simple** de calculer le lien entre une variable qualitative et une variable quantitative.
  
  - corrélation de rang 
  - régression logistique 
  - analyse de la variance (ANOVA)
  
$\implies$ Alors on fait un graphique ! 

La variable qualitative sert de **catégorie**, on fait varier la représentation graphique de la variable quantitative suivant cette catégorie.

2 possibilités :

  - boîtes à moustaches
  - superposition d' histogrammes / densités 

## Les boîtes à moustaches 

"Boîte à moustaches", aussi appellées "boxplot", montre les quartiles d'une variable

Quartile = «quantile de 4», valeurs qui séparent une variable quantitative en quarts, i.e. paquets d'un quart de l'effectif

- Premier quartile : sépare les 25% inférieurs des valeurs
- Second quartile : **mediane**, sépare les 50% inférieurs des valeurs
- troisième quartile : sépare les 75% inférieurs des valeurs


Dans R, fonction `quantile` 

## Boxplot et Distribution


```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.width=8, fig.height=6}
library(ggplot2)
library(cowplot)


n <-  4000
xval <-  rbeta(n, shape1 = 2, shape2 = 5) * 1000
mydf <-  data.frame(x=xval)
xmean <-  mean(xval)
xsd <-  sd(xval)

xlabels <- data.frame(
  x = c(quantile(mydf$x)),
  y = c(rep(330,5) ),
  text = paste0(c("","Q1: ", "Q2: ", "Q3: ", ""),names(quantile(mydf$x)))
)
distriPlot <-  ggplot(mydf, aes(x=x))+
  geom_histogram(bins = 50, fill="#0FAF96", color="grey")+
  geom_vline(data = xlabels, xintercept = xlabels$x, linetype=c(2,2,1,2,2))+
  geom_label(data= xlabels, aes(x=x,y=y,label=text) )+
  xlab("X")+
  ylab("Effectif")

# Marginal density plot of x (top panel) and y (right panel)
yplot <- ggplot(mydf, aes(y = xval))+
  geom_boxplot(outlier.alpha = 0.2)+
  coord_flip()+
  xlab("")+
  ylab("")+
  theme_void()

#assemblage
plot_grid(yplot, distriPlot,  ncol = 1, align = "hv",scale = c(1,1), 
          rel_widths = c(1, 1), rel_heights = c(1, 3))
```



## Boxplot par catégories


Principe : on trace un boxplot par modalité de la variable qualitative.

E.g. : consommation de véhicules par type (dataset `mpg` de R)


```{r, echo=FALSE, warning=FALSE}
p <- ggplot(mpg, aes(class, hwy))
p + geom_boxplot(outlier.alpha = 0.2) +xlab("type de véhicule (var. qualitative)") +ylab("consommation (var. quantitative")+theme_light()
```


# Références supplémentaires


## Refs


Cours complet sur les modèles linéaires : [https://www.math.univ-toulouse.fr/~barthe/M1modlin/poly.pdf]


Interprétation des graphique de `lm` en R : [https://data.library.virginia.edu/diagnostic-plots/]





