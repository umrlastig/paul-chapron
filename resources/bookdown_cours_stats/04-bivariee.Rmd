# Analyse Bivariée {#bivariee}


L'analyse bivariée , comme son nom l'indique, a pour objectif d'analyser le lien qui peut exister entre **deux** variables.


En guise de rappel sur les types de variables (cf section \@ref(vocabulaire)) , donnons des exemples: 

Pour deux variables quantitatives , on pourrait analyser le lien entre : 

- le nombre d'habitants et nombre de lignes de bus des départements français
- le nombre de lignes de bus en Isère en 1998 et en 2018  


Pour deux variables qualitatives, on pourrait analyser le lien entre : 

- la couleur des yeux  et le fait de porter des lunettes
- les catégories de séries télé et la plate-forme  où ils sont disponibles 


Enfin, pour une variable quantitative et une variable qualitative :

- le lien entre la taille et la couleur des yeux, 
- le lien entre  le nombre d'aces d'un joueur et le côté du cours de tennis qu'il occupe


### Analyse bivariée, mais sans la localisation


  
L'analyse bivariée que nous allons aborder dans ce chapitre concerne des variables traditionnelles, i.e. **pas des variables de localisation** . Si elles proviennent de données spatiales, ce sera : 

- soit des individus restreints spatialement (sélection spatiale)
- soit des variables "géographiques" (e.g. lieu de résidence) renseignées pour les individus

La **localisation en tant que variable** n'interviendra **pas** dans ce cours.
  

### Ressources pour l'analyse des localisation et des distances 

Il existe des outils statistiques pour analyser le lien qui existe entre des variables localisées dans l'espace *et leur localisation elle-même*. Ces techniques **ne sont pas au programme de ce cours**, je les mentionne pour les curieux. 


On peut ainsi mesurer l' **auto-corrélation spatiale** , qui indique si les valeurs proches sont regroupées ou au contraire disséminées dans l'espace, à l'aide de l'[indice de Moran  global](https://en.wikipedia.org/wiki/Moran%27s_I), et identifier des clusters de valeurs plus fortes ou plus faibles que la normale à l'aide de l'[indice de Moran local](https://geodacenter.github.io/workbook/6a_local_auto/lab6a.html).


On peut également effectuer des  régressions qui tiennent  compte de la localisation des observations dans l'espace, qu'on appelle GWR pour **Geographicaly Weighted Regression** (plus de détails [sur wikipedia](https://fr.wikipedia.org/wiki/R%C3%A9gression_g%C3%A9ographiquement_pond%C3%A9r%C3%A9e) et la [fiche de l'INSEE](https://www.google.com/url?sa=t&rct=j&q=&esrc=s&source=web&cd=&ved=2ahUKEwjxtrK85pvuAhX2DmMBHdDaCTwQFjAAegQIAxAC&url=https%3A%2F%2Fwww.insee.fr%2Fen%2Fstatistiques%2Ffichier%2F3635545%2Fimet131-m-chapitre-9.pdf&usg=AOvVaw0Mli34vfm-BI1PymvssD5Y)) 


Enfin , si on désire prendre en compte les **distances** dans les flux entre unités spatiales (par exemple pour expliquer les déplacements domicile-travail dans une région) on peut se tourner vers les [modèles gravitaires](https://www.hypergeo.eu/spip.php?article76) 

Nous donnerons à la fin de ce cours quelques éléments à ce sujet dans la section \@ref(anaspat) 





## Corrélation n'implique pas causalité 


Nous allons voir comment quantifier l'intensité du **lien statistique** qui peut exister entre deux variables.

<center>
 <span style="color:red; font-size:1.5em"> &#9888; Une liaison, même très forte, entre deux variables, n'indique pas la causalité! &#9888;</span>
</center>


<br>
Cette erreur d'amalgame entre corrélation et causalité est très courante, très tentante, justement à cause du fait que l'amalgame «marche» dans de nombreux cas empiriques. 


De nombreux contre-exemples sont heureusement disponibles pour finir de se convaincre que la **corrélation n'implique pas la causalité**: 


<img src="chart.svg"/>

© TylerVigen http://tylervigen.com/spurious-correlations $\leftarrow$ D'autres exemples sont disponibles à cette adresse.




### Diverses formes de dépendances 


Ce qu'on appelle **lien** ou **liaison** ou encore **dépendance** entre les variables expriment le fait que les valeurs de deux variables n'évoluent pas indépendamment mais au contraire présentent une certaine **forme** une certaine **régularité**.

Ces «régularités» peuvent être de plusieurs formes, en voici quelques unes : 


```{r formes, echo=FALSE, fig.width= 8, fig.height=5}
x <- runif(100, min = 0, max=10)
y1 <- 8.5*x - 23 + rnorm(100,0, 12)
y2 <- -16*x +12 + rnorm(100,0,10)
y3 <- sqrt(exp(x)) + rnorm(100,0,8)
y4 <-  50*sin(x) + rnorm(100,0,10)
y5 <-  rnorm(100, 0, 60)
y6 <- 21.345
data2 <-  data.frame(x, y1, y2, y3, y4, y5, y6)
library(reshape2)
library(ggplot2)
data2 <- melt(data2, idvars=c("x"), measure.vars=c("y1","y2","y3","y4", "y5", "y6"))
plo2 <-  ggplot(data2, aes(x=x, y=value))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  facet_wrap(~variable)+
  xlab("value of x variable")+
  ylab("value of yi variable")
  
plo2
```

Cette matrice de graphes, de gauche à droite et de haut en bas montre :

- une dépendance linéaire positive
- une dépendance linéaire négative
- une dépendance non-linéaire , peut-être exponentielle
- une dépendance périodique, sinusoïdale
- une absence de dépendance, les deux variables sont indépendantes
- une absence de dépendance, la variable de l'axe des $y$ est constante


En pratique les formes sont beaucoup moins régulières que ces exemples très «mathématiques» : les données peuvent être bruitées, incomplètes, contenir des outliers, etc.


### Les étapes de l'analyse bivariée 

On peut résumer la démarche 'mentale' à adopter devant un jeu de données par cette séquence:

1. Tracer le nuage de points
2. Existe-t-il une relation ? 
3. Est-elle de forme linéaire ?  De quel sens ?
4. Si la liaison est de forme linéaire $\rightarrow$ faire une **régression** 
5. Si la liaison est non linéaire, est-elle monotone ?  De forme connue ?$\rightarrow$  proposer un **modèle**, i.e. une équation qui décrive la forme de la dépendance entre les deux variables.
6. (5bis)Réaliser un modèle **LOESS** avec prudence (uniquement descriptif , aucun pouvoir de généralisation) 
cf le blog de Lise Vaudor [http://perso.ens-lyon.fr/lise.vaudor/regression-loess/]


Voici un arbre de décision plus précis : 


<img src="decisionTreeCorr.svg"/>





## Régression linéaire 



### Avant toute chose  


Vous devriez commencer à avoir l'habitude de ce mantra, encore plus valable dans le cas d'une analyse bivariée : 

<span style="color:red; font-size: 1.5em;">Toujours en premier: Regarder l'aspect des données avec des graphiques </span> 


Si le nuage de point n'est pas allongé, si vous "voyez" clairement qu'une droite ne le résulera pas, ou alors très mal, il n'est pas nécessaire d'entreprendre une rregression linéaire, qui sera de toute façon décevante!  


La regression linéaire ne concerne que les **variables quantitatives**.


### Principe et Vocabulaire
 
#### Droite de régression

Si la forme du nuage de points s'y prête, c'est-à-dire bien allongée, rectiligne, on peut entreprendre une **régression linéaire** (aussi appelé  **ajustement** linéaire).
 
Cela consiste à trouver la droite qui passe «**au mieux**» dans le nuage de points de deux variables quantitatives $V_1$ et $V_2$, celle qui **résume** le nuage de points d'une façon satisfaisante.


«**au mieux**» est ici employé au sens des **moindres carrés**, c'est-à-dire que parmi toutes les droites qui peuvent passer au milieu du nuage de points, on va choisir celle pour laquelle l'erreur commise est la plus faible. 



On cherche la droite (en fait on cherche les deux coefficients $a$ et $b$ de son équation $y=ax+b$) qui minimise la somme des carrés des écarts, d'où le nom d'estimation MCO (Moindres Carrés Ordinaires) ou OLS (Ordinary Least Squares in english).


Si le nuage de points n'est pas amorphe, sans forme, et que les deux variables ne sont pas totalement indépendantes, ou constantes, alors cette droite est **unique**.  


#### Modèle linéaire


En réalisant une régression linéaire, on cherche à **expliquer** une variable à partir d'une autre. «Expliquer» signifie ici «avoir un **modèle** pour calculer une valeur de la variable à partir d'une autre». 


Dans le cas d'une régression linéaire, le modèle est l'équation de **la** droite ajustée, de la forme $y=ax +b$, celle qui minimise la somme des écarts au carré.


On cherche donc les valeurs de $a$ et de $b$ (les coefficients du modèles linéaire) qui nous permettrait de calculer la valeur d'une variable (la variable dite **expliquée**) à partir d'une autre (la variable dite **explicative** ), que l'on peut écrire naïvement de cette façon : 

$$Valeurs\ de\ la\ variable\ expliquée = modèle(Valeurs\ de\ la\ variable\ explicative)$$

$$Valeurs\ prédites\ de\ V_2 = modèle(V_1)$$

Ici, notre modèle est linaire, donc:


$$Valeurs\ prédites\ de\ V_2 = a\times V_1 + b$$


 **Variable explicative**:
 C'est celle qui est utilisée par le modèle pour calculer les valeurs de la variable expliquée. Dans notre exemple , c'est $V_1$. Par convention , on la met sur l'axe des $x$ dans les graphes.


 **Variable expliquée** : 
 C'est la variable pour laquelle le modèle propose des valeurs: dans notre exemple c'est $V_2$. Par convention on la met sur l'axe des $y$ dans les graphes.


#### Estimation, Prédiction, Résidu


On peut utiliser le modèle, pour **estimer** (=calculer, prédire) les valeurs de $V_2$, avec les valeurs de $V_1$ et à l'aide de l'équation de droite. On  note souvent les valeurs prédites $\hat{V_2}$.

L'écart entre $\hat{V_2}$ et $V_2$ est appelé **résidu**. 

Ainsi pour toutes les observations du jeu de données, on a :  


$$V_2 =  prediction(V_1) + residu$$ 
$$V_2 =  \hat{V_2} + residu$$ 


#### L'erreur commise par le modèle

L'erreur commise par la droite, c'est la somme des écarts entre les points du nuage et la droite, les **résidus**, ces écarts sont élevés au carré avant d'être sommés pour que les erreurs positives et négatives ne se compensent pas. On appelle parfois cette somme la SSE (Sum of Square Erros in english) ou la RSS (Residuals Sum of Squares in english).

On peut l'écrire avec des notations différentes, mais la signification est toujours la même :

$$SSE = \sum_i (prediction_i - observation_i)^2$$
$$SSE = \sum_i ( modele(x_i)- x_i)^2$$
$$SSE = \sum_i ( ax_i + b - x_i)^2$$
$$SSE = \sum_i ( \hat{y_i}- x_i)^2$$


> On peut noter qu'il ne s'agit pas de la distance entre les points et la droite (leur projeté orthogonal), mais bien l'écart en ordonnée qui intervient dans ce calcul.


Quasiment toutes les statistiques inférentielles reposent sur ce genre d'équation : on cherche un modèle d'une variable, à partir d'autres variables explicatives, et parmi les modèles possibles, on essaye de réduire l'erreur et de trouver le «bon» modèle: celui qui produise des résidus avec de nombreuses propriétés souhaitables : valeurs faibles,  identiquement distribuées, de distributions proches de la gaussienne, sans auto-corrélation temporelle ni spatiale, etc. 

Nous reviendrons dans la section bonus \@ref{residuslm} sur la façon de qualifier les erreurs commises par un modèle linaire avec R.



### Interpréter la droite de régression

L'objectif de la régression linéaire est trouver le meilleur modèle linéaire entre deux variables. Ce modèle est l'équation d'une droite, qu'on appelle **droite de régression** et qui permet de visualiser: 

* l'**intensité** de la dépendance, suivant que les points sont proches de la droite ou non
* la **forme** de la dépendance, suivant que le nuage soit bien de forme linéaire
* le **sens** de la dépendance : nulle, positive ou négative 


```{r regline, echo=FALSE, fig.width=6, fig.height=4, message=F}
library(ggplot2)
data <-  data.frame(X=c(1,2,3,5,6,7,12,14,22, 2,3 ,10,10,9,7),Y=c(15,16,13, 9,9,12,3,2,5,11,12,7,8,9,8))
plo <-  ggplot(data, aes(x=X, y=Y))+
  geom_point()+
  geom_smooth(method="lm", se=FALSE)+
  xlab("Var1")+
  ylab("Var2")
plo
```

 
Dans cet exemple, le nuage de point est assez allongé, on constate une dépendance linaire négative. Les points sont relativement proches de la droite.

 
### Utiliser un modèle linéaire 
 

L'**équation** de la droite  est un **modèle linéaire** de la relation statistique qui lie $V_1$ et $V_2$; 

Ici le modèle est :  $\hat{V_2}=aV_1+b$

Si la régression linéaire est réussie, alors pour un individu $i$ dont on connait $V1_i$, on infère la valeur $V_{2i}$ par le modèle :    $\hat{V_{2i}} = aV_{1i} +b$
 
On dit aussi que $V_1$ **explique** $V_2$ , ou que le modèle **prédit** $V_2$ à partir de $V_1$ (on note les valeurs prédites $\hat{V_2}$).


Il est intéressant d'avoir un (bon) modèle de la relation entre $V_2$ et $V_1$, car en l'absence d'observations supplémentaires de $V_2$ , on peut estimer les valeurs qu'elles prendraient si on dispose d'observations supplémentaires de $V_1$.

Imaginez que des observations de $V_2$ soient particulièrement coûteuses à recueillir, et les observations de $V_1$ particulièrement peu coûteuses,  par exemple la production de salive d'un tigre adulte en colère ($V_2$) et la largeur de ses empreintes ($V_1$).
En cas de bonne qualité de régressions, le.la zoologiste sera ravi.e de mesurer les largeurs d'empreintes.



 
### Évaluer la  qualité d'une régression linaire : le $R^2$



En pratique, pour considérer qu'une régression linéaire est de bonne qualité, i.e. que le modèle linéaire qui lie les deux variables décrit (explique, prédit, ..) bien les données,   il faut réunir deux critères : 

- des coefficients avec des **p-values** associées **faibles** (e.g. <0.05)  qu'on peut grossièrement traduire par "on a peu de chances de se tromper"
- un **$R^2$ élevé**,   qu'on peut traduire par  "le modèle prédit bien les observations"
  

D'autres critères sont donnés en bonus à la section \@ref(residuslm).

On commence par le $R^2$.

 
#### le $R^2$
 
Le **coefficient de détermination linéaire** ,  noté $R^2$  est une valeur qui décrit la **qualité de prédiction** de la régression,  c'est-à-dire à quel point la droite de régression estime correctement les valeurs de la variable expliquée.



Il est défini par : 
 $$R^2  = 1 - \frac{\sum_{i=1}^{n} (y_i - \hat{y})^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$


- $R^2 \in [0;1]$
- Plus le $R^2$ est proche de 1, meilleure est la qualité.

Pour avoir l'intuition de l'interprétation de cette formule, on peut remarquer que la fraction est un ratio entre la somme des résidus au carrés (écart entre valeurs observées $y_i$ et valeur prédite $\hat{y_i}$) et la variance (cf. section \@ref(variance)) de la variable expliquée ($y$).  

Le dénominateur de la fraction est constant , il vaut $var(y)$
Alors, intuitivement , pour un mauvais modèle, qui prédit mal les observations de $y$, la somme des résidus au carré sera importante, donc la fraction également, et la valeur de $R^2$  sera petite, proche de 0.
De la même manière, pour un bon modèle, qui prédit bien les valeurs de $y$, les résidus seront faibles, donc leur somme quadratique également, ainsi que la valeur de la fraction, ce qui donnera un $R^2$ proche de 1.

Mentalement , on peut réécrire la formule ainsi : 

$$R^2 \approx 1- \frac{résidus^2}{variance\ de\ y}$$

$$R^2 \approx 1- (erreur\ commise\ normalisée)$$


####  Propriétés et interprétation additionnelles du $R^2$


Le $R^2$ peut s'écrire de plusieurs façons, notamment ainsi : 

$$R^2  =  \frac{\sum_{i=1}^{n} (\hat{y_i} -\bar{y} )^2}{\sum_{i=1}^{n} (y_i - \bar{y})^2}$$


Cette expression est intéressante pour nous car on reconnait encore une fois une fraction de *variances* (cf. section \@ref(variance)) : 

- au dénominateur , c'est la variance de $y$ , la variable expliquée
- au numérateur , c'est la variance de $\hat{y}$, la variable prédite par le modèle 

(les termes en $\frac{1}{n}$  des variances au numérateur et dénominateur se simplifient )

On peut donc voir le $R^2$ comme un ratio de variances : celles des estimations et celles des observations.
En faisant une regression, on cherche à «capturer» la variance de la variable expliquée à l'aide de la variance de la variable estimée par le modèle, d'où une valeur de $R^2$ proche de 1 lorsque la variance des estimations est proche de  la variance des observations



#### Exemples avec différentes valeurs de $R^2$


Voici des exemples de régression linéaire de différentes qualités avec la valeurs correspondante de $R^2$, on donne aussi la valeur de la p-value, que nous aborderons juste après.  



```{r expleR2_1,fig.width=6, fig.height=4, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE}
regression <- lm(iris$Petal.Length~iris$Petal.Width)
pvals <- coef(summary(regression))[,4]

titre <- paste0("R2=", summary(regression)$adj.r.squared %>% round(3))
soustitre <-  paste0("p-values associées à a : ",pvals[2] %>% signif(3), ", à b: ", pvals[1] %>% signif(3) )
ggplot(iris, aes(x=Petal.Length, y=Petal.Width))+
  geom_point()+
  geom_smooth(se=FALSE, method="lm")+
  theme_light()+
  labs(title=titre, subtitle = soustitre)
```


Cette première régression est de bonne qualité: le $R^2$ est proche de 1, la droite décrit bien les données : peu de point s'en écartent significativement. 


```{r expleR2_2,fig.width=6, fig.height=4, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE  }
regression <- lm(penguins$flipper_length_mm ~penguins$bill_length_mm * runif(nrow(penguins)))
pvals <- coef(summary(regression))[,4]

titre <- paste0("R2=", summary(regression)$adj.r.squared %>% round(3))
soustitre <-  paste0("p-values associées à a : ",pvals[2] %>% signif(3), ", à b: ", pvals[1] %>% signif(3) )
ggplot(penguins, aes(x=flipper_length_mm, y=bill_length_mm))+
  geom_point()+
  geom_smooth(se=FALSE, method="lm")+
  theme_light()+
  labs(title=titre, subtitle = soustitre)
```

Cette deuxième régression est de moindre qualité.
Certes, le nuages de points à une forme suffisamment "allongée" pour qu'on entreprenne une régression linéaire, mais celle-ci est de qualité intermédiaire : beaucoup de points sont éloignés de la droite, le nuage est assez dispersé, ce qui augmente les erreurs et donc amoindrit le score du $R^2$.





```{r expleR2_3,fig.width=6, fig.height=4, cache=TRUE, echo=FALSE, message=FALSE, warning=FALSE  }
nbpts <- 1000
xx <-  runif(nbpts)
yy <- 2*xx + 4 + runif(nbpts,-5,5) 
df <-  data.frame(xx,yy)
regression <- lm(df$yy ~ df$xx)
pvals <- coef(summary(regression))[,4]

titre <- paste0("R2=", summary(regression)$adj.r.squared %>% round(3))
soustitre <-  paste0("p-values associées à a : ",pvals[2] %>% signif(3), ", à b: ", pvals[1] %>% signif(3) )
ggplot(df, aes(x=xx, y=yy))+
  geom_point()+
  geom_smooth(se=FALSE, method="lm")+
  theme_light()+
  labs(title=titre, subtitle = soustitre, x="Variable 1 ", y="Variable 2")
```

Avec ce dernier exemple volontairement exagéré, la qualité de la régression est mauvaise.

Certes la forme du nuage de points est linéaire, mais la bande est tellement large que les erreurs seront très grandes, même si la droite de régression passe bien au milieu. La variance de la variable 2 est telle qu'elle n'est pas suffisamment "capturée" par la droite.
On notera que les p-value sont très faibles,  ce qui on va le voir, indique qu'on est à peu près certains de l*'équation de la droite*, c'est bien celle-ci qui passe "au mieux" dans le nuage de points. Le problème, c'est que ce nuage de points est tout simplement «mal» décrit par une droite, vu sa forme !   



 
### Évaluer la  qualité d'une régression linaire : la p-value


#### la p-value et l'hypothèse nulle


La p-value est la seconde quantité qui nous renseigne sur la qualité d'une régression linéaire.
Le $R^2$ mesurait la qualité de prédiction du modèle linéaire, la **p-value** est plutôt associée avec la notion de confiance qu'on peut placer dans les coefficients du modèle linéaire que l'on obtient.

La façon d'interpréter correctement une p-value fait l'objet de nombreux débats, on en propose une ici, mais il y en a de nombreuses autres.



**Approximation grossière **  : 

la p-value est le  **pourcentage de chances de se tromper** en rejetant l'hypothèse nulle, c'est-à-dire se tromper en considérant que  les deux séries ne sont pas indépendantes et qu'il existe une relation entre les deux (ici, linéaire car nous testons un modèle linéaire).

Sur la base de cette approximation, on peut d'ores et déjà dire que plus cette p-value est petite , mieux c'est. 

La p-value est associée à la notion d'**hypothèses nulle**. 
Ici , l'hypothèse nulle  $H_0$ est **"les deux variables sont indépendantes"**. 

- **rejeter $H_0$** c'est considérer que les deux variables **ne sont pas indépendantes**
- (par abus de langage) **conserver $H_0$** c'est  considérer qu'elles **sont indépendantes** 


En statistique, on raisonne avec des hypothèses, qu'on rejette ou qu'on conserve, suivant la valeur de **tests statistiques**.

Par exemple, lorsqu'on teste si deux variables sont linéairement liées, on formule en fait une hypothèse **alternative** , $H_1$, qui est «les deux variables sont linéairement liées».

Quand on rejette $H_0$ et qu'on  a formulé une hypothèse alternative , on accepte *de facto* $H_1$.


$H_0$, est l'hypothèse à formuler dans le cas **le plus général possible**, quand on ne sait rien du tout sur les données. Dans ce cas là, étant donné deux variables, on pose l'hypothèse qu'elles sont indépendantes, tout simplement parce qu'il n'y a aucune raison qu'elles soient liées.



#### Approximation moins grossière à propos de la p-value

La p-value peut s'interpréter comme «la probabilité d'avoir un résultat au moins aussi marqué étant donné l'hypothèse nulle»


Imaginez que vous ayez un jeu de données $D_{ini}$ constitué de 2 variables quantitatives pour 500 individus.
Le nuage de points est suffisamment allongé pour que vous entrepreniez une régression linéaire.
Vous obtenez une certaine valeur pour vos coefficients $a_{D_ini}$ et $b_{D_ini}$ de régression linéaire.
Vous pouvez même faire une test de corrélation de Pearson, et obtenir une valeur de corrélation élevée, bref , **vous rejetez $H_0$ pour $D_{ini}$**





Maintenant imaginez qu'on génère une infinité de jeu de données $\Gamma$ à 2 variables et à 500 individus **sachant** $H_0$, donc avec des variables indépendantes, des variables pour lesquelles on **sait** qu'il n'y a aucun lien.

Parmi cette infinité de jeu de données, il y en a une proportion qui, **par hasard**, **par chance**,  présente des données qui ont la même forme que notre jeu de données, ou une forme suffisamment proche, qui donnerait des résultats **aussi remarquables** c'est à dire égaux ou encore plus marqués ,  de régression, la même droite, la même corrélation.


Trions l'infinité de jeu de données $\Gamma$ selon ce critère de résultats, d'un côté les jeux de données qui indique les même résultats que notre régression sur $D_{ini}$, qu'on va noter $\Gamma_{corrélés}$, de l'autre les jeux de données pour lesquels on ne peut pas conclure aux même résultats: il n'y a pas de corrélation entre les deux variables, la forme du nuage de points n'est pas assez linéaire pour qu'un analyste décide d'en faire une régression linéaire. On note ces jeux $\Gamma_{indépendants}$.


Rappelons qu'on a généré cette infinité de jeu de données sous l'hypothèse $H_0$.
On s'est donc trompés, pour tous les jeux de données étiquetés $\Gamma_{corrélés}$, en obtenant des résultats de corrélation et de régression linéaire similaires (ou plus marqués encore) à ceux obtenus sur $D_{ini}$.

On ne s'est pas trompés pour tous les jeux de $\Gamma_{indépendants}$

La p-value c'est la proportion $$\frac{\Gamma_{corrélés}}{\Gamma}=\frac{\Gamma_{corrélés}}{\Gamma_{indépendants}+\Gamma_{corrélés}} $$.

C'est la proportion de fois où on s'est trompés en affirmant à propos de $\Gamma_{corrélés}$ les résultats obtenus sur $D_{ini}$ alors qu'on avait des jeux de données où $H_0$ était vraie par construction. 


> Évidemment cette proportion est très faible pour des cas où le lien entre les deux variables est marquée, mais comme on raisonne à l'infini elle ne sera jamais nulle !





####  Ce qu'il faut retenir de la p-value


- plus elle est petite, mieux c'est
- en sciences expérimentales, le seuil de significativité est bas (~5%), en sciences sociales , il est plus haut.
- elle s'interprète comme un pourcentage de chances de se tromper en première approximation
- il faut plutôt le voir comme la probabilité d'obtenir par chance les même résultats de régression linéaire, pour une jeu de données de variables indépendantes de même taille. 






## Effectuer une régression linéaire avec  R

### La commande `lm`

La régression linéaire entre deux variables `x` et `y` s'obtient avec la fonction `lm(y ~ x)`.

Le premier argument est sous une forme particulière d'expression en R :une **formule**.

Elle s'écrit `Variable expliquée ~ Variable explicative` dans notre cas, puisque nous faisons une régression *univariée* , c'est-à-dire qu'on essaye d'expliquer une variable à l'aide d'une seule autre variable. 

```{r reslm,fig.width=6, fig.height=4}
lm(iris$Petal.Length~iris$Petal.Width)
```

### Format des résultats 


La fonction `lm` renvoie une objet complexe, qui contient beaucoup d'informations.
Elles sont heureusement résumées sous une forme lisible en console par la fonction `summary()`

```{r reslm2,fig.width=6, fig.height=4}
regression <- lm(iris$Petal.Length~iris$Petal.Width)
summary(regression)
```


On y trouve dans l'ordre: 

- la formule de la regression (section `Call`)
- les quartiles des résidus (section `Residuals`)
- les coefficients du modèle ajusté et leur **p-value** associée, ici sur un test de Student, notée `Pr(>|t|)` (section `Coefficients`) 
- le $R^2$, on pourra prendre la valeur étiquetée `Adjusted R-squared` (dernière section) 






### Bonus: Critères de significativité du lien linéaire {#residuslm}

Ces critères portent sur les **résidus** $\epsilon_i$, i.e. l'écart entre valeur observée et valeur prédite de la variable $y$ pour l'individu $i$

$$\epsilon_i= y_i - \hat{y_i}$$


Les résidus doivent:

- être indépendants  : covariance nulle ou très faible $cov(x_i, \epsilon_i) = 0$
- être distribués selon un loi normale de moyenne nulle $\epsilon \sim \mathscr{N}(0,\sigma_{\epsilon})$
- être distribués de façon homogène (homoscédasticité), i.e. de variance constante $var(\epsilon_i)=\sigma_{\epsilon}^2$ , indépendante de l'observation


On peut vérifier une partie de ces critères à l'aide des quartiles que donne R dans les résultats de régression.

R nous aide avec plusieurs fonctions :


#### Évaluation de l'indépendance des résidus avec R 

On utilise les graphique de la fonction `acf`.  

Si une barre exceptée la première dépasse  la ligne en pointillés, on peut remettre en cause l'indépendance des résidus. Ici, c'est le cas. Cela ne veut pas pour autant dire que le modèle linéaire est à mettre à la poubelle, simplement qu'il y a une erreur systématique dans ses résidus, qui ne sont pas assez indépendants.

```{r residualSigni3,fig.width=6, fig.height=4, cache= TRUE}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
acf(residuals(modele1))
```



 


#### Les 4 graphiques résultats de la fonction `lm`


La fonction `lm` de R et ses résultats permettent de tracer 4 graphiques pour évaluer certains des critères de significativité.
Pour cela il suffit d'appeler la fonction `plot` sur l'objet résultat de la régression.


```{r residualSignif,fig.width=6, fig.height=4, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
par(mfrow=c(2,2)) # pour avoir une matrice de graphes
plot(modele1)
```



#### Évaluer l'homogénéité des résidus avec R

Le premier graphique sert à vérifier que le nuage de points est homogène c'est-à-dire qu'il n'y a pas de relation non-linéaire entre résidus et valeurs prédites. 

Une éventuelle relation non-linéaire pourrait se retrouver dans les résidus, et c'est normal. C'est «normal» puisqu'un modèle linéaire ne va capturer que des structures, des tendances **linéaires** entre deux variables. En cas de structure trop marquée dans les résidus, il faut peut être proposer un modèle non-linéaire, plus délicat à ajuster mais qui capturera plus de variance et laissera des résidus moins marqués.



Ici on observe une légère structure parabolique: 

```{r residualSignif1,fig.width=6, fig.height=3, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,1)
```


#### Évaluer la normalité de la distribution des résidus 

Le deuxième graphique appelé "Q-Q plot" nous permet de vérifier l'**hypothèse de normalité des résidus**.

Pour cela, on compare donc les valeurs de quantiles des résidus $\epsilon_i$ avec des quantiles de la distribution de loi Normale. 

On représente le nuage de points $(Q_{theo}, Q_{obs})$ , avec $Q_{obs}$ les valeurs de quantiles des résidus en ordonnée et  $Q_{theo}$ les quantiles de la distribution théorique (une Gaussienne donc) en abscisse.

Si les résidus suivent une loi normale , les points seront être proches de la bissectrice , puisque leurs valeurs de quantiles seront proches.


```{r residualSignif2,fig.width=6, fig.height=4, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,2)
```




#### Évaluer l'homoscédasticité des résidus 



Le troisième graphique intitulé "Scale location" indique si les résidus sont distribués de façon homogène suivant les valeurs "fittées", par rapport à une droite.
Dans ce cas  la droite est plutôt horizontale et les points sont disposés de façon homogène autour.

```{r residualSignif3,fig.width=6, fig.height=2.5, cache=T}
modele1 <- lm(iris$Petal.Length~ iris$Petal.Width)
plot(modele1,3)
```

Dans cet exemple: on voit une légère pente mais les points sont distribués de façon relativement homogène autour de la droite



# Corrélation de deux variables quantitatives {#correlation}

## Corrélation (linéaire)

Dans le cas d'une liaison statistique linéaire entre deux variables, on peut calculer l'«intensité» et le sens de ce lien sans nécessairement trouver les coefficients du modèle linéaire : c'est ce que nous décrit la valeur   de **corrélation**.

$cor(x,y) \in [-1;1]$ entre deux variables $x$ et $y$ .

- +1 : les deux variables croissent ou décroissent conjointement
- -1 : quand l'une des variables croît, l'autre décroît.
- 0 : pas de relation **linéaire** entre les deux variables
 
 <font size="3">R donne le coefficient de Pearson par defaut, l'argument `method` de la fonction `cor()` permet de spécifier deux autres coefficients : Kendall et Spearman.</font>


## Test de corrélation entre deux variables avec R

Version plus complète : c'est un **test**, on a plusieurs indicateurs statistiques sur ce test, notamment la **p-value** et **l'intervalle de confiance**

```{r cortestIris}
cor.test(iris$Petal.Length, iris$Petal.Width)
```
Rappel : la p-value quantifie la **significativité** du test.
En général, on considère le test significatif si elle est en dessous de 5%, soit 0.05.  


## Calcul direct du coefficient de corrélation

Soient deux variables $V_1$ et $V_2$

Le coefficient de corrélation $r$ de $V_1$ et $V_2$ est la normalisation de la covariance par le produit des écart-types des variables 

$r= \frac{cov(V_1,V_2)}{\sigma_{V_1}\sigma_{V_2}}$ 




La covariance est la **moyenne du produit des écarts à la moyenne**

$cov(V_1,V_2)= E[(V_1-E[V_1])(V_2-E[V_2])]$



#### Lien entre la corrélation et le $R^2$ d'une régression 


Si on fait une régression linéaire entre deux variables $x$ et $y$ , $y$ étant la variable expliquée, alors  


$R^2=cor(x,y)^2= r^2$


et 

$R^2=cor(\hat{y},y)^2$   

avec $\hat{y}$ les valeurs de $y$ prédites par le modèle linéaire.


La démonstration n'est pas évidente , nous ne la donnerons pas ici, mais l'intuition des conséquences de ces deux égalités est assez directe: 

- Plus les deux variables sont linéairement corrélés (ce qui traduit le coefficient de corrélation de Pearson, avec des valeurs proches de 1 ou de -1 ), plus la régression linéaire sera de bonne qualité (si $r$ proche de -1 ou 1 , alors $r^2$ est proche de 1 )

- Dans le même ordre d'idée, plus les deux variables sont très corrélées, meilleure sera la régression et plus  les valeurs prédites par le modèle seront  proches des valeurs observées, et donc également très corrélées aux valeurs observées.






## Matrice de corrélations


```{r corMatIris}
cor(iris[,1:4])
```

Présentation des corrélations entre les variables quantitatives d'un tableau, pour tous les couples de variables.

La matrice de corrélation est symétrique, et sa diagonale est constituée de 1.




## Sensibilité aux 'outliers'

```{r sensitiveOut1}
X <-  c(3,2,3,4,1,2,3,4,5,2,3,4,3)
Y <-  c(1,2,2,2,3,3,3,3,3,4,4,4,5)
plot(X, Y, xlim = c(0,16), ylim= c(0,16))
cor.test(X,Y)$estimate
```


## Sensibilité aux 'outliers'

```{r sensitiveOut2}
X <-  c(3,2,3,4,1,2,3,4,5,2,3,4,3,15)
Y <-  c(1,2,2,2,3,3,3,3,3,4,4,4,5,15)
plot(X, Y, xlim = c(0,16), ylim= c(0,16))
cor.test(X,Y)$estimate
```

## Sensibilité aux 'outliers'

Outlier : observation "*anormale*",  par sa valeur extrème , comparée aux autres.


La corrélation et la régression linéaire sont très sensibles aux outliers.

$\rightarrow$ s'interroger sur la nécessité de nettoyer/filter les données et des conséquences 



# Régression linéaire avec R

## Regression linéaire avec R

Fonction  `lm()` , modèle de la forme `Variable_a_expliquer ~ Variable_explicative` 

```{r regress, echo=TRUE }
my_model <- lm(Petal.Width~Petal.Length, data=iris)
summary(my_model)
```


## Que faire lorsque la relation n'est pas linéaire ?

Quand les deux variables sembles corrélées , de façon **monotone** mais **non linéaire**,

$\rightarrow$ Coefficient de **Spearman**, basé sur le **rang** des individus.

<center>
$\rho = 1 - \frac{6\sum_{i=1}^{n}(rg(X_i)-rg(Y_i))^2 }{n^3 -n}$
</center>


avec :

$rg(X_i)$ le *rang* de $X_i$  (le classement de sa valeur) dans la distribution de $X$

$n$ le nombre d'individus

## Obtenir le coefficient  de Spearman avec R


```{r rho}
cor.test(iris$Sepal.Length, iris$Sepal.Width, method="spearman", exact = FALSE)
```


l'argument `exact` doit être précisé en cas de valeurs ex aequo dans les données.  


## Utilisation conjointe des coefficients de Pearson et Spearman 

$r$ (Pearson) et $\rho$ (Spearman) sont deux moyens d'estimer la corrélation: lequel choisir ?

si $r = \rho$: on garde $r$ (plus simple à interpréter)

si $r < \rho$: la relation est non-linéaire : prendre  $\rho$

si $r > \rho$: il y a un biais, prendre $\rho$ (plus robuste)

... et toujours **tracer le nuage de points** pour examiner  la nature de la relation.


# "trucs" pour linéariser des relations non-linéaires


## Relation log-linéaire 

```{r loglin, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- 1.5*x^2.2  + rnorm(1000,10, 12)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```


Relation de type $y=ax^b$ 

se linéarise par $ln(y)=aln(x) + ln(b)$



## Relation géométrique (exponentielle) 

```{r expolin, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- exp(1.5*x+ 0.2)  + 10000*rnorm(1000,10, 12)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```


Relation de type $y=e^{ax+b}$ 

se linéarise par $ln(y)=ax + ln(b)$

## Relation logarithmique

```{r logar, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
y1 <- 1.5*log(x)+ 0.2  + 0.22*rnorm(1000,1, 2)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```

Relation de type $y=a*ln(x)+b$ 

$\rightarrow$ changement de variable 

## Relation logistique

```{r logistic, echo=FALSE, fig.width= 8, fig.height=4, cache=T}
x <- runif(1000, min = 0, max=10)
ymax <-  10
ymin <- 0
y1 <- ymin+ ((ymax -ymin) /(1 + exp(-1.5*x+6.8))) + 0.5*rnorm(1000,1,1)
data2 <-  data.frame(x, y1)
library(ggplot2)
plo2 <-  ggplot(data2, aes(x=x, y=y1))+
  geom_point(size =1, color = "#0FAF96", alpha=0.8)+
  xlab(" x ")+
  ylab(" y ")
plo2
```

Relation de type $y= y_{min} * \frac{y_{max}-y_{min}}{1+e^{ax+b}}$

se linéarise par $ln\bigg(\frac{y_{max}-y}{y-y_{min}}\bigg)=ax+b$

# Lien entre deux variables qualitatives

## Représentation graphique

Pour deux variables qualitatives, on ne peut pas produire de nuages de points, ni de droite de régression.

$\rightarrow$ on peut représenter la table de contingence (cf. fonction `mosaicplot` de R).

```{r mosaic, echo=FALSE, fig.height=3}
library(RColorBrewer)
mypalette <-  brewer.pal(12,"Set3")
par(mar=c(0,0,0,0))
mosaicplot(~Class+Survived, data=Titanic, color=mypalette,main = "")
``` 


## Test statistique dit du "Chi 2" ou "Chi carré"

Le test du $\chi ^2$ est un test d'indépendance, il mesure l'**écart**, la différence,  entre deux distributions de **variables qualitatives**  

Il répond à la question : "Existe-t-il un lien statistique entre deux séries de valeurs qualitatives"  

(La réponse est de type  OUI/NON , le $\chi^2$ ne donne pas l'**intensité** du lien)


## Test statistique dit du "Chi 2" ou "Chi carré"

- Hypothèse nulle $H_0$ : les deux distributions sont indépendantes.
- «faire le test» permet de conserver ou de rejeter cette hypothèse

## Principe du Chi 2

- On génère une **population théorique** à laquelle on va comparer la **population observée** en considérant leurs **distribution**.
- Cette distribution théorique reflête ce qui se passerait si on suppose que $H_0$ est vraie 
- Avec cette comparaison,  on pourra rejeter ou conserver l'hypothèse nulle.

La construction de cette distribution se fait à partir du **tableau de contingence**


##  Tableau de contingence

C'est un tableau à double entrée qui croise deux **variables qualitatives**. 


Dans une case on trouve l'**effectif** (= le nombre) des individus caractérisés par la conjonction des modalités en ligne et en colonnes.


Exemple sur des formes géométriques de couleurs :

$$\begin{array}{c|c|c}
   & blanc & noir \\ 
   \hline
carré &  22 &   12 \\ 
  \hline
  rond &   10 &  30 \\ 
    \hline
  triangle &  26 &   5 \\ 
\end{array}$$


Dans R : fonction `table()`

## Construction de la distribution théorique.

On commence par sommer les effectifs selon les modalités (en ligne et en colonne)

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  22 &   12 & 34\\ 
  \hline
  rond &   10 &  30 &  40  \\ 
    \hline
  triangle &  26 &   5 &  31\\
  \hline
  \texttt{total} & 58 & 47 & 105
  \end{array}$$

On appelle les sommes en lignes et en colonnes **sommes marginales**, elles sont mises dans les "marges" du tableau.



## Construction de la distribution théorique

En divisant par la taille de la population, on obtient les **fréquences observées**.

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.20952381&  0.11428571 &  0.3238095\\ 
  \hline
  rond &    0.09523810 & 0.28571429 & 0.3809524  \\ 
    \hline
  triangle & 0.24761905 & 0.04761905 &   0.2952381\\
  \hline
  \texttt{total} &  0.552381 & 0.447619 & 1
  \end{array}$$

<font size="4">
On obtient les **pourcentages de l'effectif** dans les cases du tableau.


C'est également la **probabilité** , qu'un individu de la **population observée** soit caractérisé par les modalités en ligne et en colonne.
</font>

## Construction de la distribution théorique

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.20952381&  0.11428571 &  \textbf{0.3238095}\\ 
  \hline
  rond &    0.09523810 & 0.28571429 & \textbf{0.3809524}  \\ 
    \hline
  triangle & 0.24761905 & 0.04761905 &   \textbf{0.2952381}\\
  \hline
  \texttt{total} &  \textbf{0.552381} & \textbf{0.447619} & 1
  \end{array}$$


De la même façon, les **fréquences marginales** (marges divisées par la taille de la pop.), donnent la **probabilité** d'observer un individu de la modalité correspondant à la ligne ou à la colonne considérée.

<font size="4">
Exemple : dans cette population , j'ai 29.5% de chances de tirer un triangle, et 55% de chances de tirer une pièce blanche.
</font>

## Construction de la distribution théorique

Rappel : Probabilité conjointe de deux évènements $A$ et $B$ **indépendants**  
$P(A \cap B) = P(A) \times P(B)$

À partir des **fréquences marginales** précédentes, on obtient pour chaque couple de modalités, la probabilité **théorique**,  celle qui suppose $H_0$, par un simple **produit**.

<font size="4">
Exemple :  Si $H_0$ est vraie, la probabilité d'observer un triangle noir est donnée par: 

$P(triangle \cap noir) = P(triangle) \times P(noir)$

$P(triangle \cap noir) =0.447619 \times 0.2952381  = 0.1321542$

La probabilité théorique d'observer un triangle noir est de 13,2%  
</font>

## Construction de la distribution théorique

On crée un second tableau, dont chaque case vaut le produit des fréquences marginales calculées sur le tableau des observations.

$$\begin{array}{c|c|c|c}
   & blanc & noir & \texttt{total}\\ 
   \hline
carré &  0.1788662 &   0.1449433 &  \textbf{0.3238095}\\ 
  \hline
  rond &    0.2104309 &0.1705215 & \textbf{0.3809524}  \\ 
    \hline
  triangle &  0.1630839 & 0.1321542 &   \textbf{0.2952381}\\
  \hline
  \texttt{total} &  \textbf{0.552381} & \textbf{0.447619} & 1
  \end{array}$$

C'est le tableau des **fréquences théoriques**.


## Tableau des effectifs théoriques

On l'obtient en multipliant les fréquences théoriques par la taille de la population observée (ici 105)


$$\begin{array}{c|c|c}
   & blanc & noir \\ 
   \hline
carré &  18.78095 &  15.21905 \\ 
  \hline
  rond &   22.09524 & 17.90476    \\ 
    \hline
  triangle & 17.12381 & 13.87619   \\
  \end{array}$$

<font size="4">
N.B. Il n'est pas nécessaire d'arrondir les effectifs théoriques
</font>


## Calcul du Chi 2


C'est la somme, pour chaque case du tableau de contingence (i.e. pour chaque couple de modalités),  des écarts carrés entre effectif observé et effectif théorique**, divisés par l'effectif théorique.

Soient $T^{obs}$ le tableau des effectifs observés, $T^{theo}$ le tableau des effectifs théoriques, 


<center>
$\chi^2 =  \sum_{i,j}  \frac{( T^{obs}_{i,j} -  T^{theo}_{i,j})^2}{T^{obs}_{i,j}}$       
</center>

ici : $\chi^2 = 26.30329$


## Interprétation du Chi 2 

Il faut comparé la valeur du $\chi^2$ calculée avec la **valeur critique** qu'on trouve   dans une **table de loi de Student** (ou table de loi du chi 2).

C'est un tableau à double entrée : une **valeur de quantile**, et un **degré de liberté**.

On peut considérer que la valeur de quantile est le pourcentage d'erreur qu'on s'autorise de faire.  On prend souvent **5%** :  la colonne 1-0.05 =  0.95

Le degré de liberté est obtenu en calculant la valeur $(nb\_lignes - 1)*(nb\_colonnes -1)$.

Dans notre exemple , le degré de liberté est 2*1 = 2


## Table de loi de Student 

<center>
<img src="table.loi.de.student.png", width=100%></img>
</center>


## Interprétation du Chi 2 


D'après le tableau de la loi de Student , la valeur critique pour un test avec 5% de chances de se tromper est un degré de liberté de 2 vaut 4.303.

Si la valeur calculée du $\chi^2$ est **supérieure** à la valeur critique, on **rejette** $H_0$.



Pour notre exemple: On rejette $H_0$, i.e. les deux variables sont **dépendantes**,  car $\chi ^2 \approx 26 > 4.303$

Interprétation: «la forme est liée à la couleur dans cette population, nous pouvons l'affirmer avec un risque d'erreur d'au moins  5%»


## Les étapes du $\chi ^2$

- Tableau de contingence
- Sommes marginales 
    - Calcul des fréquences observées 
    - Calcul des fréquences théoriques
- Tableau d'effectifs théoriques
- Calcul de la valeur du test 
- Comparaison avec les valeurs de la table de Student 



# Lien entre une variable qualitative et une variable quantitative.


## Représentation graphique.

Pas de moyen **simple** de calculer le lien entre une variable qualitative et une variable quantitative.
  
  - corrélation de rang 
  - régression logistique 
  - analyse de la variance (ANOVA)
  
$\implies$ Alors on fait un graphique ! 

La variable qualitative sert de **catégorie**, on fait varier la représentation graphique de la variable quantitative suivant cette catégorie.

2 possibilités :

  - boîtes à moustaches
  - superposition d' histogrammes / densités 

## Les boîtes à moustaches 

"Boîte à moustaches", aussi appellées "boxplot", montre les quartiles d'une variable

Quartile = «quantile de 4», valeurs qui séparent une variable quantitative en quarts, i.e. paquets d'un quart de l'effectif

- Premier quartile : sépare les 25% inférieurs des valeurs
- Second quartile : **mediane**, sépare les 50% inférieurs des valeurs
- troisième quartile : sépare les 75% inférieurs des valeurs


Dans R, fonction `quantile` 

## Boxplot et Distribution


```{r, echo=FALSE, cache=TRUE, warning=FALSE, fig.width=8, fig.height=6}
library(ggplot2)
library(cowplot)


n <-  4000
xval <-  rbeta(n, shape1 = 2, shape2 = 5) * 1000
mydf <-  data.frame(x=xval)
xmean <-  mean(xval)
xsd <-  sd(xval)

xlabels <- data.frame(
  x = c(quantile(mydf$x)),
  y = c(rep(330,5) ),
  text = paste0(c("","Q1: ", "Q2: ", "Q3: ", ""),names(quantile(mydf$x)))
)
distriPlot <-  ggplot(mydf, aes(x=x))+
  geom_histogram(bins = 50, fill="#0FAF96", color="grey")+
  geom_vline(data = xlabels, xintercept = xlabels$x, linetype=c(2,2,1,2,2))+
  geom_label(data= xlabels, aes(x=x,y=y,label=text) )+
  xlab("X")+
  ylab("Effectif")

# Marginal density plot of x (top panel) and y (right panel)
yplot <- ggplot(mydf, aes(y = xval))+
  geom_boxplot(outlier.alpha = 0.2)+
  coord_flip()+
  xlab("")+
  ylab("")+
  theme_void()

#assemblage
plot_grid(yplot, distriPlot,  ncol = 1, align = "hv",scale = c(1,1), 
          rel_widths = c(1, 1), rel_heights = c(1, 3))
```



## Boxplot par catégories


Principe : on trace un boxplot par modalité de la variable qualitative.

E.g. : consommation de véhicules par type (dataset `mpg` de R)


```{r, echo=FALSE, warning=FALSE}
p <- ggplot(mpg, aes(class, hwy))
p + geom_boxplot(outlier.alpha = 0.2) +xlab("type de véhicule (var. qualitative)") +ylab("consommation (var. quantitative")+theme_light()
```


# Références supplémentaires


## Refs


Cours complet sur les modèles linéaires : [https://www.math.univ-toulouse.fr/~barthe/M1modlin/poly.pdf]


Interprétation des graphique de `lm` en R : [https://data.library.virginia.edu/diagnostic-plots/]





