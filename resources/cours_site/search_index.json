[["index.html", "Analyse Statistique M2 IGAST Préambule Programme du cours et Contenu Reférences Ressources pour l’apprentissage du langage R", " Analyse Statistique M2 IGAST PC 2021-03-01 Préambule Ce document s’adresse en priorité aux étudiants du Master 2 IGAST de l’ENSG, dans le cadre du Module intitulé “Analyse Statistique,” dirigé par Ana-Maria Olteanu-Raimond. Programme du cours et Contenu Ce cours comporte une introduction générale pour situer les statistiques qui y sont abordées. Dans cette introduction, on parle beaucoup d’analyse spatiale, parce que c’est dans ce contexte que les rudiments de statistiques exposés ici vont etre utilisés par les étudiant.e.s du M2 IGAST, mais aucune technique purement spatiale n’est abordée ici. Le programme stricto-sensu est le suivant : Analyse Univariée: Vocabulaire Description de distribution : indicateurs de tendance centrale, quantiles, dispersion , symétrie , applatissement Analyse Bivariée: regression linéaire : R2, p-value corrélation : r de Pearson , rho de Spearman, hypothèse nulle Test du Chi 2 : Construction du tableau d’effectif théorique, calcul du Chi 2 , hypothèse nulle Le reste, c’est du bonus ! Reférences Pour constituer ce cours, j’ai utilisé les références suivantes: Cours M2 IGAST 2018 d’Ana-Maria Olteanu-Raimond Cours M2 IGAST 2017 d’Élodie Buard Probabilités, analyse de données et statistiques , Gilbert Saporta, Editions TECHNIP, 2011 l’excellent cours de Hadrien Commenges https://gitlab.huma-num.fr/hcommenges/cours_statcomplet/-/raw/master/cours_statcomplet.pdf Nombreuses ressources en ligne, parmi lesquelles : http://www.foad-mooc.auf.org/IMG/pdf/424B_-Application_des_methodes_statistiques_d_analyse.pdf http://www.itse.be/statistique2010/co/Module_statistique_FSP.html J’ai également fait un usage immodéré de Wikipedia, Google, et StackOverflow Pour les principes de la visualisation, j’essaye de suivre quelques uns des conseils prodigués par Claus O. Wilke dans son ouvrage Fundamentals of Data Visualization Ressources pour l’apprentissage du langage R R est un langage qui s’apprend «au fil de l’eau», en trouvant des exemples de manipulations et des réponses à des questions sur des forums et sites spécialisé (Stackoverflow principalement). Cependant, l’étudiant.e IGAST désireuse d’aller droit au but et de manipuler des données spatiales avec R pourra se référer en priorité à : R et espace https://framabook.org/r-et-espace/ l’excellente formation à R du Ministère de la Transition Écologique et Solidaire : https://mtes-mct.github.io/parcours-r/ notamment le module 7 : « Analyse spatiale » la présentation du package sf sur https://r-spatial.github.io/sf/articles/sf1.html le livre https://rcarto.github.io/carto_avec_r/ très complet pour la cartographie. "],["intro.html", "Chapitre 1 Introduction générale 1.1 Analyse spatiale : définition 1.2 Analyse spatiale &amp; Analyse Statistique 1.3 Deux approches en analyse spatiale 1.4 Deux familles statistiques 1.5 Vocabulaire 1.6 Difficultés de la statistique 1.7 À quelle échelle observer ? le MAUP 1.8 Rappel: La première “chose à faire”", " Chapitre 1 Introduction générale Commençons par le plus important : En présence de données, , avant tout calcul, la première chose à faire est de les visualiser. Cette exploration visuelle permet: d’identifier les intervalles de valeurs des variables, ou leur modalités de se faire une idée de la répartition des valeurs des variables à l’intérieur de leurs intervalles d’identifier des valeurs aberrantes d’observer pour les variables spatialisées , leur répartition dans l’espace en R, pour tracer directement tout un dataframe (=jeu de données) , on utilise la commande plot library(palmerpenguins) #charge un jeu de données nommé &#39;penguins&#39; plot(penguins) #affiche une matrice de graphique Avec un peu d’expérience, on identifie rapidement des caractéristiques utiles. par exemple ici : les deux premières variables sont des variables qualitatives à trois modalités les variables suivantes sont quantitatives, les deux dernières semblent corrélées positivement les deux dernières variables sont qualitatives, à deux et trois modalités Ce cours a pour but de vous donner le vocabulaire et les outils nécessaires à préciser le sens de ces caractéristiques, et d’en chiffrer certaines propriétés. L’analyse statistique ne se confond pas avec l’analyse spatiale, bien qu’elles soient parfois cousines. Nous allons commencer par préciser ce qu’on entend par analyse spatiale et statistique, sachant que ce support de cours se concentre sur les prémices de l’analyse statistique : l’analyse univariée (cf section 2) et l’analyse bivariée (cf. section 5) 1.1 Analyse spatiale : définition L’analyse spatiale étudie la répartition et l’organisation d’objets localisés L’objectif est de : «déceler en quoi la localisation apporte un élément utile à la connaissances des objets étudiés et peut en expliquer les caractéristiques» — [Pumain, Saint-Julien 97] 1.2 Analyse spatiale &amp; Analyse Statistique L’analyse statistique peut être vue comme l’utilisation de méthodes de calcul pour résumer et généraliser des observations. Ces observations constituent les données. En analyse statistique : On suppose a priori que les unités d’analyse sont des éléments indépendants, et on examine avec des outils statistiques, si les variables qui les décrivent sont elles-aussi indépendantes. On ne s’intéresse pas à la localisation de ces unités d’observation, ni à leur interactions spatiales. En analyse spatiale statistique, au contraire : Les unités d’analyse sont localisables, et donc ne sont pas toujours indépendants du point de vue spatial. On s’intéresse à leur propriétés y compris et surtout leur localisation et l’effet de celle-ci : on fait l’hypothèse que la localisation des unités peut influencer les valeurs des variables des unités observées. Prenons des exemples volontairement simplistes: En statistiques «normales», on peut examiner le lien entre revenu mensuel et la pointure d’une population: ce lien existe-t-il ? Si oui, quelle est son intensité ? En statistiques spatiales, on peut examiner la répartition des personnes de pointure supérieur à 42 dans les zones littorales : cette répartition a-t-elle une répartition particulière ou au contraire aléatoire ? Quelles régularités peut-on observer dans cette répartition ? , etc. 1.3 Deux approches en analyse spatiale L’analyse spatiale peut désigner deux sortes d’analyse : l’analyse géométrique et l’analyse de données spatiales. L’ analyse géométrique se concentre sur la géométrie (i.e. la forme) des représentations numériques qui décrivent les objets du monde réel : analyse de forme : aire , périmètre, compacité, etc. analyse de réseaux : densité, centralité des nœuds, indice de Shimbel, (voir par exemple https://groupefmr.hypotheses.org/3740) calculs de proximité (distance euclidienne, grands cercles, distance sur réseau, isochrones) Par extension l’analyse géométrique concerne également la création d’objets géométriques : buffers , intersections , unions, etc. L’analyse de données (spatiales) , elle , s’attache à découvrir des relations (des groupes, des lois, des régularités) dans des données spatiales pour aider l’étude de certains phénomènes. En guise d’exemple, on pourra se référer à ce qui est peut être la toute première analyse spatiale par cartographie : l’ étude de la propagation du choléra par John Snow (#fig:img_john_snow)Original map made by John Snow in 1854. Cholera cases are highlighted in black Dans ce cours, nous n’aborderons ni l’une ni l’autre. Nous donnerons des bases d’analyse statistique (donc a-spatiale), dont pourra dépendre une partie de l’analyse de données spatiales 1.4 Deux familles statistiques On peut distinguer dans les statistique les statistiques inférentielles et les statistiques descriptives . 1.4.1 Statistiques inférentielles Les statistiques inférentielles cherchent à caractériser une population (=ensemble d’unités) à partir des caractéristiques d’un échantillon pour lequel on a recueilli des données : par des mesures, des observations, des enquêtes. Les statistiques inférentielles cherchent à répondre de façon mathématiquement rigoureuse à la question suivante : « A partir d’un échantillon , que peut-on attendre (=inférer) de la population ? » Cette famille de statistiques emploie des modèles et des estimateurs pour réaliser des régressions, des estimations, des extrapolations, voire des prévisions. Ce sont les statistiques qui sont utilisées lors des sondages, des recensements, mais aussi dans l’analyse de résultats expérimentaux (par ex. efficacité d’un médicament). La plupart du temps quand on emploie le terme «statistiques» dans le langage courant, c’est de statistiques inférentielles qu’il s’agit. 1.4.2 Statistiques inférentielles : l’exemple des pingouins Penguins data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER, a member of the Long Term Ecological Research Network. [https://github.com/allisonhorst/palmerpenguins] Dans cet exemple, une régression linaire a été réalisée entre deux variables : la masse du corps de pingouins et la longueur de leurs nageoires. Pour autant que cette régression linaire soit de bonne qualité (cf la section dédiée dans le chapitre 5, la droite de régression, ici en bleu, est un modèle linéaire qui permet , à partir de la valeur d’une des deux variables, de déduire la valeur de l’autre. Par exemple, si on était en présence d’un pingouin avec une règle graduée , mais sans balance, et qu’on mesurait sa nageoire à 197mm, on peut se servir de la droite de régression pour postuler que ce pingouin pèse 4000g. C’est ce calcul qui est appelé parfois prédiction : le modèle linéaire, s’il est bien ajusté au nuage de points , permet de prédire la valeur de la masse du pingouin. 1.4.3 Statistiques descriptives Les statistiques descriptives ont pour but de décrire, résumer, synthétiser les propriétés d’une population, potentiellement très nombreuse, à partir des variables qui décrivent ses individus. Ce résumé peut prendre plusieurs formes : Graphiques : nuages de points , histogrammes, distributions, camemberts… Mesures agrégées : moyenne , médiane, fréquences, caractéristiques des distributions, par exemple leur symétrie, leur aplatissement … calculées à partir des valeurs des variables Liaisons statistiques entre variables : corrélation, covariance, qui sont des mesures de l’intensité du lien qui peut exister entre deux variables Forme et structure des données, par exemple des regroupements, détectées par classification , Analyse en Composantes Principale,… Voici des exemples de graphiques habituels : Le nuage de points est la représentation la plus courrante pour observer deux variables d’une population. L’histogramme est utilisé pour observer la répartition des valeurs d’une variable. Il est recommandé de faire varier le nombre ou l’épaisseur des barres qui le constituent pour ne pas «manquer» des phénomènes dans la distribution de valeurs (par ex. plusieurs modalités fondues en une seule du fait d’une résolution trop grossière) la densité d’une distribution est un autre moyen de représenter la répartition des valeurs d’une variable quantitative? Pour l’interprétation d’un tel graphique, reportez-vous à la section dédiée @ref{interpret_dens} du chapitre @ref{univariee} Ce type de représentation est très populaire malgré des défauts certains (voir par exemple ; https://www.data-to-viz.com/caveat/pie.html) , il est à utiliser avec parcimonie! Dans ce module, nous ferons majoritairement de la statistique descriptive, notamment dans le chapitre d’Analyse Univariée 2. Dans le chapitre d’Analyse Bivariée 5, nous verrons la régression linéaire, qui, lorsque qu’elle est «utile» – c’est à dire que les deux variables présentent effectivement une dépendance suffisamment linéaire pour être bien décrite par une droite– peut être considérée comme une technique de statistique inférentielle , et utilisée pour «prédire» des valeurs, même si nous préférerons parler d’explication plutôt que de prédiction. 1.4.4 Les formats de données wide et long Les données sont la plupart du temps tabulaires, et peuvent apparaître sous deux formats : le format dit long, où chaque observation d’un même individu occupe une ligne le format dit wide, où chaque observation d’un même individu occupe une colonne Exemple de données au format wide: ## subject_ID sex mass measure_t1 measure_t2 ## 1 1 M 7.9 12.3 10.7 ## 2 2 F 6.3 10.6 11.1 ## 3 3 F 9.5 13.1 13.8 ## 4 4 M 11.5 13.4 12.9 Exemple des mêmes données au format long ## subject_ID sex measure value ## 1 1 M pct 7.9 ## 2 2 F pct 6.3 ## 3 3 F pct 9.5 ## 4 4 M pct 11.5 ## 5 1 M measure_t1 12.3 ## 6 2 F measure_t1 10.6 ## 7 3 F measure_t1 13.1 ## 8 4 M measure_t1 13.4 ## 9 1 M measure_t2 10.7 ## 10 2 F measure_t2 11.1 ## 11 3 F measure_t2 13.8 ## 12 4 M measure_t2 12.9 1.5 Vocabulaire Population : Ensemble d’individus. Le nombre d’individus d’une population est appelé l’effectif. On parle aussi de “données,” “corpus,” “échantillon,” “data,” “dataset” . Individus : l’individu est l’unité statistique élémentaire. On parle parfois d’observations, en particuliers lorsqu’on mesure une évolution de quelque chose dans le temps. Très souvent, les individus sont les lignes du tableau des données Variables : les variables sont les caractéristiques d’un individu. On obtient les valeurs des variables par des mesures, des enquêtes, des observations… Très souvent, les variables sont les colonnes du tableau des données. On distingues deux types de variables : les variables quantitatives et les variables qualitatives 1.5.1 Variables quantitatives Ce sont des variables qui représentent une quantité, une grandeur. Ce sont des nombres , et ils sont parfois accompagnés d’une unité. Par exemple : la taille, la masse, le revenu mensuel, la surface, les points de vie,etc. Les variables quantitatives peuvent être continues i.e. prendre des valeurs réelles: \\(var \\in \\mathbb{R}\\) , ou discrètes i.e. prendre des valeurs entières : \\(var \\in \\mathbb{N}\\) . 1.5.2 Variables qualitatives appelées aussi facteurs ou variables catégorielles. Ce sont des variables dont les valeurs ne sont pas en général des nombres, mais des catégories, qu’on appelle modalités. Par exemple : la couleur des yeux , le genre, la catégorie socio-professionnelle, le type de pokemon, etc. Une variable qualitative ne peut prendre qu’un nombre fini de modalités, qui sont définies par extension (i.e. on donne la liste des valeurs possibles) Il peut arriver qu’une variable qualitative soit codée avec des valeurs numériques, par exemple le niveau d’une alerte ou le code postal d’un département. 1.5.2.1 variables qualitatives nominales Les modalités ne sont pas ordonnées explicitement e.g. situation matrimoniale \\(\\in\\) {marié·e, célibataire, veuf·ve} 1.5.2.2 variables qualitatives ordinales Les modalités sont ordonnées selon un ordre total et non-ambigu. e.g. Echelle de Likert (5 ou 7 valeurs, dont une neutre) satisfaction \\(\\in\\) {Très satisfait, Satisfait, Ni satisfait ni insatisfait, Peu satisfait, Pas du tout satisfait} 1.5.3 Valeur et Nature des variables Les données étant stockées sous la forme de caractères dans des fichiers par exemple CSV, un même caractère ou suite de caractères pourra être interprété de différentes façons à la lecture de ce fichier. Ce n’est pas le cas dans d’autres systèmes comme les bases de données, où les colonnes sont typées à la création. Ainsi la valeur «trois» peut être exprimée dans des variables de nature différentes. Cela peut-être : la valeur numérique ‘3’ , d’une variable quantitative, par exemple une hauteur en mètres. la valeur caractère ‘3,’ d’une variable qualitative, par exemple l’arrondissement de paris où se trouve une adresse. Un logiciel ne peut pas trancher seul entre ces deux alternatives, on doit lui indiquer la nature des variables, en l’occurence pour un logiciel de statistiques , indiquer si la variable est quantitative (de type numeric ou integer en R) ou qualitative (de type character ou factor en R). Également, les valeurs manquantes d’un dataset sont parfois exprimées avec les caractères NA (qui signifie “non attribué”), parfois avec NULL , parfois avec rien. Il revient à la personne qui traite les données d’identifier ces valeurs manquantes et de les traiter de façon adéquate. 1.5.4 Types de variables et représentations La représentation des données doit être adaptées au type de variables. La table suivante indique les choix de représentations conventionnels. Type Échelle/Axes quantitative continue continue quantitative discrète discrète qualitative/modale non ordonnée discrète qualitative/modale ordonnée discrète dates continue ou discrète texte aucune ou discrète tiré de Fundamentals of Data Visualization Claus O. Wilke [https://serialmentor.com/dataviz/] 1.6 Difficultés de la statistique Nous donnons ci-dessous quelques exemples de difficultés inhérentes à l’analyse statistique. 1.6.1 Plusieurs discours sont possibles Les statistiques sont délicates parce qu’un.e statisticien.ne ne peut pas se limiter à des calculs et à la représentation graphique de ses données, mais doit également les décrire et les interpréter. Cette interprétation est elle-même difficile car des discours différents peuvent être produits sur la base de chiffres identiques. Considérons cet exemple, tiré du cours d’Ana-Maria Olteanu-Raimond : Voici les chiffres du bilan d’une entreprise en 2013 et 2014 Ouvriers Cadres 2013 effectif : 500 effectif : 100 salaire : 1300 salaire :2200 2014 effectif : 200 effectif : 400 salaire :1170 salaire : 1980 Mme. AAA : «tous les salaires ont baissé de 10%» Mme. BBB : «le salaire moyen a augmenté d’environ 18%» Ces deux phrases sont justes, et il n’y a pas lieu d’en choisir une plus que l’autre. 1.6.2 Taille et représentativité de l’échantillon Les données ne sont jamais exhaustives: elles sont le résultats de mesures, d’enquête et ne captent qu’une partie, un échantillon de l’ensemble des objets qu’elles décrivent. La taille et la représentativité de cet échantillon sont critiques, et il est évident qu’on ne dit pas les mêmes choses à partir d’un petit échantillon partial qu’à partir d’un vaste échantillon représentatif. Quand on en a la possibilité, il faut toujours privilégier les données les plus nombreuses et les plus représentatives du phénomène étudié. 1.6.2.1 Exemple de l’effet de la taille Prenons un exemple avec des données “réelles” , on va calculer la hauteur moyenne des arbres de paris , dont le jeu de données est disponible ici Après filtrage des données pour ne considérer que les arbres entre 1 et 40 mètres de hauteur, on va calculer la hauteur moyennes des arbres du dataset: library(geojsonsf) library(sf) arbres &lt;- geojson_sf(&quot;./data/les-arbres.geojson&quot;) arbres &lt;- filter(arbres, hauteurenm &lt; 40 &amp; hauteurenm &gt; 1 ) mean(arbres$hauteurenm) ## [1] 10.28588 On va maintenant opérer une selection sur les données et ne considérer que les arbres étiquettés comme “jeunes” (variable statdedeveloppement), et calculer leur hauteur moyenne : arbresJeunes &lt;- arbres %&gt;% filter( stadedeveloppement ==&quot;J&quot;) mean(arbresJeunes$hauteurenm) ## [1] 5.691254 Évidemment , la hauteur moyenne des jeunes arbres n’est pas la même que la hauteur moyenne des arbres en général! Observons maintenant l’effet que peut avoir la taille de l’échantillon sur un calcul de moyenne : On va prendre des échantillons du dataset des arbres de Paris de plus en plus petits (on divise par deux la taille de l’échantillons à chaque fois) et on va observer la variation de la hauteur moyenne des arbres en fonction de la taille de l’échantillon sur lequel on calcule. Pour chaque taille d’échantillon, on réalise plusieurs tirages (50) et on observe la dispersion des valeurs moyennes de la hauteur en mètres sur 50 tirages , pour montrer la variabilité de la valeur moyenne A titre indicatif, voici un moyen de coder simplement cette expérience en R : nb_samples &lt;- nrow(arbres) samples &lt;- c() while(nb_samples &gt; 1){ nb_samples &lt;- floor(nb_samples/2) samples &lt;- c(samples, nb_samples) # concaténation des listes } # fonction qui tire un échantillon d&#39;une certaione taille dans une liste myfunction&lt;- function(taille,data){ return(sample(data,taille,replace = F)) } # tirage simple samples_hauteur &lt;- sapply(samples, myfunction, arbres$hauteurenm) valeurs_moyennes &lt;- sapply(samples_hauteur, mean) names(valeurs_moyennes) &lt;- samples %&gt;% as.character() #on répète 50 fois le tirage spls_sizes &lt;- rep(samples, 50) spl_h &lt;- sapply(spls_sizes, myfunction, arbres$hauteurenm ) spl_moy &lt;- sapply(spl_h, mean) # constitution du dataframe dfhmoy &lt;- data.frame(spl_moy, spls_sizes) dfhmoy$spls_sizes &lt;- factor(dfhmoy$spls_sizes, levels=samples) names(dfhmoy) &lt;- c(&quot;h_moy&quot;, &quot;spl_size&quot;) #graphique ggplot(dfhmoy ,aes(h_moy, spl_size))+ geom_boxplot()+ coord_flip()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ labs(x=&quot;hauteur moyenne en mètre&quot;, y=&quot;taille de l&#39;échantillon&quot;, title=&quot;Sensibilité de la moyenne à la taille de l&#39;échantillon&quot;, subtitle = &quot;On tire 50 échantillons de chaque taille.&quot;) Ce graphique présente sous la forme d’un boxplot (ou boîte à moustaches), la distribution de la valeur des moyennes des hauteurs, calculées sur des échantillons dont la taille varie de 84508 individus, soit la moitié du dataset des arbres filtrés, à 1 individu. Le boxplot montre l’étendue de la variation d’une variable quantitative Pour chaque taille d’échantillon, on réalise 50 tirages pour éviter les “coups de chance,” et on calcule la moyenne de chaque échantillon. On a donc calculé 50 valeurs de moyenne , pour chaque taille d’échantillon. Plus la boîte à moustache est grande (étirée), plus les valeurs de la variable qu’elle représente sont variées. Le trait noir au milieu représente la médiane des valeurs. Sur notre graphique, on constate que la médiane des moyennes est très stable et est toujours proche de 10, mais que les tailles des boxplots sont très petites pour les échantillons de grande taille(indiquant que la moyenne des hauteurs varie peu), et qu’au contraire , pour des tailles d’échantillons faibles (moins de 100 individus), la moyenne des hauteurs varie plus fortement. Nous aborderons l’interprétation des boxplots dans le chapitre 2 de façon plus précise. Pour finir de se convaincre de ces variations, voici la représentation alternative des valeurs moyennes en fonction de la taille des échantillons sous la forme d’un nuage de point: ggplot(dfhmoy ,aes(h_moy, spl_size))+ geom_point(col=&quot;orange&quot;, size=0.5)+ coord_flip()+ theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust=1))+ labs(x=&quot;hauteur moyenne en mètre&quot;, y=&quot;taille de l&#39;échantillon&quot;, title=&quot;Hauteur moyenne en fonction de la taille d&#39;échantillon&quot;, subtitle = &quot;On tire 50 échantillons de chaque taille.&quot;) 1.6.3 Le paradoxe de Simpsons Le paradoxe de Simpsons apparaît lorsqu’une population est structurée en groupes et que deux hypothèses contradictoires peuvent être formulées, suivant qu’on prenne en compte les appartenances des individus à ces groupes ou non. source: wikipedia Par exemple, le nuage de points animé ci-dessus, illustre comment on peut formuler deux hypothèses contradictoire concernant la même population : \\(H_A\\) : la variable \\(x\\) est corrélée linéairement et négativement avec la variable \\(y\\) \\(H_B\\) : la variable \\(x\\) est corrélée linéairement positivement avec la variable \\(y\\) \\(H_A\\) est valable si l’on considère les valeurs de la population dans son ensemble et sans tenir compte des groupes qui la structurent. À l’intérieur de chaque groupe, \\(H_B\\) est valable et contredit \\(H_A\\). 1.6.4 Échelle individuelle vs. Échelle agrégée Lorsqu’on manipule des données spatiales, le territoire sur lequel les données sont disponibles peut être découpé de plusieurs façons. Souvent, ces découpages sont imbriqués : la maille d’un niveau \\(n\\) est partionnée en plusieurs mailles au niveau \\(n-1\\). Par exemple, la France se découpe en régions, elles-mêmes décomposées en départements, eux-mêmes décomposés en commune. La difficulté est qu’un jeu de données spatial est défini et disponible à un certain niveau de découpage spatial, et que considérer ces mêmes données à un niveau de découpage plus fin ou plus grossier nécessite une transformation : l’agrégation ou la désagrégation. L’agrégation est relativement directe , la désagrégation l’est beaucoup moins. Par exemple, on peut disposer du revenu mensuel moyen des foyers par commune, mais on ne dispose pas des données “brutes” qui y ont conduit: les revenus des personnes qui composent les foyer des communes. Si on désire considérer la même variable à un niveau de découpage plus grossier (ici, les régions) ou plus fin (ici, les communes), on devra agréger ou désagréger les données. 1.6.5 Agrégation On parle d’agrégation lorsqu’on désirer regrouper des données disponibles à un niveau de découpage donné au niveau de découpage supérieur (plus large, par exemple , regroupper les données de communes à l’échelle des départements.) Cela revient à inférer des caractéristiques concernant les unités agrégées d’après les caractéristiques individuelles. En fonction des données manipulées, l’agrégation peut se faire par la somme (par exemple pour des stocks) la moyenne (par exemple pour des ratios), ou tout autre combinaison bien choisie. 1.6.6 Désagrégation ou Ventilation On parle de désagrégation ou de ventilation de données lorsqu’à partir de données disponibles à un niveau de découpage donné, on cherche à simuler une répartition de données à un niveau de découpage plus fin, tout en restant cohérent avec la répartition des données au niveau de découpage originel. Solution triviale donc sans intérêt: Pour une variable de stock \\(V_s\\) (on dit aussi variable d’effectif) , i.e. mesurant une quantité d’objets, et en l’absence de toute autre données ou contraintes, on peut affecter aux \\(n\\) communes du département un nombre constant valant \\(\\frac{V_s}{n}\\). Pour une variable de ratio \\(V_r\\), on peut affecter le même ratio \\(V_r\\) à toutes les mailles qui composent la maille à désagréger. Si on écarte la solution triviale, la désagrégation de données est une opération complexe, pour lesquels il existe cependant plusieurs méthodes de génération de population synthétique respectant certaines contraintes, comme l’Iterative Proportional Fitting Les choses se compliquent encore si les découpages ne sont pas imbriqués les uns dans les autres. Généralement, on agrège les valeurs par des sommes pondérées par la surface des entités spatiales concernées. Un bon aperçu des méthodes d’allocation spatiale est donné dans ce document 1.7 À quelle échelle observer ? le MAUP Puisqu’une même zone peut être découpée de différentes façons, avec des mailles de taille variable, susceptibles d’évoluer dans le temps (par exemple les limites adminsitratives des communes en France), toute question d’analyse spatiale (et donc d’analyse spatiale statistique) comporte une question spécifique à l’échelle d’observation : À quelle échelle observer nos données ? Cette question peut se décomposer en deux sous-questions, liées : quel maillage utiliser ? Par exemple: une maille régulière carrée ou hexagonale, les contours adminsitratifs, les mailles IRIS ? quelle taille de maille choisir ? Il est très difficile de répondre à ces questions dans l’absolu, c’est même un problème «insoluble» appelé le MAUP (Modifiable Areal Unit Problem). Les raisons de cette impossibilité de déterminer le découpage idéal pour observer une donnée spatialisée sont nombreuses. En voici quelques unes, qui je pense sont les principales: La plupart des phénomènes d’intérêts sont multi-scalaires, c’est-à-dire qu’ils peuvent se produire et/ou s’observer à plusieurs niveaux d’échelle. L’agrégation peut faire disparaitre certains détails, certaines spécificités dans la cartographie des données. Des effets de la géométrie du découpage (appelés effets de zonage) modifie la représentation cartographique des données Voici quelques exemples qui montrent ces difficultés. 1.7.1 Effet de zonage Cette image tirée de la page wikipedia gerrymanding, illustre bien comment un découpage d’une même zone peut amener à des résultats de vote différents. (#fig:img_gerryManding)Différentes manières de découper des circonscriptions électorales. 1.7.2 MAUP : exemples Ces exemples sont tirés du rapport ESPON : https://www.espon.eu/sites/default/files/attachments/espon343_maup_final_version2_nov_2006.pdf Vous pouvez noter comme les zones du Sud en découpage NUTS 2 , sont représentées uniformément rose (valeur max du GDP/hab) alors qu’avec un découpage plus fin (NUTS 3), la situation du sud est loin d’être aussi uniforme : on note par exemple que la moitié Est des mailles du Sud-Est a des teintes plus mélangées dans le découpage NUTS3, avec un mélanges de mailles jaunes et bleues, que dans le découpage NUTS2 . De même, les zones frontalières du Luxembourg et de la Belgique apparaissent plus pauvres (teintes jaunes et bleues) dans le découpage NUTS 3 que dans le découpage NUTS2. De manière générale : l’agrégation va “lisser” les aspérités des données, certains contrastes ne «survivent pas» à l’agrégation spatiale. Dans ce deuxième exemple, on voit la forme et la taille des mailles changer. Là aussi, un effet de lissage, indissociable de l’agrégation, fait apparaitre certaines régions comme uniformes et cachent certains singularités: par exemple le fait que les zones les plus lointaines du littoral sont pauvres (teintes oranges à l’arrière du pays, Nord et Nord-Ouest) dans le découpage en mailles carrées de 30km n’apparait pas dans le découpage administratif(les deux vignettes de gauche), tout en longueur. 1.8 Rappel: La première “chose à faire” Représenter/Tracer/Cartographier les variables de la population. Cette exploration visuelle permet: d’identifier les intervalles de valeurs des variables, ou leur modalités de se faire une idée de la répartition des valeurs des variables à l’intérieur de leurs intervalles d’identifier des valeurs aberrantes d’observer pour les variables spatialisées , leur répartition dans l’espace D’autre part, comme le montre l’exemple ci-dessous, les valeurs de statistiques peuvent être trompeuses, et discriminent beaucoup moins bien des situations qualitativement très différentes que notre œil : Les valeurs chiffrées affichées sont la moyenne, l’écart-type et la corrélation des deux séries qui forment le nuage de points. Le chapitre suivant introduira ces notions. "],["univariee.html", "Chapitre 2 Analyse Univariée 2.1 Le concept de distribution 2.2 Exemples de distributions de lois connues 2.3 Histogramme d’une distribution réelle 2.4 Afficher histogrammes et distributions en R 2.5 La Tendance 2.6 La Dispersion 2.7 La Forme", " Chapitre 2 Analyse Univariée 2.1 Le concept de distribution L’analyse univariée a pour but de décrire et mesurer la répartition des valeurs que peut prendre une variable. On appelle la répartition des valeurs d’un variable sa distribution , que l’on peut très approximativement voir comme son «histogramme en continu». Voilà une distribution d’une variable réelle (courbe noire), superposée à son histogramme : Vous pouvez voir avec cet exemple, que la courbe suit les variations de hauteur des colonnes de l’histogramme, tout en lissant les aspérités. C’est de cette courbe que l’on parle lorsqu’on évoque la distribution de la variable. (Il serait plus rigoureux de parler de courbe de densité de probabilité.) Plus précisément, la distribution représente des probabilités (ou des densités de probabilité), et non des effectifs. En toute rigueur , représenter une courbe de distribution de probabilité par dessus un histogramme est impropre : il faudrait deux axes des ordonnées: un pour l’histogramme, représentant un effectif, l’autre pour la distribution , représentant une probabilité (variable à valeurs discrète) ou une densité de probabilité (variable à valeurs réelles) Formellement, pour une population donnée, la distribution d’une variable \\(V\\) peut être définie comme une fonction qui donne la probabilité qu’un individu \\(x\\) pris au hasard dans la population ait la valeur \\(V_x\\) pour la variable \\(V\\) : \\[distribution(V) \\equiv P(V=V_x), \\forall V_x \\in \\Omega_V\\] avec \\(\\Omega_V\\) l’ensemble des valeurs que peut prendre \\(V\\) : l’univers de \\(V\\). Lorsque la variable prend des valeurs réelles, on parle de densité de probabilité, c’est pourquoi on retrouve ce terme “density” sur les axes des ordonnées dans les graphiques de distribution. La forme d’une distribution donne beaucoup d’informations sur les valeurs d’une variable dans une population : valeurs les plus représentées dans la population : les “pics” présence de valeurs extrêmes : la courbe de la distribution est tirées à gauche ou à droite du graphique caractéristiques de sa forme : symétrie, aplatissement etc… Dans notre exemple de distribution de longueur de nageoires en millimètres, on observe deux pics assez doux : l’un aux alentours de 190mm, l’autre de 215mm. On peut l’interpréter ainsi : «la valeurs la plus représentée dans les longueurs de nageoires de cette population de pingouins est de 190mm, suivie de 215mm» 2.1.1 Interpréter la courbe de densité L’histogramme représente l’effectif de la population en fonction de classes de valeur d’une variable, son interprétation est directe et aisée puisque ce graphique donne une représentation du nombre d’individus par intervalles de valeurs. La représentation d’une densité est légèrement plus délicate à comprendre mathématiquement. En première approximation , vous pouvez l’interpréter comme un histogramme dont les barres seraient infiniment fines, et dont l’axe des \\(y\\) représenterait une probabilité au lieu d’un effectif. La densité de probabilité comme son nom l’indique, représente des probabilités: celles d’obtenir, pour un individu dans la population, une certaine valeur de la variable. Le point délicat est qu’une variable continue (i.e. définie sur un intervalle de \\(\\mathbb{R}\\)), peut prendre une infinité de valeurs possibles, et que la probabilité d’obtenir exactement, c’est à dire avec une précision infinie, une valeur est infinitésimale, en fait carrément nulle. Il faut alors considérer la probabilité d’obtenir une valeur, non pas de façon exacte , mais dans un intervalle de valeur. Par exemple, dans le graphique ci-dessous, de s’interroger sur la probabilité , pour un individu tiré au hasard dans la population , d’avoir une valeur de Variable 1 dans l’intervalle \\([25;30]\\). Cela pourrait s’écrire \\(P( 25 \\leq V_1 \\leq 30)\\) et serait égal à l’intégrale de la fonction de densité notée \\(f_{V_1}\\) entre les bornes 25 et 30 de l’intervalle de \\(V_1\\) : \\[P( 25 \\leq V_1 \\leq 30) = \\int_{25}^{30} f_{V_1}(x)dx\\] En pratique , le graphique d’une densité s’interprète en observant la quantité d’aire sous la courbe L’aire totale sous la courbe vaut 1 (cela revient à considérer la somme de toutes les probabilités d’avoir une valeur particulière \\(X\\) dans l’intervalle de valeur de \\(V_1\\)) et par approximation , la valeur de la probabilité d’obtenir une certaine valeur se lit comme la proportion d’aire sous la courbe comprise entre deux bornes proche de la valeur. ## [1] 0.3126056 Par exemple ici , la valeur de \\(P( 25 \\leq V_1 \\leq 30) \\approx 0.312\\) 2.1.2 Bonus: Estimation la probabilité à partir de la densité de probabilité Attention, je ne donne pas ici la méthode d’approximation de la densité, uniquement la façon de calculer une valeur de probabilité à partir d’une densité, avec R. Le principe est assez simple : on approxime l’aire sous la courbe de densité par une somme de Riemann à pas constant. On note: \\(f\\) la fonction dont on cherche à approcher l’aire sous la courbe. On suppose que c’est une fonction continue et définie partout. \\([a;b]\\) le segment sur lequel on veut intégrer \\(f\\) on découpe \\([a;b]\\) en \\(n\\) “tranches” de taille égale, on notes les bornes de ces tranches \\(x_i\\), avec \\(n_0=a, &lt;x_1&lt;\\dots&lt;x_i&lt;\\dots&lt;x_{n-1}&lt;x_n=b\\). Chaque tranche a une largeur, qu’on note \\(\\delta\\), on l’appelle le pas d’intégration . Il est constant. dans chaque tranche \\([x_{i-1};x_i]\\), on considère une valeur \\(t_i, i\\in \\{1,\\dots,n\\}\\), pour laquelle on connaît\\(f(t_i)\\), la valeur de \\(f\\) en ce point. \\(S(f)\\)La somme de Riemann de la fonction \\(f\\), qui approxime l’aire sous la courbe de la fonction \\(f\\) est définie par: \\[S(f)=\\sum _{i=1}^{n}(x_{i}-x_{i-1})f(t_{i})\\] qu’on peut reformuler ainsi : l’intégrale est approchée par la somme des aires des “tranches” rectangulaire, de largeur \\(\\delta=x_i -x_{i-1}\\) et de hauteur \\(f(t_i)\\). Notre pas d’intégration est constant , on peut le sortir de la somme , et simplement calculer la somme des valeurs de densité dans le segment \\([a;b]\\) ! la fonction density() de R retourne un objet avec plusieurs attributs : l’attribut x du résultat est la coordonnées des points où la densité est estimée. par défaut , il y a 512 points. l’attribut y, les valeurs des densités calculées en chaque point de x On peut donc facilement approcher la probabilité d’avoir un individu dont la variable est contenue entre deux valeurs. On calcule la densité d’une variable , ici hwy du dataset mpg data(mpg) dens &lt;- density(mpg$hwy) On calcule le pas d’intégration \\(\\delta\\), en calculant l’écart entre deux points successifs de l’attribut x de la ## [1] 0.08375649 On calcule la somme des valeurs de densité entre deux bornes b1 et b2 ## 0.2562939 On peut normaliser par une constante d’intégration. Comme on approxime une probabilité par des valeurs discrètes, on fait obligatoirement une erreur , car il faudrait que \\(\\delta\\) tende vers 0 pour que (limite de) la somme de Riemann soit égale à l’intégrale \\(\\int_a^bf(t)dt\\). On peut évaluer cette erreur en calculant la somme de Riemann sur toute l’étendue de la variable. Cette valeur s’approche de 1, sa valeur théorique qui serait la probabilité de la variable calculée sur tout son univers. ## [1] 1.000938 On voit qu’on est assez proche de 1 ! On peut normaliser le calcul de la probabilité par cette constante pour tenir compte de cette erreur. ## [1] 0.2560537 On pourrait vérifier que cette erreur est d’autant plus importante que le pas d’intégration est important. 2.2 Exemples de distributions de lois connues Parfois certaines distributions ressemblent à des distributions bien connues : on appelle ces distributions des lois. Ce sont des distributions de probabilités que l’on peut formaliser par une équation et dont les statisticiens ont pu dériver des caractéristiques par le calcul. 2.2.1 Loi Gaussienne La plus connue est la distribution Gaussienne, on dit aussi distribution normale du nom de la loi de probabilité qu’elle suit : la loi normale. Cette loi a deux paramètres : \\(\\mu\\) la moyenne, i.e. la valeur moyenne qu’auront les valeurs tirées de cette distribution \\(\\sigma\\) l’écart type, qui représente leur écartement par rapport à cette moyenne Nous reviendrons plus loin sur ces deux caractéristiques Voici la distribution d’une population dont la variable \\(V1\\) suit une loi normale de moyenne 0 et d’écart-type 1, qu’on note \\(\\mathscr{N}(0,1)\\) xx &lt;- data.frame(value=rnorm(8000)) plot1 &lt;- ggplot(xx)+ geom_density(aes(x = value), color=&quot;#aaaaaa&quot;, fill=&quot;#44DD99&quot; )+ theme_light()+ labs(title = &quot;Loi Normale&quot;, x=&quot;Valeur de la variable V1 &quot;, y=&quot;densité&quot;) plot1 Voici un histogramme de la même population plot1 &lt;- ggplot(xx)+ geom_histogram(aes(x = value),bins = 50, color=&quot;#aaaaaa&quot;, fill=&quot;#44DD99&quot; )+ theme_light()+ labs(title = &quot;Loi Normale&quot;, x=&quot;Valeur de la variable V1&quot;, y=&quot;Effectif&quot;) plot1 2.2.2 Loi uniforme Comme son nom l’indique, la loi uniforme vaut partout la même valeur entre deux bornes \\(a\\) et \\(b\\), autrement dit , la probabilité d’obtenir une certaine valeur \\(v\\in[a;b]\\) est constante. La forme de sa distribution est théoriquement une fonction en créneau, qui vaut 1 partout sur \\([a;b]\\) mais en pratique quand on échantillonne (i.e. génère) des valeurs suivant cette loi, même un grand nombre de fois comme ici, la distribution, qui devrait être plate, est courbée aux extrémités. Cela est dû à la façon dont R estime la densité numériquement, biaisée aux extrémités de l’intervalle 2.2.3 Loi log-normale La loi log-normale est comme son nom l’indique, le résultat d’un logarithme appliqué à une variable suivant une loi normale. Notez comme la distribution est «tirée vers la droite». On parle de «queue de distribution». Cette longue queue (“fat tail” in english) indique une grande inégalité dans la population: quelques individus, peu nombreux mais aux valeurs de variable très élevées, et une vaste majorité d’individus dont la valeur de la variable est faible qui constitue le pic de la distribution. Cette loi modélise par exemple l’effet d’un «grand nombre de petits facteurs considérés comme indépendants». Wikipedia nous apprend qu’elle modélise des phénomènes réels tels que la répartition de 97% des salaires du monde, celle de la la longueur et le poids de spécimen d’animaux, la durée des parties d’échecs, etc. 2.3 Histogramme d’une distribution réelle Les distribution de données empiriques ont rarement des formes aussi régulière et identifiable que celles des lois. Voici par exemple l’histogramme de la hauteur des arbres à Paris, selon les données disponibles sur [https://opendata.paris.fr/] Cet histogramme n’est pas très informatif en l’état : si on le lit naïvement, il semblerait que tous les arbres aient une valeur nulle ou quasi-nulle pour leur hauteur en mètres et la circonférences de leur tronc en centimètres. Pourquoi ? Parce que le logiciel qui trace l’histogramme (R) fait du mieux qu’il peut pour tracer les colonnes de l’histogramme correspondant aux valeurs : il ne doit pas en oublier, la hauteur des colonnes doit correspondre à l’effectif (le nombre) d’individus (ici des arbres) par valeur de la variable l’échelle de l’axes \\(x\\) doit «faire tenir» l’étendue des valeurs (\\(x_{max} - x_{min}\\)),sur une quantité de pixels limitée. Ce qui se produit ici est que certains individus ont des hauteurs ou des circonférences renseignées à des valeurs totalement irréalistes, comme nous l’indique les bornes du dernier décile (i.e. les 10% des valeurs les plus élevées ) : ## 0% 10% 20% 30% 40% 50% 60% 70% 80% 90% 100% ## 0 0 4 5 6 8 10 11 15 16 881818 10% des arbres ont une hauteur comprise entre 12 et 881818 mètres: il y a donc quelques arbres, au moins un, dont la hauteur est clairement defectueuse, on peut supposer qu’il s’agit d’une erreur de saisie ou d’ encodage des données (les plus hauts arbres font autour de 120m). On constate aussi qu’au moins 10% des données ont une hauteur nulle. En affichant l’histogramme de cette variable, R a donc du afficher une colonne aux alentours de la valeur 881818, dont la hauteur est vraisemblablement très faible (1 ou 2 individus), en tout cas si faible qu’on ne la distingue pas : son épaisseur est dans le trait de l’axe des \\(x\\) Nous allons donc filtrer les données, pour ne conserver que les arbres dont la hauteur est comprise entre 1 et 60 mètres, ce qui me semble correct comme intervalle pour des hauteurs d’arbres parisiens, mais qui pourrait être discuté. De même , on écarte les arbres dont le tronc excède 2500 cm : On voit ici comme la représentation graphique des variables nous renseigne à deux niveaux : elle nous indique la présence de valeurs aberrantes lors de son affichage “brut,” et une fois filtrée, elle nous montre comment la population varie dans les valeurs de ses variables. 2.4 Afficher histogrammes et distributions en R 2.4.1 Histogramme d’une variable quantitative la fonction hist affiche un histogramme d’un vecteur numerique: x &lt;- rnorm(2500) #init hist(x) Supposons que nous ayons un fichier au format CSV , voici comment le transformer en dataframe et en tirer un histogramme : mydata &lt;- read.csv(&quot;data/pop_communes.csv&quot;) names(mydata) # pour connaître le nom des colonnes du tableau ## [1] &quot;X&quot; &quot;Code.INSEE&quot; &quot;Commune&quot; ## [4] &quot;Population.totale&quot; &quot;Département&quot; &quot;Région&quot; ## [7] &quot;Densité.d.habitants..hab.km2.&quot; hist(mydata$Population.totale,breaks = 30) # je choisis d&#39;afficher 30 colonnes Voici comment obtenir un histogramme avec le package ggplot2 plt1 &lt;- ggplot(mydata, aes(x=Population.totale))+ geom_histogram(bins=30, fill=&quot;darkcyan&quot;, color=&quot;grey&quot;)+ # exemple de spécification de couleur theme_light() plt1 2.4.2 Histogramme et variable qualitative un histogramme n’a pas de sens pour une variable qualitative. On peut utiliser barplot,⚠ mais ce n’est plus une distribution ! x &lt;- sample(month.name, 2500, replace=T) # month.name est la liste des noms de mois. tx &lt;- table(x) #table de comptage barplot(tx , las=2) Nous allons maintenant voir comment décrire la forme de la répartition de ces valeurs avec des mesures statistiques. La première d’entre elles est la tendance 2.5 La Tendance Les distributions de variables dans les données du monde réel sont rarement constantes ou uniformes. Elles exhibent ce qu’on appelle une tendance, c’est-à-dire une valeur autour de laquelle se retrouvent la majorité des individus. Si la forme de la distribution est suffisamment régulière, cette tendance peut servir de résumé statistique de la variable de la population. Les indicateurs de tendance centrale de la distribution d’une variable sont la moyenne et ses variantes, la médiane et le mode. 2.5.1 Moyenne(s) La moyenne d’une variable \\(x\\) , notée \\(\\bar{x}\\) , s’écrit : \\[ \\bar{x} = \\frac{1}{n}\\sum_{i=0}^{n} x_i \\] 2.5.1.1 Moyenne pondérée Lorsque des poids \\(p_i\\) sont affectés aux individus, la moyenne pondérée s’écrit : \\[ \\bar{x} = \\frac{1}{\\sum_{i=0}^n pi}\\sum_{i=0}^{n} p_i x_i \\] 2.5.1.2 Avantages et inconvénients de la moyenne Avantage : chaque valeur compte dans le calcul. Inconvénients : sensibilité aux valeurs extrêmes pas de signification directe sur les variables quantitatives discrètes (e.g. «2.5 enfants/femme» ) Pour y remédier : exclure les outliers, ou restreindre les valeurs considérées par filtrage utiliser un autre estimateur, par exemple la médiane étudier la distribution des valeurs et en cas de multi-modalités, opérer une classification 2.5.1.3 Autres Moyennes La moyenne géométrique est définie ainsi \\[ \\bar{x}_{geom} = \\sqrt[n]{\\prod _{i=0}^{n} x_i}\\] Elle a l’avantage d’être moins sensible à la présence de valeurs extrêmes que la moyenne algébrique. La moyenne quadratique (par fois appelée RMS pour Root Mean Square in english) est définie ainsi: Elle s’écrit : \\[\\bar{x}_{quad} = \\sqrt{\\frac{1}{n}\\sum _{i=0}^{n} x_i^2} \\] Moyenne glissante Ce n’est pas une moyenne comme les autres, au sens où elle ne résume pas toute une série de valeurs Dans le cas de séries temporelles (i.e. valeurs successives de la même variable), la moyenne glissante est calculée sur une «fenêtre» de \\(n\\) valeurs consécutives. La fenêtre est centrée sur l’instant auquel on calcule la valeur de la moyenne. Par exemple, pour une moyenne glissante sur une fenêtre de taille 10, la valeur en chaque points \\(x_i\\) à la position \\(i\\) dans la série temporelle (on suppose que \\(i&gt;5\\)) vaut la moyenne des de \\(x_i\\) et des 10 valeurs environnantes, 5 en avant , 5 en arrière : \\[\\bar{x} = \\frac{1}{11}\\sum _{j=i-5}^{j=i+5} x_j\\] 2.5.2 Mode Le mode d’une variable est la valeur la plus fréquente ( d’effectif maximum) d’une variable. Si la variable est quantitative et continue, il faut découper l’étendue de la variable (la différence entre la valeur maximum et minimum) en intervalle égaux , puis réaliser une classification des individus dans ces intervalles et un comptage des effectifs de chaque classe. Dans ce cas, le mode est la moyenne des valeurs min et max des bornes de la classe de plus grand effectif. C’est exactement ce que fait un histogramme graphiquement ! Par définition le mode est unique, mais on peut appeler modes les valeurs des autres pics d’une distribution On parle de distribution bi-modale ou tri-modale lorsqu’une distribution présente deux ou trois pics. Les valeurs modales d’une distribution sont les valeurs correspondant à) ces pics. Voici l’histogramme d’une variable dont la distribution est trimodale: 2.5.2.1 Avantages et inconvénients du mode Avantages : Peu sensible aux valeurs extrêmes (moins sensible que la moyenne) il s’interprète facilement : c’est la situation la plus fréquente dans la population Inconvénients : le mode ne dépends pas de toutes les observations : la modification d’une seule valeur n’entraîne pas une modification du mode. Cet inconvénient explique sa robustesse aux valeurs extrêmes 2.5.3 Médiane La médiane est la valeur qui partage une série de valeurs en deux sous-ensembles d’égal effectif Comme en géométrie, la médiane est la valeur de la variable qui est la plus proche de toutes les autres. 2.5.3.1 Étapes de calcul Déterminer la médiane d’un ensemble de valeurs est très simple : Ordonner les \\(n\\) valeurs de \\(V\\) selon un ordre croissant Calculer le rang \\(rg=\\frac{n+1}{2}\\) si \\(n\\) impair, la valeur médiane est \\(V[rg]\\). Si \\(n\\) est pair, la valeur médiane est entre deux valeurs et est égale à la moyenne de \\(V[\\frac{n}{2}]\\) et \\(V[\\frac{n}{2}-1]\\) Dans cet algo , on suppose qu’on compte les cellules d’un tableau à partir de 0, comme en python. En R, où on compte les cellules à partir de 1 , il faudrait ajouter 1 à tous les indices. 2.5.3.2 Avantages et inconvénients de la médiane Avantages : Souvent plus pertinente que la moyenne Peu sensible aux valeurs extrêmes: quelques valeurs très fortes ou très faibles ne modifie pas sa valeur elle s’interprète facilement : comme elle divise en deux la distribution, un individu sur deux a une valeur inférieure (respectivement supérieure) à la médiane. Inconvénient : Comme le mode, la médiane ne dépend pas de toutes les observations : la modification d’une seule valeur n’entraine pas une modification de la médiane. Notons que la robustesse de la médiane est bien utile dans le cas de distribution particulièrement asymétriques, où la moyenne est dégradée par les valeurs extrêmes, à droite (valeurs très élevées) ou à gauche (valeurs très faibles). 2.5.3.3 Moyenne, médiane et forme de la distribution L’écart enter la moyenne et la médiane est un bonne indicateur de l’asymétrie d’une distribution , d’autant plus importante que l’écart entre moyenne et médiane est important. Prenons un exemple avec les revenus mensuels en équivalent temps plein en France en 2016 : le revenu mensuel net moyen est de 2 238 €, le revenu mensuel net médian est de 1 789 € : selon l’[https://www.insee.fr/fr/statistiques/4277680?sommaire=4318291] Supposons qu’on cherche à évaluer si un salaire mensuel net équivalent temps plein de 2000€ est un bon salaire en France, sans définir trop rigoureusement ce qui signifie «bon». 2000€ est inférieur à la moyenne du pays, on peut le considérer comme trop bas pour être «bon». 2000€ est supérieur au salaire médian, il est supérieur à (au moins) la moitié des salaires du pays, et on peut logiquement le considérer comme un «bon» salaire. Cette double interprétation est due au fait que certains salaires très élevés, mais d’effectifs peu nombreux, tirent la distribution du salaire vers la droite, et avec eux, la moyenne. 2.5.4 Quelle mesure de tendance choisir ? Tout dépend de la distribution ! (cette réponse est malheureusement quasiment universelle, d’où l’importance de toujours représenter visuellement les variables pour décider en connaissance de cause ) De manière générale , quand les distributions ne sont pas trop «biscornues» (déformées, asymétriques), la médiane est un bon choix par défaut. Également, observer l’écart numérique entre médiane et moyenne peut être une bonne heuristique pour évaluer rapidement l’asymétrie d’une distribution. Pour les variables quantitatives : Si la distribution n’a pas de longue queue (on dit aussi traîne) , la moyenne et la médiane sont adaptées. Si la distribution exhibe plusieurs modes , il faut réaliser une classification puis calculer médiane et moyenne pour chaque classe. Pour les variables qualitatives : Le mode est privilégié pour les variables nominales c’est-à-dire des variables qualitatives dont les modalités ne sont pas ordonnées, et si on désire considérer «le cas le plus fréquent». 2.5.4.1 Exemple idéal: Distribution unimodale symétrique xx &lt;- data.frame(value=rnorm(9000,mean = 5, sd = 1)) plot1 &lt;- ggplot(xx)+ geom_line(aes(x = value),stat = &quot;density&quot;, color=&quot;#44DD99&quot;, lwd= 1.3)+ geom_vline(xintercept = mean(xx$value), color=&quot;red&quot;)+ geom_vline(xintercept = median(xx$value) + 0.02,color=&quot;blue&quot;)+ annotate(&quot;text&quot;, x=c(4.5,5.5), y=c(0.2,0.2), colour=c(&quot;red&quot;,&quot;blue&quot;),label=c(&quot;mean&quot;, &quot;median&quot;))+ theme_light() plot1 Dans ce cas tout se passe bien : la distribution ne présente qu’un seul pic, et est symétrique : médiane et moyenne sont confondues, ce sont deux bons résumés de la tendance de cette variable, et on peut choisir l’une ou l’autre. 2.5.4.2 Exemple d’un cas délicat : distribution bimodale Parfois une variable peut être bimodale, c’est-à-dire lorsque la population est regroupée autour de deux tendances, dans deux intervalles de valeurs majoritaires. Voici un exemple: Que choisir dans un tel cas : moyenne ou médiane ? … Ni l’une ni l’autre , on voit qu’elles sont peu informatives. Il faudrait dans ce cas établir un seuil et raisonner sur les deux sous-populations. 2.6 La Dispersion La dispersion désigne la façon dont les valeurs des variables des individus d’une population s’écartent de leur tendance. Elle est décrite par la variance, l’ écart-type, et le coefficient de variation (varmean). Voici un exemple de distributions suivant des lois normales de moyenne 0 et dont la dispersion varie. Dans cet exemple, on voit que les distributions dont l’écart-type est important ont beaucoup d’individus éloignées de leur moyenne (qui vaut 0). Cet autre exemple montre un nuage de points formées par deux variables, échantillonnées selon une loi normale \\(\\mathscr{N}(0,1)\\) , de moyenne 0 , et d’écart type 1 (nous allons voir dans un instant ce que signifie l’écart-type) mydataset &lt;- data.frame(X=rnorm(900), Y=rnorm(900)) plot1 &lt;- ggplot(mydataset)+ geom_point(aes(x=X, y=Y), fill=&quot;#44DD99&quot;, color=&quot;#666666&quot;, shape=21)+ coord_equal()+theme_light() plot1 Dans ce dernier exemple, la dispersion quantifie à quel point les individus s’écartent de la tendance centrale de \\(X\\) et de celle de \\(Y\\), ici le point (0,0). 2.6.1 Variance et Écart-type La variance et l’écart-type rendent compte de la dispersion de la variable autour de sa moyenne. On peut les interpréter comme une «quantité moyenne d’écart» entre les valeurs des individus et la valeur moyenne. Mathématiquement, la variance est définie comme la somme des écarts carrés à la moyenne rapporté à l’effectif \\[ var(X)= \\frac{1}{n}\\sum_{i=1}^{n}(x_i -\\bar{x})^2\\] Avec : \\(X\\) une variable \\(x_i\\) les valeurs de la variables pour les individus de la population \\(\\bar{x}\\) la moyenne de \\(X\\) \\(n\\) l’effectif de la population L’écart-type (standard deviation in english) est la racine carrée de la variance: \\[\\sigma_X=\\sqrt{var(X)}\\] Conséquences de ces définitions : si \\(var(X) = 0\\) ou \\(\\sigma_X = 0\\) , alors \\(X\\) est constante (et vaut sa moyenne évidemment) Ces deux mesures sensibles aux valeurs extrêmes et toujours positives. Un écart-type ou un variance faible indique que les valeurs sont réparties de façon homogène autour de la moyenne. Du fait de la racine carrée, l’écart-type permet d’exprimer la dispersion dans les mêmes unités que celles des données, ce que ne permet pas la variance. 2.6.1.1 Variance et écart type avec R Les fonctions pour calculer la variance et l’écart-type d’une variable sont var et sd : xx &lt;- iris$Sepal.Width var(xx) ## [1] 0.1899794 sd(xx) ## [1] 0.4358663 2.6.1.2 Si la distribution est proche de la Gaussienne ⚠ Variance et écart type n’ont d’intérêt que pour qualifier des distributions unimodales, et (à peu près) symétriques , c’est-à-dire d’une allure proche de la distributions Gaussienne. Si c’est le cas , alors la valeur de l’écart-type nous rend bien des services , puisque qu’il nous donne la largeur de la tranche autour de la moyenne qui prend un certaine proportion de la population, en rouge et bleu dans l’image ci-dessous: L’intervalle \\([-\\sigma;\\sigma]\\) contient environ \\(\\frac{2}{3}\\) de l’effectif L’intervalle \\([-2\\sigma;2\\sigma]\\) contient environ 95% de l’effectif Cette façon de «couper» la population en tranches d’une certaine «épaisseur» (i.e. part de la population) est à la base de la mesure suivante : les quantiles 2.6.2 Quantiles Nous avons vu que la médiane d’une variable sépare une population en deux classes d’égal effectif selon la valeur de cette variable (quantitative). Les quantiles séparent une population en \\(n\\) classes d’égal effectif. En pratique, toutes les valeurs de \\(n\\in\\mathbb{N}\\) ne sont pas utilisées. Les cas les plus courants sont : \\(n=4\\) : les quartiles, qui contiennent 25% de la population \\(n=10\\) : les déciles, qui en contiennent 10% plus rarement \\(n=100\\) : les centiles qui en contiennent 1% 2.6.2.1 Quartiles Les quartiles d’une variable \\(X\\) sont trois valeurs, \\(Q_1,Q_2,Q_3\\) qui séparent la population en quatre classes d’égal effectif selon leur valeur de la variable : 25% des valeurs de \\(X\\) sont strictement inférieures à \\(Q_1\\) 50% des valeurs de \\(X\\) sont strictement inférieures à \\(Q_2\\) (médiane) 75% des valeurs de \\(X\\) sont strictement inférieures à \\(Q_3\\) 2.6.2.2 Déciles Les déciles sont les 9 quantiles \\(Q_1,Q_2,\\dots,Q_9\\) qui séparent une population selon la valeur d’une variable quantitative en 10 classes d’égal effectif. 2.6.2.3 Écarts inter-quartiles et inter-déciles Les quantiles permettent de mesurer la dispersion d’une distribution : l’Écart inter-quartile: \\(Q_3-Q_1\\) , nous donne l’étendue de 50% des valeurs de la population les plus proches de la médiane, l’Écart inter-déciile: \\(Q_9-Q_1\\) , nous donne l’étendue de 80% des valeurs de la population les plus proches de la médiane 2.6.2.4 Avantages et inconvénient des quantiles Avantages : les quantiles sont peu sensibles aux distributions aplaties et aux valeurs extrêmes, (puisque seuls les quantiles des extrémités de l’étendue de la variable sont concernés, et ne perturbent pas les autres) L’écart inter-quantile est plus robuste que l’écart-type aux valeurs extrêmes (pour cette même raison: les valeurs extrêmes de la variables ne sont pas prises en compte dans son calcul) Inconvénients : leur calcul peut s’avérer délicat pour les variables quantitatives discrètes les écarts inter-quantiles négligent l’influence des valeurs extrêmes sur la distribution, entraînant un risque de “passer à côté” d’individus ou évènements critiques. Les écarts inter-quantiles sont partiaux : l’écart inter-quartile néglige 50% de la population ! 2.6.3 Les boîtes à moustaches (boxplots) avec R Les boxplots , ou boîtes à moustaches sont la représentation courante de la dispersion d’une variable à l’aide de quartiles. Voici comment obtenir un boxplot avec R . On prend pour cet exemple le dataset iris, toujours disponible dans une session R. boxplot(iris$Sepal.Width ~ iris$Species) L’interprétation des formes des boîtes est la suivante: La marque centrale de la boîte est la médiane Les bords de la boîte sont les quartiles \\(Q_1\\) et \\(Q_3\\) Les extrémités des moustaches vont jusqu’à la plus grande (resp. la plus petite ) valeur inférieure (resp. supérieure) à 1.5 fois l’écart interquartile Les valeurs qui dépassent les moustaches sont affichées sous formes de points Voici un exemple de boxplots obtenus avec le package ggplot2 plot1 &lt;- ggplot(iris)+ geom_boxplot(aes(y=Sepal.Width,x= Species) ) + coord_flip() plot1 On peut voir un boxplot comme un résumé de distribution “vue du dessus,” comme dans l’exemple ci-dessous où sont alignés (à quelques pixels près) un histogramme et un boxplot de la même variable: 2.6.4 Le coefficient de variation Le coefficient de variation (\\(CV\\)) est une autre mesure de dispersion. C’est le ratio entre l’écart-type \\(\\sigma_x\\) et la moyenne \\(\\bar{x}\\) d’une variable quantitative \\(X\\). \\[\\displaystyle CV(X)=\\frac{\\sigma_x}{\\bar{x}}\\] Plus il est important , plus la dispersion est grande. Plus il est proche de 0, plus les données sont homogènes. Il souffre des mêmes inconvénients que la moyenne et l’écart-type : il est sensible aux valeurs extrêmes. Comme l’écart type est une sorte de “quantité d’écart à la moyenne,” il est parfois difficile d’interpréter directement sa valeur numérique, qui dépend des unités de la variables. Ici pour le \\(CV\\) le fait de diviser par la moyenne normalise en quelque sorte cette mesure et permet d’avoir une mesure un peu plus absolue de la dispersion de la variable, quelle que soit son unité et son intervalle de valeurs. Cela permet par exemple de comparer la dispersion de variables quantitatives entre elles ! 2.6.5 Comparer les dispersions de deux distributions. Interpréter la dispersion d’une seule variable est un exercice un peu vain: tenter de décider si la dispersion des valeurs d’une variable d’une population autour de sa moyenne est «notable» n’a pas beaucoup de sens si nous n’avons pas une valeur de référence qui nous permette de qualifier la dispersion calculée par comparaison. Or, il n’existe pas de telle valeur de référence. Nous ne pouvons pas donner de règle générale de la forme «si l’écart type \\(\\sigma_{tutu}&gt;5\\) , vous avez un problème !» En revanche il est beaucoup plus aisé et pertinent de comparer la dispersion d’une variable avec celle d’une autre. Prenons un exemple : Deux communes, A et B, versent des aides aux associations locales. moyenne écart-type Commune A 390€ \\(\\sigma_A = 30\\) Commune B 152€ \\(\\sigma_B = 8\\) Pour quelle commune les aides sont les plus homogènes? Le calcul du coefficient de variation nous donne la réponse : \\(CV_A=\\frac{\\sigma_A}{\\bar{x_A}}=0.07692\\) \\(CV_B=\\frac{\\sigma_B}{\\bar{x_B}}=0.052631\\) La commune A dispense des aides moins homogènes que la commune B. Ce n’est pas le cas dans cet exemple, mais on pourrait tout aussi bien comparer des distribution de variables qui n’ont rien à voir, exprimées dans des unités différentes. 2.6.5.1 Comparaison visuelle de deux distributions Si nous avions des données réelles, nous aurions pu (et même dû, puisque c’est la première chose à faire) répondre également par comparaison visuelle des distributions de la variable. Pour avoir des données à tracer, j’ai échantillonné 1000 individus (c’est beaucoup pour un nombre d’association, mais ça permet d’avoir des distributions graphiquement plus lisses) selon des lois normales aux caractéristiques (moyenne et écart type) des communes. Voici ce que donne une visualisation des deux distributions: A &lt;- rnorm(n = 1000, mean = 390, sd = 30) B &lt;- rnorm(n = 1000, mean = 152, sd = 8) par(mfrow=c(1, 2)) #2 graphes en colonnes hist(A, probability = T) lines(density(A), col=&quot;red&quot;) hist(B, probability = T) lines(density(B), col=&quot;red&quot;) C’est paradoxal : les distributions ont la même allure, alors que leur coefficient de variations sont différents. Pourquoi? Parce que pour comparer visuellement la dispersion de deux variables, il faut une échelle commune , pour que les écarts en x et en y soit comparables. Cette remarque est valable que les deux distributions soient représentées sur des graphiques juxtaposés ou non. Recommencçons l’affichage en fixant l’échelle pour les deux distributions: A &lt;- rnorm(n = 1000, mean = 390, sd = 30) B &lt;- rnorm(n = 1000, mean = 152, sd = 8) par(mfrow=c(1, 2)) hist(A, probability = T, xlim = c(50,600), ylim = c(0,0.05)) lines(density(A), col=&quot;red&quot;) hist(B, probability = T,xlim = c(50,600), ylim = c(0,0.05)) lines(density(B), col=&quot;red&quot;) On retrouve alors que la distribution de la commune B est beaucoup plus piquée que la distribution de la commune A : les valeurs sont resserrées autour de la moyenne et s’en écartent peu, en tout cas beaucoup moins que dans la distribution de la commune A. Les aides de la commune B sont donc plus homogènes. 2.7 La Forme La forme d’une distribution est décrite par deux caractéristiques : sa symétrie (ou son asymétrie) et son applatissement. 2.7.1 Asymétrie L’asymétrie d’une distribution est positive si les valeurs les plus fréquentes (i.e. celles du pic) sont à gauche et la queue de distribution (due à quelques valeurs très élevées surreprésentées) est à droite . L’asymétrie est négative si les valeurs les plus fréquentes (i.e. celles du pic) sont à droite et la queue de distribution (due à des valeurs très faibles surreprésentées) est à gauche 2.7.1.1 les Coefficients d’asymétrie de Pearson Il existe deux mesures simples pour estimer l’asymétrie (skewness in english) d’une distribution d’une variable : \\(\\displaystyle C_1 = \\frac{\\bar{x} - mode(X)}{\\sigma_x}\\) et sa version “normalisée” \\(\\displaystyle C_2 = \\frac{3(\\bar{x} - mediane(X))}{\\sigma_x}\\) L’interprétation de ces coefficients est directe si le coefficient est nul, la distribution est symétrique si le coefficient est négatif, la distribution est déformée à gauche de la médiane (sur-représentation de valeurs faibles, à gauche) si le coefficient est positif, la distribution est déformée à droite de la médiane (sur-représentation de valeurs fortes, à droite) 2.7.1.2 Le coefficient d’asymétrie de Fischer Ce coefficient est défini comme le moment d’ordre 3 de la variable \\(X\\) centrée réduite: \\(\\displaystyle skewness&#39;=\\mathbb{E}\\bigg[\\bigg(\\frac{X-\\mu}{\\sigma}\\bigg)^3\\bigg]=\\frac{\\sum_{i=0}^{n} (x_i - \\mu)^3}{n\\sigma^3}\\) avec \\(X\\) une variable de moyenne \\(\\mu\\) et d’écart-type \\(\\sigma\\). Au passage : Centrer une variable, c’est lui soustraire sa moyenne. Réduire une variable, c’est la diviser par son écart-type. Vous connaissez déjà un «moment» , le moment d’ordre 2 : c’est la variance . 2.7.1.3 Calculer le coefficient d’asymétrie avec R Nous utilisons la fonction skewness() du package moments et library(moments) skewness(iris$Sepal.Length) ## [1] 0.3117531 2.7.2 L’Aplatissement (kurtosis) L’ aplatissement d’une distribution , aussi appelée kurtosis quantifie la déviation de la forme de la distribution par rapport à une distribution normale. une courbe de distribution piquée indique peu de variations dans les valeurs, une distribution relativement homogène, avec beaucoup de valeurs égales ou proches de la moyenne. La courbe aplatie suggère des variations importantes, une distribution relativement hétérogène, avec beaucoup de valeurs éloignées de la moyenne. 2.7.2.1 Coefficient d’aplatissement (ou kurtosis) le kurtosis d’une variable s’écrit : \\[K=\\frac{\\sum_{i=1}^{n}(x_i -\\bar{x})^4}{n\\sigma^4}\\] Si la distribution est normale , \\(K= 3\\) Si \\(K&gt;3\\), la distribution est plus applatie Si \\(K&lt;3\\), la distribution est moins applatie On normalise parfois en considérant \\(K&#39;=K-3\\) (qui mesure donc l’excès d’applatissement) Au passage : on reconnait le moment statistiques d’ordre 4 dans l’équation de la kurtosis il s’agit de la version de Pearson cette mesure ne doit pas être confondue avec la dispersion. En pratique elle traduit plutôt l’existence d’outliers qui “étirent” la courbe de la distribution au delà d’un ou deux écarts-types , à droite et à gauche de la moyenne. 2.7.2.2 Exemple de distribution à écart-type faible, mais à kurtosis important Il est tout à fait normal d’amalgamer dispersion et kurtosis en première approximation, puisqu’une distribution symétrique à fort écart-type sera plus aplatie qu’une distribution à symétrique à faible écart-type. Il existe néanmoins des distributions à kurtosis élevé, sans pour autant que leur variance (ou écart-type ) le soit. Par exemple ,la distribution de Laplace : 2.7.2.3 Calculer le kurtosis avec R Nous utilisons la fonction kurtosis() du package moments et library(moments) kurtosis(iris$Sepal.Length) ## [1] 2.426432 2.7.3 Transformations des données les distributions dont les formes sont particulièrement irrégulières sont délicates à appréhender (et représenter). Parfois, il est possible d’améliorer les choses en utilisant des transformations mathématiques. \\(x \\mapsto log(x)\\) pour une distribution asymétrique à droite ou \\(x \\mapsto \\sqrt x\\) si moins asymétrique (le logarithme «aplatit» les pics dus aux choses exponentielles) \\(x \\mapsto x^2\\) pour une distribution asymétrique à gauche ou \\(x \\mapsto x^3\\) si très asymétrique . (Élever des choses au carré amplifie les petites valeurs)) Il faut toujours vérifier l’allure de la distribution transformée ! Une autre transformation courante est de centrer et réduire des variables. Centrer une variable signifie lui soustraire sa moyenne. Réduire une variable signifie la diviser par son écart-type. Une variable centrée réduite est alors exprimée en «écarts-types à la moyenne» Cela permet de repérer les valeurs extrêmes (\\(&lt;2\\sigma\\) ou \\(&gt;2\\sigma\\)) , du moins si la distribution n’est pas trop irrégulière. C’est utile pour comparer des individus selon un grand nombre de variables Cela permet aussi de comparer des variables définies sur des intervalles de valeurs très différentes 2.7.4 Fat-tail distributions : un exemple Les distributions très asymétriques et très étendues sont délicates à résumer. Les indicateurs traditionnels sont plus efficaces lorsque la variabilité des valeurs est moindre, et leur distribution plus symétrique. e.g. Considérer la population moyenne des villes de France a-t’elle du sens ? 2.7.4.1 Distribution rang-taille des villes de france Pour mieux voir la distribution et les écarts, on trace la taille des villes en fonction de leur rang Lorsqu’on rencontre des distributions aussi piquée, on peut chercher à appliquer une transformation monotone, bijective et inversible qui “aplatisse” la distribution. Cette transformation a pour objectif de réduire les écarts entre les valeurs resserrer la visualisation sur l’essentiel des valeurs Cela aura aussi pour conséquence de mesurer façon plus robuste la tendance, dispersion et forme de la distribution (puisqu’elle sera moins éparpillée) Ici, on choisit le logarithme décimal, qu’on va appliquer sur les ordonnées du graphique grâce à la fonction scale_y_log10() Cette transformation fait apparaître une régularité “linéaire” qu’il sera facile d’ajuster par une régression linéaire. Cette représentation (dire “rang-taille”) et l’ajustement d’un modèle géométrique entre rang et taille, est très utilisée en géographie, et beaucoup de propriétés du système de villes (plus de détails à ce sujet : [https://www.hypergeo.eu/spip.php?article657]) dont on trace ainsi le profil s’y retrouvent : “âge” du système, déviation de certaines villes par rapport à la droite de la loi, longueur de la traîne formée par les petites villes, hiérarchisation du système, macrocéphalie etc… "],["carto-discretis.html", "Chapitre 3 Classification et Échelle de couleurs pour la cartographie 3.1 Méthodes usuelles de discrétisation 3.2 Les données 3.3 Résultats des méthodes de classification 3.4 Quelle méthode de classification choisir ?", " Chapitre 3 Classification et Échelle de couleurs pour la cartographie Contexte : on veut faire une carte choroplèthe d’une variable sur une couche vecteur. Pour une variable quantitative continue qu’on souhaite représenter par une couleur, il n’est pas toujours possible d’avoir la possibilité d’afficher en légende un gradient couleur (d’ailleurs pas toujours lisible). Il faut (souvent) classer les valeurs en catégories, et les logiciels proposent diverses méthodes de classifications (on dit aussi discrétisation) où le nombre \\(k\\) de classes de valeurs (et donc de couleurs) et les méthodes de classification pour fixer leurs bornes varient. En général on propose 5,7 ou 9 classes : pour \\(k&lt;5\\) trop peu de détails pour \\(k&gt;9\\) difficile de distinguer les classes proches Il existe également une règle purement indicative : l’indice de Huntsberger, qui fixe le nombre de casse “optimum” : \\[k = 1 + 3.3 log_{10}(N)\\] Avec \\(N\\) , le nombre d’observations et \\(k\\) le nombre de classe. Priorité au cartographe: C’est le cartographe qui décide en dernière instance du nombre de classes et de leurs bornes. Tout dépend de la question (ou des questions) que le cartographe souhaite aborder avec sa carte. Les valeurs de bornes calculées par des méthodes générales peuvent ne pas être pertinentes. Par exemple , si je devais cartographier des vitesses d’automobilistes sur un réseau routier , je choisirais certainement des bornes égales aux limitations de vitesses : 30, 50 , 90 , 110 et 130, au lieu des bornes proposées par la méthodes des ruptures naturelles ! Tout est question d’intelligibilité pour les gens qui liront la carte. 3.1 Méthodes usuelles de discrétisation Voici les méthodes de classification que propose QGis : Ruptures Naturelles (Jenks) : Minimisation des variances intra-classe et maximisation des variances inter-classe. Effectifs égaux (c’est-à-dire des quantiles cf 2.6.2 ) Intervalles égaux Écart-type : intervalles de 1 ou 0.5 \\(\\sigma\\) Jolies ruptures : intervalle égaux “décalés” pour faire joli : nombre ronds, puissances de 10, … Nous allons voir l’effet de ces méthodes sur une carte choroplèthe de la surface des quartiers de Paris. 3.2 Les données 3.2.1 Géométrie des quartiers de Paris Pour manipuler et afficher des objets spatiaux avec R, on utilise le package sf. Les données viennent du site opendata.paris.fr, et sont disponibles à l’adresse : [https://opendata.paris.fr/explore/dataset/quartier_paris/information/] library(sf) quartiers &lt;- st_read(&quot;data/quartier_paris.shp&quot;) plot(quartiers$geometry) 3.2.2 Distribution des surfaces des quartiers hist(quartiers$surface,breaks = 20) On voit que la distribution est très asymétrique : l’essentiel des quartiers sont de surface faible, et quelques quartiers sont très grands. On peut en déduire qu’une classification par intervalle égaux produira des classes de surface avec très peu d’individus (valeurs élevées) et des classes avec beaucoup d’individus (valeurs faibles). 3.3 Résultats des méthodes de classification 3.3.1 Classification par défaut Par defaut la fonction plot du package sf utilise la méthode pretty() avec 10 ruptures (\\(9\\pm1\\) classes) : ce sont des intervalles égaux. par(mar=c(0,0,0,0)) plot(quartiers[&quot;surface&quot;]) À la lecture de cette carte, on retrouve les défauts de la classification par intervalle égaux. On voit que les quartiers du centre, nombreux et petits, sont tous dans la même classe , ici en bleu, et visuellement peu discriminés: les autres couleurs que le bleu sont affectés aux grands quartiers, peu nombreux. 3.3.2 Méthode Jenks à 5 , 7 et 9 classes La méthode de Jenks essaye de “bien classer” la population en minimisant la variance intra-classe et en maximisant la variance inter-classe, à la manière de l’algorithme des k-means. On peut donc s’attendre à une coloration des quartiers qui colle mieux aux surfaces : par(mar=c(0,0,0,0)) plot(quartiers[&quot;surface&quot;], breaks=&quot;jenks&quot;, nbreaks = 5, main = &quot;5 breaks&quot;) plot(quartiers[&quot;surface&quot;], breaks=&quot;jenks&quot;, nbreaks = 7, main=&quot;7 breaks&quot;) plot(quartiers[&quot;surface&quot;], breaks=&quot;jenks&quot;, nbreaks = 9, main= &quot;9 breaks&quot;) On observe sur ces cartes une coloration plus concentrique: les quartiers du centre sont toujours en bleu foncé, mais les quartiers immédiatement voisins sont bien distincts, en violet, ainsi que les quartiers périphériques, en rose, qui sont nettement plus grands. 3.3.3 Effectifs égaux On utilise la fonction quantile avec une séquence de proportions de 25% (argument probs de la fonction), ce qui va nous calculer les quartiles de la surface des quartiers. Ces quartiles sont donnés en argument de la fonction plot du package sf. Cette carte affiche donc les quartier répartis dans 4 classes , chacune contenant 25% des quartier de Paris. Elle est forcément un peu grossière, mais très discriminante : les quartiers du centre ne sont plus d’une couleur homogène, et les frontières entre les classes sont plus accidentées, moins douces que pour Jenks par exemple. Essayons avec 5 classes de 20% de l’effectif (on modifie la séquence de proportions dans la fonction quantile) : Ça n’est guère plus convaincant que la version à 4, mais il fallait essayer ! 3.3.4 Intervalles égaux Comme pour la méthode par défaut (pretty), qui est grosso modo la même chose, on enpeut pas s’attendre, étant donné l’allure asymétrique de la distribution des surfaces, que la méthode des effectifs égaux soit très adaptée. par(mar=c(0,0,0,0)) plot(quartiers[&quot;surface&quot;], breaks=&quot;equal&quot;, nbreaks = 5, main = &quot;5 Intervalles égaux&quot;) plot(quartiers[&quot;surface&quot;], breaks=&quot;equal&quot;, nbreaks = 7, main = &quot;7 Intervalles égaux&quot;) plot(quartiers[&quot;surface&quot;], breaks=&quot;equal&quot;, nbreaks = 9, main = &quot;9 Intervalles égaux&quot;) Quel que soit le nombre de classe, le résultat est décevant : le gros tas de quartiers bleus au centre est indistinct, et seules changent les couleurs des quartiers périphériques. 3.3.5 Écart-types par(mar=c(0,0,0,0)) plot(quartiers[&quot;surface&quot;], breaks=&quot;sd&quot;,main = &quot;Écart-type&quot;) Cette méthode est similaire à la méthode des jolies ruptures sur la variable centrée réduite. Comparons les deux sorties en affichant les cartes côte à côte (attention cela implique de ne plus afficher la légende, je le fais juste pour cette comparaison) : par(mar=c(0,0,0,0),mfrow=c(1, 2)) plot(quartiers[&quot;surface&quot;], breaks=&quot;sd&quot;,main = &quot;Écart-type&quot;, reset = F,key.pos = NULL) quartiers$surfCR &lt;- (quartiers$surface - mean(quartiers$surface)) *(1/sd(quartiers$surface) ) #centrer reduire plot(quartiers[&quot;surfCR&quot;], breaks=&quot;pretty&quot;,main = &quot;Jolies ruptures&quot;, reset=F, key.pos = NULL) Effectivement , c’est très proche ! 3.4 Quelle méthode de classification choisir ? Il n’y a évidemment pas de critère de choix universel et absolu. Il faut tester, bidouiller, tester, jusqu’à converger vers une solution satisfaisante. Si on décide de discrétiser les valeurs soi-même, il y a tout de même quelques principes à respecter : les classes doivent contenir toutes les valeurs, être sans recouvrement, contigües et distinctes. attention aux décimales et aux extrémités ! Il faut matérialiser la structure interne des données, c’est-à-dire les caractéristiques de la forme de sa distribution : seuils naturels, bi-modalités, queus de distribution etc… On peut donc en déduire des principes spécialisés : si la distribution est uniforme ou très plate \\(\\rightarrow\\) Intervalles égaux si la distribution est asymétrique \\(\\rightarrow\\) Effectifs égaux, Jenks , plus rarement progression géométrique pour les phénomènes aux distributions très exponentielles et piquées. si la distribution est symétrique \\(\\rightarrow\\) Écart-type, intervalle égaux Si jamais les méthodes de classifications abordées dans ce chapitre n’étaient pas suffisantes , vous en trouverez d’autres à essayer dans la doc de la fonction classInt du package du même nom. "],["visudistrib.html", "Chapitre 4 Visualiser une distribution avec R", " Chapitre 4 Visualiser une distribution avec R Ce chapitre montre les différentes façons d’afficher une distribution, ainsi qu’un exemple de code R pour l’obtenir pour chacun des types de graphiques suivants . Histogramme Distribution BoxPlot Violin plot Pyramides (histogrammes juxtaposés) Polygones de fréquences Distribution cumulée , Fonction de répartition , CDF Dot strip plot 4.0.1 Histogramme : Code R + ggplot library(palmerpenguins) data(package = &#39;palmerpenguins&#39;) mydata &lt;- penguins histo_mass &lt;- ggplot(mydata)+ geom_histogram(aes(x=body_mass_g), fill=&quot;darkorchid4&quot;, color=&quot;darkgray&quot;, bins=50)+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;Histogram&quot;)+ ylab(&quot;Count&quot;)+theme_light() histo_mass 4.0.2 Distribution/densité : Code R + ggplot ⚠ ce n’est pas exactement une probabilité, mais une densité de probabilité. Pour obtenir la probabilité pour une valeur \\(x\\) , il faut intégrer sur un petit \\(dx\\) autour de \\(x\\) (cf. 2.1). library(palmerpenguins) mydata &lt;- penguins distrib_mass &lt;- ggplot(mydata)+ geom_density(aes(x=body_mass_g), fill=&quot;darkorchid4&quot;, color=&quot;darkgray&quot;)+ geom_vline(aes(xintercept=mean(body_mass_g, na.rm = T)), color=&quot;black&quot;, size=0.4, linetype=&quot;dashed&quot; )+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;Probability density and mean&quot;)+ theme_light() distrib_mass 4.0.3 BoxPlot : Code R + ggplot library(palmerpenguins) mydata &lt;- penguins boxplot_mass &lt;- ggplot(mydata)+ geom_boxplot(aes(x=body_mass_g, y=species, color=species))+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;BoxPlot by Species&quot;)+ theme_light() boxplot_mass 4.0.4 Violin plot : Code R + ggplot Le violin plot est obtenu en faisant une symétrie de la distribution par rapport à l’axe des \\(x\\) d’un affichage classique de distribution. Cette visualisation est utile pour des distributions complexes, e.g. mal résumées par la moyenne et la dispersion, observées pour plusieurs facteurs ou classe (comme ici, avec l’espèce des pingouins). library(palmerpenguins) mydata &lt;- penguins violin_mass &lt;- ggplot(mydata)+ geom_violin(aes(y=body_mass_g, x=species, fill=species), color=&quot;gray&quot;, trim=F)+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;ViolinPlot by Species&quot;)+ theme_light() violin_mass 4.0.5 Violin plot et Boxplot : Code R + ggplot 2 On peut superposer un boxplot de la distribution d’une variable à son violin-plot. library(palmerpenguins) mydata &lt;- penguins violin_mass &lt;- ggplot(mydata)+ geom_violin(aes(y=body_mass_g, x=species, fill=species), color=&quot;lightgray&quot;, trim=F)+ geom_boxplot(aes(y=body_mass_g, x=species, fill=species), color=&quot;black&quot;, fill=&quot;#eeeeee&quot; ,width=0.1)+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;ViolinPlot and Boxplot by Species&quot;)+ theme_light() violin_mass 4.0.6 Pyramides (histogrammes juxtaposés) Lorsqu’une des variables est qualitative à deux modalités, on peut classer les individus selon cette variable en deux sous-populations, et représenter les histogrammes d’une autre variable quantitative pour ces deux sous-populations : library(palmerpenguins) mydata &lt;- penguins pyramide_mass &lt;- ggplot(mydata, aes(fill = sex)) + geom_bar(data = subset(mydata, sex == &quot;female&quot;), stat = &quot;bin&quot;, aes(x=body_mass_g, y=..count..*(-1)), color=&quot;grey&quot;) + geom_bar(data = subset(mydata, sex == &quot;male&quot;), stat = &quot;bin&quot;, aes(x=body_mass_g), color=&quot;grey&quot;) + scale_y_continuous(labels = paste0(as.character(c(seq(20, 0, -10), seq(10, 20, 10))))) + ylab(&quot;count&quot;)+ coord_flip()+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;Pyramid Plot by sex&quot;)+ theme_light() pyramide_mass Une version très connue de cette visualisation est la pyramide des âges, utilisée en démographie. 4.0.7 Polygones de fréquences On peut voir cette visualisation de distribution comme un «histogramme en courbe» Elle est utile pour comparer plusieurs distributions. library(palmerpenguins) mydata &lt;- penguins freqpoly_mass &lt;- ggplot(mydata)+ geom_freqpoly(aes(x=body_mass_g, color=species), bins=50)+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;Frequence Polygons&quot;)+ ylab(&quot;Count&quot;)+theme_light() freqpoly_mass 4.0.8 Distribution cumulée, Fonction de répartition, CDF Les termes “Distribution cumulée,” “Fonction de répartition,” et “CDF” (Cumulative Distribution Function in english) sont en principe synonymes. Cette représentation diffère des représentation habituelle des distributions car on représente sur l’axe des ordonnées non plus la valeur de l’effectif ou la densité, mais la somme cumulée de l’effectif ou de la densité. Cette courbe indique en abscisse la valeur de la variable \\(V\\), et en ordonnée la probabilité empirique d’avoir dans la population, un individu pour lequel \\(V\\leq x\\) i.e. c’est la fonction \\[F_{V}(x)=\\mathbb {P} (V\\leq x)\\] library(palmerpenguins) mydata &lt;- penguins plot_mass &lt;- ggplot(mydata, aes(x= body_mass_g))+ stat_ecdf(color=&quot;darkorchid4&quot;)+ labs(title = &quot;Penguins Body Mass&quot;, subtitle = &quot;Cumulative Distribution Function&quot;)+ ylab(&quot;Probability&quot;)+theme_light() plot_mass Cette représentation permet par exemple de superposer les CDF de sous groupes de la population (ici selon le facteur de l’espèce des penguins) library(palmerpenguins) mydata &lt;- penguins plot_mass &lt;- ggplot(mydata)+ stat_ecdf(aes(x= body_mass_g, color=species))+ labs(title = &quot;Penguins Body Mass by Species&quot;, subtitle = &quot;Cumulative Distribution Function&quot;)+ ylab(&quot;Probability&quot;)+theme_light() plot_mass 4.0.9 Dot Strip Plot Cette visualisation est pratique lorsque les individus sont nombreux. En jouant sur la transparence et le décalage artificiel des points (fonction geom_jitter à la place de geom_point()), on peut faire apparaître les zones de la plages de valeurs de la variable où se retrouvent un nombre notable d’individus. Cela reste à mon avis moins parlant qu’une bonne vieille courbe de densité traditionnelle. library(palmerpenguins) mydata &lt;- penguins plot_dot &lt;- ggplot(mydata)+ geom_jitter(aes(x= bill_depth_mm, y=species, color=species),width=0, height = 0.12, alpha=0.4)+ labs(title = &quot;Penguins Bill Depth by Species&quot;, subtitle = &quot;Dot plot / Strip plot&quot;)+ theme_light() plot_dot "],["bivariee.html", "Chapitre 5 Analyse Bivariée 5.1 Introduction 5.2 Contenu du chapitre 5.3 Régression linéaire 5.4 Corrélation 5.5 Régression linéaire et corrélation avec R 5.6 “trucs” pour linéariser des relations non-linéaires 5.7 Lien entre deux variables qualitatives 5.8 Lien entre une variable qualitative et une variable quantitative. 5.9 Références supplémentaires", " Chapitre 5 Analyse Bivariée 5.1 Introduction L’analyse bivariée , comme son nom l’indique, a pour objectif d’analyser le lien qui peut exister entre deux variables. En guise de rappel sur les types de variables (cf section 1.5) , donnons des exemples: Pour deux variables quantitatives , on pourrait analyser le lien entre : le nombre d’habitants et nombre de lignes de bus des départements français le nombre de lignes de bus en Isère en 1998 et en 2018 Pour deux variables qualitatives, on pourrait analyser le lien entre : la couleur des yeux et le fait de porter des lunettes les catégories de séries télé et la plate-forme où ils sont disponibles Enfin, pour une variable quantitative et une variable qualitative : le lien entre la taille et la couleur des yeux, le lien entre le nombre d’aces d’un joueur et le côté du cours de tennis qu’il occupe 5.1.1 Analyse bivariée, mais sans la localisation L’analyse bivariée que nous allons aborder dans ce chapitre concerne des variables traditionnelles, i.e. pas des variables de localisation . Si elles proviennent de données spatiales, ce sera : soit des individus restreints spatialement (sélection spatiale) soit des variables “géographiques” (e.g. lieu de résidence) renseignées pour les individus La localisation en tant que variable n’interviendra pas dans ce cours. 5.1.2 Ressources pour l’analyse des localisation et des distances Il existe des outils statistiques pour analyser le lien qui existe entre des variables localisées dans l’espace et leur localisation elle-même. Ces techniques ne sont pas au programme de ce cours, je les mentionne pour les curieux. On peut ainsi mesurer l’ auto-corrélation spatiale , qui indique si les valeurs proches sont regroupées ou au contraire disséminées dans l’espace, à l’aide de l’indice de Moran global, et identifier des clusters de valeurs plus fortes ou plus faibles que la normale à l’aide de l’indice de Moran local. On peut également effectuer des régressions qui tiennent compte de la localisation des observations dans l’espace, qu’on appelle GWR pour Geographicaly Weighted Regression (plus de détails sur wikipedia et la fiche de l’INSEE) Enfin , si on désire prendre en compte les distances dans les flux entre unités spatiales (par exemple pour expliquer les déplacements domicile-travail dans une région) on peut se tourner vers les modèles gravitaires Nous donnerons à la fin de ce cours quelques éléments à ce sujet dans la section 6 5.1.3 Corrélation n’implique pas causalité Nous allons voir comment quantifier l’intensité du lien statistique qui peut exister entre deux variables. ⚠ Une liaison, même très forte, entre deux variables, n’indique pas la causalité! ⚠ Cette erreur d’amalgame entre corrélation et causalité est très courante, très tentante, justement à cause du fait que l’amalgame «marche» dans de nombreux cas empiriques. De nombreux contre-exemples sont heureusement disponibles pour finir de se convaincre que la corrélation n’implique pas la causalité: © TylerVigen http://tylervigen.com/spurious-correlations \\(\\leftarrow\\) D’autres exemples sont disponibles à cette adresse. 5.1.4 Diverses formes de dépendances Ce qu’on appelle lien ou liaison ou encore dépendance entre les variables expriment le fait que les valeurs de deux variables n’évoluent pas indépendamment mais au contraire présentent une certaine forme une certaine régularité. Ces «régularités» peuvent être de plusieurs formes, en voici quelques unes : Cette matrice de graphes, de gauche à droite et de haut en bas montre : une dépendance linéaire positive une dépendance linéaire négative une dépendance non-linéaire , peut-être exponentielle une dépendance périodique, sinusoïdale une absence de dépendance, les deux variables sont indépendantes une absence de dépendance, la variable de l’axe des \\(y\\) est constante En pratique les formes sont beaucoup moins régulières que ces exemples très «mathématiques» : les données peuvent être bruitées, incomplètes, contenir des outliers, etc. 5.1.5 Les étapes de l’analyse bivariée On peut résumer la démarche ‘mentale’ à adopter devant un jeu de données par cette séquence: Tracer le nuage de points Existe-t-il une relation ? Est-elle de forme linéaire ? De quel sens ? Si la liaison est de forme linéaire \\(\\rightarrow\\) faire une régression Si la liaison est non linéaire, est-elle monotone ? De forme connue ?\\(\\rightarrow\\) proposer un modèle, i.e. une équation qui décrive la forme de la dépendance entre les deux variables. (5bis)Réaliser un modèle LOESS avec prudence (uniquement descriptif , aucun pouvoir de généralisation) cf le blog de Lise Vaudor [http://perso.ens-lyon.fr/lise.vaudor/regression-loess/] Voici un arbre de décision plus précis : 5.2 Contenu du chapitre Lien entre deux variables quantitatives La section 5.3 traite de la régression linéaire, la section 5.4 traite de la corrélation, la section 5.5 montre comment réaliser ces opérations avec R la section 5.6 montrer quelques astuces pour linéariser des dépendances de formes connues Lien entre deux variables qualitatives la section 5.7 traite de test du \\(\\chi^2\\) Lien entre une variable qualitative et une variable quantitative la section 5.8 montre quelques représentations graphique faisant intervenir une variable qualitative et une variable quantitative. 5.3 Régression linéaire 5.3.1 Avant toute chose Vous devriez commencer à avoir l’habitude de ce mantra, encore plus valable dans le cas d’une analyse bivariée : Toujours en premier: Regarder l’aspect des données avec des graphiques Si le nuage de point n’est pas allongé, si vous “voyez” clairement qu’une droite ne le résulera pas, ou alors très mal, il n’est pas nécessaire d’entreprendre une rregression linéaire, qui sera de toute façon décevante! La regression linéaire ne concerne que les variables quantitatives. 5.3.2 Principe et Vocabulaire 5.3.2.1 Droite de régression Si la forme du nuage de points s’y prête, c’est-à-dire bien allongée, rectiligne, on peut entreprendre une régression linéaire (aussi appelé ajustement linéaire). Cela consiste à trouver la droite qui passe «au mieux» dans le nuage de points de deux variables quantitatives \\(V_1\\) et \\(V_2\\), celle qui résume le nuage de points d’une façon satisfaisante. «au mieux» est ici employé au sens des moindres carrés, c’est-à-dire que parmi toutes les droites qui peuvent passer au milieu du nuage de points, on va choisir celle pour laquelle l’erreur commise est la plus faible. On cherche la droite (en fait on cherche les deux coefficients \\(a\\) et \\(b\\) de son équation \\(y=ax+b\\)) qui minimise la somme des carrés des écarts, d’où le nom d’estimation MCO (Moindres Carrés Ordinaires) ou OLS (Ordinary Least Squares in english). Si le nuage de points n’est pas amorphe, sans forme, et que les deux variables ne sont pas totalement indépendantes, ou constantes, alors cette droite est unique. 5.3.2.2 Modèle linéaire En réalisant une régression linéaire, on cherche à expliquer une variable à partir d’une autre. «Expliquer» signifie ici «avoir un modèle pour calculer une valeur de la variable à partir d’une autre». Dans le cas d’une régression linéaire, le modèle est l’équation de la droite ajustée, de la forme \\(y=ax +b\\), celle qui minimise la somme des écarts au carré. On cherche donc les valeurs de \\(a\\) et de \\(b\\) (les coefficients du modèles linéaire) qui nous permettrait de calculer la valeur d’une variable (la variable dite expliquée) à partir d’une autre (la variable dite explicative ), que l’on peut écrire naïvement de cette façon : \\[Valeurs\\ de\\ la\\ variable\\ expliquée = modèle(Valeurs\\ de\\ la\\ variable\\ explicative)\\] \\[Valeurs\\ prédites\\ de\\ V_2 = modèle(V_1)\\] Ici, notre modèle est linaire, donc: \\[Valeurs\\ prédites\\ de\\ V_2 = a\\times V_1 + b\\] Variable explicative: C’est celle qui est utilisée par le modèle pour calculer les valeurs de la variable expliquée. Dans notre exemple , c’est \\(V_1\\). Par convention , on la met sur l’axe des \\(x\\) dans les graphes. Variable expliquée : C’est la variable pour laquelle le modèle propose des valeurs: dans notre exemple c’est \\(V_2\\). Par convention on la met sur l’axe des \\(y\\) dans les graphes. 5.3.2.3 Estimation, Prédiction, Résidu On peut utiliser le modèle, pour estimer (=calculer, prédire) les valeurs de \\(V_2\\), avec les valeurs de \\(V_1\\) et à l’aide de l’équation de droite. On note souvent les valeurs prédites \\(\\hat{V_2}\\). L’écart entre \\(\\hat{V_2}\\) et \\(V_2\\) est appelé résidu. Ainsi pour toutes les observations du jeu de données, on a : \\[V_2 = prediction(V_1) + residu\\] \\[V_2 = \\hat{V_2} + residu\\] 5.3.2.4 L’erreur commise par le modèle L’erreur commise par la droite, c’est la somme des écarts entre les points du nuage et la droite, les résidus, ces écarts sont élevés au carré avant d’être sommés pour que les erreurs positives et négatives ne se compensent pas. On appelle parfois cette somme la SSE (Sum of Square Erros in english) ou la RSS (Residuals Sum of Squares in english). On peut l’écrire avec des notations différentes, mais la signification est toujours la même : \\[SSE = \\sum_i (prediction_i - observation_i)^2\\] \\[SSE = \\sum_i ( modele(x_i)- x_i)^2\\] \\[SSE = \\sum_i ( ax_i + b - x_i)^2\\] \\[SSE = \\sum_i ( \\hat{y_i}- x_i)^2\\] On peut noter qu’il ne s’agit pas de la distance entre les points et la droite (leur projeté orthogonal), mais bien l’écart en ordonnée qui intervient dans ce calcul. Quasiment toutes les statistiques inférentielles reposent sur ce genre d’équation : on cherche un modèle d’une variable, à partir d’autres variables explicatives, et parmi les modèles possibles, on essaye de réduire l’erreur et de trouver le «bon» modèle: celui qui produise des résidus avec de nombreuses propriétés souhaitables : valeurs faibles, identiquement distribuées, de distributions proches de la gaussienne, sans auto-corrélation temporelle ni spatiale, etc. Nous reviendrons dans la section bonus @ref{residuslm} sur la façon de qualifier les erreurs commises par un modèle linaire avec R. 5.3.3 Interpréter la droite de régression L’objectif de la régression linéaire est trouver le meilleur modèle linéaire entre deux variables. Ce modèle est l’équation d’une droite, qu’on appelle droite de régression et qui permet de visualiser: l’intensité de la dépendance, suivant que les points sont proches de la droite ou non la forme de la dépendance, suivant que le nuage soit bien de forme linéaire le sens de la dépendance : nulle, positive ou négative Dans cet exemple, le nuage de point est assez allongé, on constate une dépendance linaire négative. Les points sont relativement proches de la droite. 5.3.4 Utiliser un modèle linéaire L’équation de la droite est un modèle linéaire de la relation statistique qui lie \\(V_1\\) et \\(V_2\\); Ici le modèle est : \\(\\hat{V_2}=aV_1+b\\) Si la régression linéaire est réussie, alors pour un individu \\(i\\) dont on connait \\(V1_i\\), on infère la valeur \\(V_{2i}\\) par le modèle : \\(\\hat{V_{2i}} = aV_{1i} +b\\) On dit aussi que \\(V_1\\) explique \\(V_2\\) , ou que le modèle prédit \\(V_2\\) à partir de \\(V_1\\) (on note les valeurs prédites \\(\\hat{V_2}\\)). Il est intéressant d’avoir un (bon) modèle de la relation entre \\(V_2\\) et \\(V_1\\), car en l’absence d’observations supplémentaires de \\(V_2\\) , on peut estimer les valeurs qu’elles prendraient si on dispose d’observations supplémentaires de \\(V_1\\). Imaginez que des observations de \\(V_2\\) soient particulièrement coûteuses à recueillir, et les observations de \\(V_1\\) particulièrement peu coûteuses, par exemple la production de salive d’un tigre adulte en colère (\\(V_2\\)) et la largeur de ses empreintes (\\(V_1\\)). En cas de bonne qualité de régressions, le.la zoologiste sera ravi.e de mesurer les largeurs d’empreintes. 5.3.5 Évaluer la qualité d’une régression linaire : le \\(R^2\\) En pratique, pour considérer qu’une régression linéaire est de bonne qualité, i.e. que le modèle linéaire qui lie les deux variables décrit (explique, prédit, ..) bien les données, il faut réunir deux critères : des coefficients avec des p-values associées faibles (e.g. &lt;0.05) qu’on peut grossièrement traduire par “on a peu de chances de se tromper” un \\(R^2\\) élevé, qu’on peut traduire par “le modèle prédit bien les observations” D’autres critères sont donnés en bonus à la section 5.5.3. On commence par le \\(R^2\\). 5.3.5.1 le \\(R^2\\) Le coefficient de détermination linéaire , noté \\(R^2\\) est une valeur qui décrit la qualité de prédiction de la régression, c’est-à-dire à quel point la droite de régression estime correctement les valeurs de la variable expliquée. Il est défini par : \\[R^2 = 1 - \\frac{\\sum_{i=1}^{n} (y_i - \\hat{y})^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] \\(R^2 \\in [0;1]\\) Plus le \\(R^2\\) est proche de 1, meilleure est la qualité. Pour avoir l’intuition de l’interprétation de cette formule, on peut remarquer que la fraction est un ratio entre la somme des résidus au carrés (écart entre valeurs observées \\(y_i\\) et valeur prédite \\(\\hat{y_i}\\)) et la variance (cf. section 2.6.1) de la variable expliquée (\\(y\\)). Le dénominateur de la fraction est constant , il vaut \\(var(y)\\) Alors, intuitivement , pour un mauvais modèle, qui prédit mal les observations de \\(y\\), la somme des résidus au carré sera importante, donc la fraction également, et la valeur de \\(R^2\\) sera petite, proche de 0. De la même manière, pour un bon modèle, qui prédit bien les valeurs de \\(y\\), les résidus seront faibles, donc leur somme quadratique également, ainsi que la valeur de la fraction, ce qui donnera un \\(R^2\\) proche de 1. Mentalement , on peut réécrire la formule ainsi : \\[R^2 \\approx 1- \\frac{résidus^2}{variance\\ de\\ y}\\] \\[R^2 \\approx 1- (erreur\\ commise\\ normalisée)\\] 5.3.5.2 Propriétés et interprétation additionnelles du \\(R^2\\) Le \\(R^2\\) peut s’écrire de plusieurs façons, notamment ainsi : \\[R^2 = \\frac{\\sum_{i=1}^{n} (\\hat{y_i} -\\bar{y} )^2}{\\sum_{i=1}^{n} (y_i - \\bar{y})^2}\\] Cette expression est intéressante pour nous car on reconnait encore une fois une fraction de variances (cf. section 2.6.1) : au dénominateur , c’est la variance de \\(y\\) , la variable expliquée au numérateur , c’est la variance de \\(\\hat{y}\\), la variable prédite par le modèle (les termes en \\(\\frac{1}{n}\\) des variances au numérateur et dénominateur se simplifient ) On peut donc voir le \\(R^2\\) comme un ratio de variances : celles des estimations et celles des observations. En faisant une regression, on cherche à «capturer» la variance de la variable expliquée à l’aide de la variance de la variable estimée par le modèle, d’où une valeur de \\(R^2\\) proche de 1 lorsque la variance des estimations est proche de la variance des observations 5.3.5.3 Exemples avec différentes valeurs de \\(R^2\\) Voici des exemples de régression linéaire de différentes qualités avec la valeurs correspondante de \\(R^2\\), on donne aussi la valeur de la p-value, que nous aborderons juste après. Cette première régression est de bonne qualité: le \\(R^2\\) est proche de 1, la droite décrit bien les données : peu de point s’en écartent significativement. Cette deuxième régression est de moindre qualité. Certes, le nuages de points à une forme suffisamment “allongée” pour qu’on entreprenne une régression linéaire, mais celle-ci est de qualité intermédiaire : beaucoup de points sont éloignés de la droite, le nuage est assez dispersé, ce qui augmente les erreurs et donc amoindrit le score du \\(R^2\\). Avec ce dernier exemple volontairement exagéré, la qualité de la régression est mauvaise. Certes la forme du nuage de points est linéaire, mais la bande est tellement large que les erreurs seront très grandes, même si la droite de régression passe bien au milieu. La variance de la variable 2 est telle qu’elle n’est pas suffisamment “capturée” par la droite. On notera que les p-value sont très faibles, ce qui on va le voir, indique qu’on est à peu près certains de l’équation de la droite, c’est bien celle-ci qui passe “au mieux” dans le nuage de points. Le problème, c’est que ce nuage de points est tout simplement «mal» décrit par une droite, vu sa forme ! 5.3.6 Évaluer la qualité d’une régression linaire : la p-value 5.3.6.1 la p-value et l’hypothèse nulle La p-value est la seconde quantité qui nous renseigne sur la qualité d’une régression linéaire. Le \\(R^2\\) mesurait la qualité de prédiction du modèle linéaire, la p-value est plutôt associée avec la notion de confiance qu’on peut placer dans les coefficients du modèle linéaire que l’on obtient. La façon d’interpréter correctement une p-value fait l’objet de nombreux débats, on en propose une ici, mais il y en a de nombreuses autres. Approximation grossière : la p-value est le pourcentage de chances de se tromper en rejetant l’hypothèse nulle, c’est-à-dire se tromper en considérant que les deux séries ne sont pas indépendantes et qu’il existe une relation entre les deux (ici, linéaire car nous testons un modèle linéaire). Sur la base de cette approximation, on peut d’ores et déjà dire que plus cette p-value est petite , mieux c’est. La p-value est associée à la notion d’hypothèses nulle. Ici , l’hypothèse nulle \\(H_0\\) est “les deux variables sont indépendantes”. rejeter \\(H_0\\) c’est considérer que les deux variables ne sont pas indépendantes (par abus de langage) conserver \\(H_0\\) c’est considérer qu’elles sont indépendantes En statistique, on raisonne avec des hypothèses, qu’on rejette ou qu’on conserve, suivant la valeur de tests statistiques. Par exemple, lorsqu’on teste si deux variables sont linéairement liées, on formule en fait une hypothèse alternative , \\(H_1\\), qui est «les deux variables sont linéairement liées». Quand on rejette \\(H_0\\) et qu’on a formulé une hypothèse alternative , on accepte de facto \\(H_1\\). \\(H_0\\), est l’hypothèse à formuler dans le cas le plus général possible, quand on ne sait rien du tout sur les données. Dans ce cas là, étant donné deux variables, on pose l’hypothèse qu’elles sont indépendantes, tout simplement parce qu’il n’y a aucune raison qu’elles soient liées. 5.3.6.2 Approximation moins grossière à propos de la p-value La p-value peut s’interpréter comme «la probabilité d’avoir un résultat au moins aussi marqué étant donné l’hypothèse nulle» Imaginez que vous ayez un jeu de données \\(D_{ini}\\) constitué de 2 variables quantitatives pour 500 individus. Le nuage de points est suffisamment allongé pour que vous entrepreniez une régression linéaire. Vous obtenez une certaine valeur pour vos coefficients \\(a_{D_ini}\\) et \\(b_{D_ini}\\) de régression linéaire. Vous pouvez même faire une test de corrélation de Pearson, et obtenir une valeur de corrélation élevée, bref , vous rejetez \\(H_0\\) pour \\(D_{ini}\\) Maintenant imaginez qu’on génère une infinité de jeu de données \\(\\Gamma\\) à 2 variables et à 500 individus sachant \\(H_0\\), donc avec des variables indépendantes, des variables pour lesquelles on sait qu’il n’y a aucun lien. Parmi cette infinité de jeu de données, il y en a une proportion qui, par hasard, par chance, présente des données qui ont la même forme que notre jeu de données, ou une forme suffisamment proche, qui donnerait des résultats aussi remarquables c’est à dire égaux ou encore plus marqués , de régression, la même droite, la même corrélation. Trions l’infinité de jeu de données \\(\\Gamma\\) selon ce critère de résultats, d’un côté les jeux de données qui indique les même résultats que notre régression sur \\(D_{ini}\\), qu’on va noter \\(\\Gamma_{corrélés}\\), de l’autre les jeux de données pour lesquels on ne peut pas conclure aux même résultats: il n’y a pas de corrélation entre les deux variables, la forme du nuage de points n’est pas assez linéaire pour qu’un analyste décide d’en faire une régression linéaire. On note ces jeux \\(\\Gamma_{indépendants}\\). Rappelons qu’on a généré cette infinité de jeu de données sous l’hypothèse \\(H_0\\). On s’est donc trompés, pour tous les jeux de données étiquetés \\(\\Gamma_{corrélés}\\), en obtenant des résultats de corrélation et de régression linéaire similaires (ou plus marqués encore) à ceux obtenus sur \\(D_{ini}\\). On ne s’est pas trompés pour tous les jeux de \\(\\Gamma_{indépendants}\\) La p-value c’est la proportion \\[\\frac{\\Gamma_{corrélés}}{\\Gamma}=\\frac{\\Gamma_{corrélés}}{\\Gamma_{indépendants}+\\Gamma_{corrélés}} \\]. C’est la proportion de fois où on s’est trompés en affirmant à propos de \\(\\Gamma_{corrélés}\\) les résultats obtenus sur \\(D_{ini}\\) alors qu’on avait des jeux de données où \\(H_0\\) était vraie par construction. Évidemment cette proportion est très faible pour des cas où le lien entre les deux variables est marquée, mais comme on raisonne à l’infini elle ne sera jamais nulle ! 5.3.6.3 le seuil de significativité Il est d’usage de rejeter \\(H_1\\) , l’hypothèse alternative qui considère que les deux variables ne sont pas indépendantes lorsque la p-value associée aux coefficients de la régression linéaire dépasse un certain seuil. Il n’y a pas de valeur absolue de ce seuil, par une sorte de “tradition” scientifique, le seuil est fixé à 5% (\\(p \\leq 0.05\\)) en sciences expérimentales et en sciences sociales. Dans certains cas il est tout-à-fait acceptable d’augmenter ce score. Cela se comprend d’autant mieux si on interprète la p-value comme une estimation de la plausibilité de la compatibilité des données avec l’hypothèse nulle, et surtout si on se rappelle bien que ce n’est pas parce qu’un test est significatif qu’on a démontré quoique ce soit : on a juste chiffré ce qui sépare notre cas d’étude des cas de même taille dont les valeurs seraient purement dues au hasard. 5.3.6.4 Ce qu’il faut retenir de la p-value «plus elle est petite, mieux c’est» le seuil usuel de significativité est bas (~5%), elle s’interprète comme un pourcentage de chances de se tromper en première approximation il faut plutôt le voir comme la probabilité d’obtenir par chance des résultats de régression linéaire aussi bons, pour une jeu de données de variables indépendantes de même taille. 5.4 Corrélation Dans le cas d’une liaison entre deux variables, on peut calculer l’«intensité» et le sens de ce lien sans nécessairement que la dépendance soit linéaire, ni trouver les coefficients du modèle linéaire : c’est ce que nous décrit la valeur de corrélation. \\(cor(x,y) \\in [-1;1]\\) entre deux variables \\(x\\) et \\(y\\) . +1 : les deux variables croissent ou décroissent conjointement -1 : quand l’une des variables croît, l’autre décroît. 0 : pas de relation entre les deux variables Nous verrons deux corrélations : celle de Pearson et celle de Spearman. La corrélation de Pearson s’utilise lorsque la dépendance est de forme linéaire, allongée, i.e. comme la régression. La corrélation de Spearman est utilisée lorsque la dépendance est monotone, mais pas forcément linéaire. monotone signifie que le sens de variation de la variable est constant: uniquement croissante ou uniquement décroissante. 5.4.1 Calcul direct du coefficient de corrélation (de Pearson) Soient deux variables \\(V_1\\) et \\(V_2\\) Le coefficient de corrélation \\(r\\) de \\(V_1\\) et \\(V_2\\) est la normalisation de la covariance ce \\(V_1\\) et \\(V_2\\) par le produit des écart-types des variables. \\(r= \\frac{cov(V_1,V_2)}{\\sigma_{V_1}\\sigma_{V_2}}\\) La covariance est la moyenne du produit des écarts à la moyenne \\(cov(V_1,V_2)= E[(V_1-E[V_1])(V_2-E[V_2])]\\) Dans cette formule, et de façon courante en statistique, on note \\(E[X]\\) l’espérance de la variable \\(X\\), ce qui revient dans ce cas à sa moyenne. 5.4.1.1 Lien entre corrélation et le \\(R^2\\) d’une régression Si on fait une régression linéaire entre deux variables \\(x\\) et \\(y\\) , \\(y\\) étant la variable expliquée, alors \\(R^2=cor(x,y)^2= r^2\\) et \\(R^2=cor(\\hat{y},y)^2\\) avec \\(\\hat{y}\\) les valeurs de \\(y\\) prédites par le modèle linéaire. La démonstration n’est pas évidente, nous ne la donnerons pas ici, mais l’intuition des conséquences de ces deux égalités est assez directe: Plus les deux variables sont linéairement corrélés (ce qui traduit le coefficient de corrélation de Pearson, avec des valeurs proches de 1 ou de -1 ), meilleure sera la qualité de la régression linéaire (si \\(r\\) proche de -1 ou 1 , alors \\(r^2\\) est proche de 1 ) Dans le même ordre d’idée, plus les deux variables sont corrélées, meilleure sera la régression et plus les valeurs prédites par le modèle seront proches des valeurs observées, et donc également très corrélées aux valeurs observées. 5.4.2 Le coefficient de corrélation de Spearman Quand les deux variables sembles corrélées , de façon monotone mais non linéaire, on utilise le coefficient de corrélation de Spearman, basé sur le rang des individus. \\(\\rho = 1 - \\frac{6\\sum_{i=1}^{n}(rg(X_i)-rg(Y_i))^2 }{n^3 -n}\\) avec : \\(rg(X_i)\\) le rang de \\(X_i\\) (le classement de sa valeur) dans la distribution de \\(X\\) et \\(n\\) le nombre d’individus. Les valeurs et les interprétations du coefficient de Spearman sont les mêmes que pour le coefficient de corrélation de Pearson. 5.4.3 Pearson ou Spearman ? Les coefficients \\(r\\) (Pearson) et \\(\\rho\\) (Spearman) sont deux moyens d’estimer la corrélation: lequel choisir ? si \\(r = \\rho\\): on garde \\(r\\) (plus simple à interpréter) si \\(r &lt; \\rho\\): la relation est non-linéaire : prendre \\(\\rho\\) si \\(r &gt; \\rho\\): il y a un biais, prendre \\(\\rho\\) (plus robuste) … et surtout toujours tracer le nuage de points pour examiner la nature de la relation. 5.4.4 Matrice de corrélations En présence de plusieurs variables, on peut rassembler les valeurs de coefficient de corrélations calculés sur toues les variables deux à deux, dans une matrice de corrélations cor(iris[,1:4]) ## Sepal.Length Sepal.Width Petal.Length Petal.Width ## Sepal.Length 1.0000000 -0.1175698 0.8717538 0.8179411 ## Sepal.Width -0.1175698 1.0000000 -0.4284401 -0.3661259 ## Petal.Length 0.8717538 -0.4284401 1.0000000 0.9628654 ## Petal.Width 0.8179411 -0.3661259 0.9628654 1.0000000 Cette matrice de corrélation est symétrique, et sa diagonale est constituée de 1. 5.4.5 Sensibilité aux valeurs extrêmes La corrélation de Pearson est un indicateur assez sensible aux valeurs extrêmes. Prenons ce dataset particulier, que j’ai choisi pour qu’il ne présente aucune corrélation: X &lt;- c(3,2,3,4,1,2,3,4,5,2,3,4,3) Y &lt;- c(1,2,2,2,3,3,3,3,3,4,4,4,5) plot(X, Y, xlim = c(0,16), ylim= c(0,16)) cor.test(X,Y)$estimate ## cor ## 0 Si on perturbe ce dataset en ajoutant le point \\((x=15, y=15)\\) et qu’on recalcule la corrélation : X &lt;- c(X, 15) Y &lt;- c(Y,15) plot(X, Y, xlim = c(0,16), ylim= c(0,16)) cor.test(X,Y)$estimate ## cor ## 0.9052224 À lui tout seul, ce point transforme la corrélation du jeu de données, le faisant passer d’une corrélation nulle à une corrrélation très forte et positive. Par ses valeurs éloignées des autres, ce point exerce une sorte de bras de levier sur tous les autres, et pourrait faire croire à une tendance linéaire dans le groupe entier. Avant qu’il n’arrive , tous les liens entre les points se compensaient, s’annulaient en quelques sorte. A son arrivée, ce point crée un “petit bout de linéarité”\" avec tous les autres, ces petits bouts vont tous globalement dans le même sens puisqu’il est loin de la masse de point, et l’écart numérique entre ce point et la moyenne des autres fait le reste: cela suffit à passer pour de la corrélation. Cet exemple est volontairement exagéré : il y a seulement 13 individus dans le jeu de données avant la perturbation, ce qui est vraiment peu. Mais c’est l’occasion de rappeler qu’après l’affichage des données brutes, il est utile de s’interroger sur la nécessité de nettoyer/filter les données et des conséquences de ce nettoyage sur le reste des analyses. Voyons ce qui se passerait avec un jeu de données plus conséquent : 1000 points dont les valeurs \\(x\\) et \\(y\\) sont échantillonnées chacune selon une loi normale\\(\\mathscr{N}(0,1)\\) , perturbés par un outlier en \\((x=15,y=15)\\): set.seed(55) X &lt;- rnorm(1000) Y &lt;- rnorm(1000) cor.test(X,Y)$estimate # Pearson sans perturbation ## cor ## -0.0420421 X &lt;- c(X, 15) Y &lt;- c(Y,15) ggplot(data.frame(X,Y), aes(X,Y))+ geom_point(size=.2)+theme_light() cor.test(X,Y)$estimate # Pearson après perturbation ## cor ## 0.15278 cor.test(X,Y, method = &quot;spearman&quot;, exact = TRUE)$estimate #Spearman après perturbation ## rho ## -0.04201555 La perturbation par l’ajout d’un point (un millième de l’effectif seulement) fait passer la corrélation de pearson de -0.04 à 0.15 ! On peut remarquer que le coefficient de Spearman, puisqu’il prend en compte le rang des individus et non leurs valeurs de variables est bien plus robuste à ce type de perturbations. Ajoutons une deuxième perturbation en (16,16) pour finir de s’en convaincre : X &lt;- c(X, 16) Y &lt;- c(Y,16) cor.test(X,Y)$estimate # Pearson après 2 perturbations ## cor ## 0.3011541 cor.test(X,Y, method = &quot;spearman&quot;, exact = TRUE)$estimate #Spearman après 2 perturbations ## rho ## -0.03889886 La corrélation de Pearson a doublé ! On pourrait continuer les expériences en faisant varier le nombre de points, le nombre d’outliers et leur écart avec les valeurs moyennes. On retiendrait que le nombre et la valeur de l’écart des outliers avec le points moyen augmentent l’erreur de corrélation de Pearson (ce qui est assez trivial quand on regarde la formule du coefficient de Pearson) 5.5 Régression linéaire et corrélation avec R 5.5.1 La commande lm La régression linéaire entre deux variables x et y s’obtient avec la fonction lm() et s’écrit lm(y ~ x) pour expliquer la variable y par la variable x. Le premier argument est sous une forme particulière d’expression en R :une formule. Elle s’écrit Variable expliquée ~ Variable explicative dans notre cas, puisque nous faisons une régression univariée , c’est-à-dire qu’on essaye d’expliquer une variable à l’aide d’une seule autre variable. lm(iris$Petal.Length~iris$Petal.Width) ## ## Call: ## lm(formula = iris$Petal.Length ~ iris$Petal.Width) ## ## Coefficients: ## (Intercept) iris$Petal.Width ## 1.084 2.230 5.5.2 Format des résultats La fonction lm renvoie une objet complexe, qui contient beaucoup d’informations. Elles sont heureusement résumées sous une forme lisible en console par la fonction summary() regression &lt;- lm(iris$Petal.Length~iris$Petal.Width) summary(regression) ## ## Call: ## lm(formula = iris$Petal.Length ~ iris$Petal.Width) ## ## Residuals: ## Min 1Q Median 3Q Max ## -1.33542 -0.30347 -0.02955 0.25776 1.39453 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 1.08356 0.07297 14.85 &lt;2e-16 *** ## iris$Petal.Width 2.22994 0.05140 43.39 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.4782 on 148 degrees of freedom ## Multiple R-squared: 0.9271, Adjusted R-squared: 0.9266 ## F-statistic: 1882 on 1 and 148 DF, p-value: &lt; 2.2e-16 On y trouve dans l’ordre: la formule de la régression (section Call) les quartiles des résidus (section Residuals) les coefficients du modèle ajusté et leur p-value associée, ici sur un test de Student, notée Pr(&gt;|t|) (section Coefficients) le \\(R^2\\), on pourra prendre la valeur étiquetée Adjusted R-squared (dernière section) Ce premier niveau de résultats nous suffit dans le cadre de ce cours : nous avons le \\(R^2\\) et la p-value. D’autres critères sont sont disponibles pour renforcer l’analyse, ils sont donnés dans les sections suivantes à titre indicatifs. 5.5.3 Bonus: Critères de significativité du lien linéaire Ces critères portent sur les résidus \\(\\epsilon_i\\), i.e. l’écart entre valeur observée et valeur prédite de la variable \\(y\\) pour l’individu \\(i\\) \\[\\epsilon_i= y_i - \\hat{y_i}\\] Les résidus doivent: être indépendants : covariance nulle ou très faible \\(cov(x_i, \\epsilon_i) = 0\\) être distribués selon un loi normale de moyenne nulle \\(\\epsilon \\sim \\mathscr{N}(0,\\sigma_{\\epsilon})\\) être distribués de façon homogène (homoscédasticité), i.e. de variance constante \\(var(\\epsilon_i)=\\sigma_{\\epsilon}^2\\) , indépendante de l’observation On peut vérifier une partie de ces critères à l’aide des quartiles que donne R dans les résultats de régression. R nous aide avec plusieurs fonctions : 5.5.3.1 Évaluation de l’indépendance des résidus avec R On utilise les graphique de la fonction acf. Si une barre exceptée la première dépasse la ligne en pointillés, on peut remettre en cause l’indépendance des résidus. Ici, c’est le cas. Cela ne veut pas pour autant dire que le modèle linéaire est à mettre à la poubelle, simplement qu’il y a une erreur systématique dans ses résidus, qui ne sont pas assez indépendants. modele1 &lt;- lm(iris$Petal.Length~ iris$Petal.Width) acf(residuals(modele1)) 5.5.3.2 Les 4 graphiques résultats de la fonction lm La fonction lm de R et ses résultats permettent de tracer 4 graphiques pour évaluer certains des critères de significativité. Pour cela il suffit d’appeler la fonction plot sur l’objet résultat de la régression. modele1 &lt;- lm(iris$Petal.Length~ iris$Petal.Width) par(mfrow=c(2,2)) # pour avoir une matrice de graphes plot(modele1) 5.5.3.3 Évaluer l’homogénéité des résidus avec R Le premier graphique sert à vérifier que le nuage de points est homogène c’est-à-dire qu’il n’y a pas de relation non-linéaire entre résidus et valeurs prédites. Une éventuelle relation non-linéaire pourrait se retrouver dans les résidus, et c’est normal. C’est «normal» puisqu’un modèle linéaire ne va capturer que des structures, des tendances linéaires entre deux variables. En cas de structure trop marquée dans les résidus, il faut peut être proposer un modèle non-linéaire, plus délicat à ajuster mais qui capturera plus de variance et laissera des résidus moins marqués. Ici on observe une légère structure parabolique: modele1 &lt;- lm(iris$Petal.Length~ iris$Petal.Width) plot(modele1,1) 5.5.3.4 Évaluer la normalité de la distribution des résidus Le deuxième graphique appelé “Q-Q plot” nous permet de vérifier l’hypothèse de normalité des résidus. Pour cela, on compare donc les valeurs de quantiles des résidus \\(\\epsilon_i\\) avec des quantiles de la distribution de loi Normale. On représente le nuage de points \\((Q_{theo}, Q_{obs})\\) , avec \\(Q_{obs}\\) les valeurs de quantiles des résidus en ordonnée et \\(Q_{theo}\\) les quantiles de la distribution théorique (une Gaussienne donc) en abscisse. Si les résidus suivent une loi normale , les points seront être proches de la bissectrice , puisque leurs valeurs de quantiles seront proches. modele1 &lt;- lm(iris$Petal.Length~ iris$Petal.Width) plot(modele1,2) 5.5.3.5 Évaluer l’homoscédasticité des résidus Le troisième graphique intitulé “Scale location” indique si les résidus sont distribués de façon homogène suivant les valeurs “fittées,” par rapport à une droite. Dans ce cas la droite est plutôt horizontale et les points sont disposés de façon homogène autour. modele1 &lt;- lm(iris$Petal.Length~ iris$Petal.Width) plot(modele1,3) Dans cet exemple: on voit une légère pente mais les points sont distribués de façon relativement homogène autour de la droite 5.5.3.6 TODO residual vs. leverage Pour le moment je ne sais pas comment interpréter le dernier des quatre graphiques. À suivre ! 5.5.4 Tests de corrélation avec R la corrélation peut être obtenue avec la fonction cor(), dont les deux premiers arguments sont des listes (ou vecteurs) de valeurs de même taille. La fonction cor.test est plus complète : comme c’est un test, on a plusieurs indicateurs statistiques sur ce test, notamment la p-value et l’intervalle de confiance, qui est l’intervalle dans lequel on encadre la valeur de la corrélation. On peut spécifier le niveau de confiance qu’on désire obtenir pour cet intervalle avec l’argument conf.level . PLus le niveau de confiance est élevée , plus l’intervalle est grand, plus il est bas, plus l’intervalle est étroit. cor.test(iris$Petal.Length, iris$Petal.Width, conf.level= 0.95) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Petal.Length and iris$Petal.Width ## t = 43.387, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.9490525 0.9729853 ## sample estimates: ## cor ## 0.9628654 cor.test(iris$Petal.Length, iris$Petal.Width, conf.level= 0.75) ## ## Pearson&#39;s product-moment correlation ## ## data: iris$Petal.Length and iris$Petal.Width ## t = 43.387, df = 148, p-value &lt; 2.2e-16 ## alternative hypothesis: true correlation is not equal to 0 ## 75 percent confidence interval: ## 0.9552794 0.9691849 ## sample estimates: ## cor ## 0.9628654 Rappel : la p-value quantifie la significativité du test. Elle s’interprète de la même façon que pour celles associées aux coefficients de la régression linéaire ( cf. section 5.3.6) la fonction cor.test R donne le coefficient de Pearson par defaut, l’argument method de la fonction permet de spécifier deux autres coefficients : Kendall et Spearman. Pour extraire uniquement la valeur des coefficient , il faut utiliser l’attribut estimate de l’objet renvoyé par la fonction. my_test &lt;- cor.test(iris$Sepal.Length, iris$Sepal.Width, method=&quot;spearman&quot;, exact = FALSE) my_test$estimate ## rho ## -0.1667777 l’argument exact doit être précisé en cas de valeurs ex-aequo dans les données. 5.6 “trucs” pour linéariser des relations non-linéaires Les techniques que nous avons abordées sont faites pour les relations linéaires. Tout est plus compliqué lorsque la dépendance entre deux variables est avérée, mais ne suit pas une forme linéaire : elle est mal décrite par le modèle linéaire d’une régression, ou la corrélation de Pearson est peu précise, etc. Quand on a de la chance , on peut néanmoins reconnaître dans les nuages de points quelques formes de dépendances «connues», qui peuvent être issus de lois ou provenir de phénomènes décrits par des lois usuelles. On va donner un exemples de ces relations connues, avec l’effet de leur linéarisation. Les exemples sont volontairement exagérés pour mieux cataloguer les formes. Évidemment en pratique les données sont beaucoup moins propres, et une linéarisation «aveugle» n’améliore pas toujours les choses. 5.6.1 Relation log-linéaire x &lt;- runif(500, min = 1, max=10) y1 &lt;- 1.5*x^4.5 x &lt;- x + rnorm(500) #on ajoute du bruit sur x data2 &lt;- data.frame(x, y1) library(ggplot2) plo2 &lt;- ggplot(data2, aes(x=x, y=y1))+ geom_point(size =1, color = &quot;#0FAF96&quot;, alpha=0.8)+ xlab(&quot; x &quot;)+ ylab(&quot; y &quot;) plo2 Cette forme est observée lorsque les variables sont liées par une relation de type \\(y=ax^b\\). Elle se linéarise par \\(ln(y)=bln(x) + ln(a)\\) x_transform &lt;- log(x) ## Warning in log(x): production de NaN y_transform &lt;- log(y1) data_transform &lt;- data.frame(x_transform, y_transform) library(ggplot2) plo2 &lt;- ggplot(data_transform, aes(x=x_transform, y=y_transform))+ geom_point(size =1, color = &quot;#0FAF96&quot;, alpha=0.8)+ xlab(&quot; x &quot;)+ ylab(&quot; y &quot;) plo2 ## Warning: Removed 5 rows containing missing values (geom_point). Le nuage de points fait un peu peur, mais cette transformation améliorerait la qualité d’une régression linéaire : summary(lm(y1~x))$r.squared ## [1] 0.6690354 summary(lm(y_transform~x_transform))$r.squared ## [1] 0.7227051 5.6.2 Relation géométrique (exponentielle) Relation de type \\(y=e^{ax+b}\\) , qui se se linéarise par \\(ln(y)=ax + ln(b)\\) summary(lm(y2~x))$r.squared ## [1] 0.5477567 summary(lm(y_transform~x_transform))$r.squared ## [1] 0.9905973 On a logiquement un bien meilleur coefficient de détermination 5.6.3 Relation logarithmique Cette forme est celle d’une relation de type \\(y=a*ln(x)+b\\) . Ici , un changement de variable suffit : on pose \\(x&#39; = ln(x)\\) La regression linéaire était déjà de bonne qualité , la transformation l’améliore encore : summary(lm(y3~x))$r.squared ## [1] 0.6861259 summary(lm(y_transform~x_transform))$r.squared ## [1] 0.9188707 5.6.4 Relation logistique Cette forme en ‘S’ est bien connue : c’est une courbe dite “logistique” La fonction qui la décrit est par exemple : \\(y= K \\frac{1}{1+ae^{rx}}\\) avec \\(a&gt;0\\) et \\(r&gt;0\\). Pour estimer \\(a\\) et \\(r\\) , on peut linéariser par l’emploi de la fonction logit : \\(logit(y)=ln(\\frac{y}{1-y})\\) Si on applique cette fonction à \\(\\frac{y}{K}\\), on a : \\[logit\\left(\\frac{1}{1+ae^{-rx}}\\right) = \\ln\\left(\\frac{1}{1+ae^{-rx}} \\times \\frac{1}{1-\\frac{1}{1+ae^{-rx}}}\\right)\\] \\[ = \\ln\\left(\\frac{1}{1+ae^{-rx}} \\times \\frac{1}{ \\frac{1+ae^{-rx} -1}{1+ae^{-rx}}}\\right) \\] \\[ = \\ln\\left(\\frac{1}{1+ae^{-rx}} \\times \\frac{1}{ \\frac{ae^{-rx}}{1+ae^{-rx}}}\\right) \\] \\[ = \\ln\\left(\\frac{1}{1+ae^{-rx}} \\times \\frac{1+ae^{-rx}}{ ae^{-rx}}\\right) \\] \\[ = \\ln\\left(\\frac{1}{ae^{-rx}}\\right) \\] Ensuite, on a assez facilement : \\(\\ln\\left(\\frac{1}{ae^{-rx}}\\right)=rx-\\ln(a)\\) Donc en appliquant la fonction \\(logit\\) à \\(y/K\\), on a pu linéariser la relation entre \\(y\\) et \\(x\\) à la vue du graphique, on s’attend à améliorer la régression linéaire, ce qui est le cas : summary(lm(y4~x))$r.squared ## [1] 0.7774203 summary(lm(y_transform~x_transform))$r.squared ## [1] 0.9895875 On a donc pu estimer par régression (linéaire) les coefficients \\(r\\) et \\(ln(a)\\) ! 5.7 Lien entre deux variables qualitatives 5.7.1 Table de contingence Pour deux variables qualitatives, on ne peut pas produire de nuages de points dans un repère \\((x,y)\\), ni calculer de corrélation, encore moins de droite de régression. La seule chose qu’on puisse faire avec deux variables qualitatives décrivant les individus d’une population , c’est compter le nombre d’individus pour des combinaisons de modalités des deux variables. C’est ce que fait la fonction table() de R, lorsqu’on lui donne deux séries de variables qualitatives (de même taille). ## ## Adelie Chinstrap Gentoo ## female 73 34 58 ## male 73 34 61 On appelle ce genre de tableau croisé un tableau de contingence. On représente les tables de contingence sous la forme de diagrammes dits «en mosaïque» qui aident à décrire les données, et à identifier visuellement les combinaisons de modalités les plus (resp. les moins) représentées dans la population. Ces diagramme s’obtiennent avec la fonction mosaicplot(), les deux variables désirées sont données en premier argument sous la forme d’une formule ~Var1+Var2 Voici un exemple avec des données sur les passagers du Titanic. En colonnes la classe d’appartenance des personnes sur le Titanic : l’équipage , et les voyageurs (en 1ère, 2nde et 3ème classe), en ligne la survie des personnes. En observant ce diagramme , on voit par exemple la proportion de survivants plus importante en première classe par rapport aux autres catégories, ou encore que le taux de survie entre l’équipage et les passagers en troisième classe est quasiment identique. Pour représenter l’imbrication de catégories, il faudrait passer à un type de visualisation plus complexe : les treemaps, dont voici un exemple tiré de la documentation du package treemap. on y représente une valeur numérique “GNI” par pays, eux-mêmes classés par continents La visualisation de l’effectif par modalités croisées peut nous donner des intuitions sur le lien qui pourrait exister entre deux variables qualitatives. Par exemple dans le cas des passagers du Titanic, il semble y avoir un lien entre la classe des passagers et leur survie. Pour confirmer cette intuition on utilise un test statistique appelé test du \\(\\chi^2\\) ( qui se prononce “ki deux” ou “ki carré”) 5.7.2 Test statistique du \\(\\chi^2\\) ou “Khi carré” Le test du \\(\\chi ^2\\) est un test d’indépendance, il mesure l’écart, la différence, entre deux distributions de variables qualitatives Il répond à la question : “Existe-t-il un lien statistique entre deux séries de valeurs qualitatives ?” La réponse qu’il donne est de type OUI/NON , le \\(\\chi^2\\) ne donne pas l’intensité du lien. Comme tout test, il a son hypothèse nulle, qu’on rejette ou non en fonction du résultat. Sans surprise, \\(H_0\\) est «les deux distributions sont indépendantes.» 5.7.2.1 Principe du \\(\\chi^2\\) Le test du \\(\\chi^2\\) que nous abordons ici est le plus simple : il s’agit du test d’indépendance. Son principe est assez simple et se résume en quelques étapes : on génère une population théorique dans laquelle les deux variables sont indépendantes (i.e. qui respecte \\(H_0\\)) on compare cette population théorique à la population observée en passant par la distribution des deux variables dans les deux population. on mesure l’écart entre ces deux distributions. la comparaison de cet écart avec une table de valeurs de référence nous indique si peut rejeter l’hypothèse nulle. 5.7.2.2 Tableau de contingence C’est un tableau à double entrée qui croise deux variables qualitatives. Dans une case on trouve l’effectif (= le nombre) des individus caractérisés par la conjonction des modalités en ligne et en colonnes. Prenons un exemple sur des formes géométriques de couleurs blanches ou noires : \\[\\begin{array}{c|c|c} &amp; blanc &amp; noir \\\\ \\hline carré &amp; 22 &amp; 12 \\\\ \\hline rond &amp; 10 &amp; 30 \\\\ \\hline triangle &amp; 26 &amp; 5 \\\\ \\end{array}\\] Dans R on obtient cette table avec la fonction table(), cf. l’exemple en introduction de cette partie. 5.7.2.3 Calcul de la valeur du \\(\\chi^2\\) Étape 1 On commence par sommer les effectifs de la population observée selon les modalités. (On fait donc la somme en ligne et en colonne) \\[\\begin{array}{c|c|c|c} &amp; blanc &amp; noir &amp; \\texttt{total}\\\\ \\hline carré &amp; 22 &amp; 12 &amp; 34\\\\ \\hline rond &amp; 10 &amp; 30 &amp; 40 \\\\ \\hline triangle &amp; 26 &amp; 5 &amp; 31\\\\ \\hline \\texttt{total} &amp; 58 &amp; 47 &amp; 105 \\end{array}\\] On appelle les sommes en lignes et en colonnes sommes marginales car elles sont mises dans les “marges” du tableau. Étape 2 On divise toutes les valeurs du tableau par la taille de la population. On appelle ces valeurs les fréquences observées. \\[\\begin{array}{c|c|c|c} &amp; blanc &amp; noir &amp; \\texttt{total}\\\\ \\hline carré &amp; 0.20952381&amp; 0.11428571 &amp; 0.3238095\\\\ \\hline rond &amp; 0.09523810 &amp; 0.28571429 &amp; 0.3809524 \\\\ \\hline triangle &amp; 0.24761905 &amp; 0.04761905 &amp; 0.2952381\\\\ \\hline \\texttt{total} &amp; 0.552381 &amp; 0.447619 &amp; 1 \\end{array}\\] On a maintenant un tableau dont les cases contiennent les pourcentages de l’effectif. Ce pourcentage est également la probabilité qu’un individu de la population observée soit caractérisé par les modalités en ligne et en colonne. De la même façon, dans les marges , on a les fréquences marginales (marges divisées par la taille de la pop.), ces fréquences marginales nous donnent la probabilité d’observer un individu de la modalité correspondant à la ligne ou à la colonne considérée. Par exemple dans cette population , j’ai 29.5% de chances de tirer un triangle, et 55% de chances de tirer une pièce blanche. Étape 3 On crée un second tableau, dont chaque case vaut le produit des fréquences marginales correspondantes (en gras ici), calculées sur le tableau des observations. \\[\\begin{array}{c|c|c|c} &amp; blanc &amp; noir &amp; \\texttt{total}\\\\ \\hline carré &amp; 0.1788662 &amp; 0.1449433 &amp; \\textbf{0.3238095}\\\\ \\hline rond &amp; 0.2104309 &amp;0.1705215 &amp; \\textbf{0.3809524} \\\\ \\hline triangle &amp; 0.1630839 &amp; 0.1321542 &amp; \\textbf{0.2952381}\\\\ \\hline \\texttt{total} &amp; \\textbf{0.552381} &amp; \\textbf{0.447619} &amp; 1 \\end{array}\\] Ce tableau est le tableau des fréquences théoriques. Pour savoir pourquoi on les obtient par un produit , voir la section 5.7.5 Étape 4 On multiplie les fréquences théoriques par la taille de la population observée (ici 105) On obtient le tableau des effectifs théoriques. \\[\\begin{array}{c|c|c} &amp; blanc &amp; noir \\\\ \\hline carré &amp; 18.78095 &amp; 15.21905 \\\\ \\hline rond &amp; 22.09524 &amp; 17.90476 \\\\ \\hline triangle &amp; 17.12381 &amp; 13.87619 \\\\ \\end{array}\\] N.B. Il n’est pas nécessaire d’arrondir les effectifs théoriques Étape 5 On calcule la valeur du \\(\\chi^2\\) Soient \\(T^{obs}\\) le tableau des effectifs observés, \\(T^{theo}\\) le tableau des effectifs théoriques \\[\\chi^2 = \\sum_{i,j} \\frac{( T^{obs}_{i,j} - T^{theo}_{i,j})^2}{T^{obs}_{i,j}}\\] C’est la somme, pour chaque case du tableau de contingence (i.e. pour chaque couple de modalités), des écarts carrés entre effectif observé et effectif théorique, divisés par l’effectif théorique. Dans notre exemple avec les formes géométriques \\(\\chi^2 = 26.30\\) 5.7.3 Interprétation de la valeur du \\(\\chi^2\\) Il faut comparer la valeur du \\(\\chi^2\\) calculée avec une valeur critique qu’on trouve dans une table de loi de Student (ou table de loi du \\(\\chi^2\\)). C’est un tableau à double entrée : une valeur de quantile un degré de liberté. La valeur de quantile est le pourcentage d’erreur qu’on s’autorise de faire. On prend souvent 5% . Le degré de liberté est obtenu en calculant la valeur \\(df=(nb\\_lignes - 1)*(nb\\_colonnes -1)\\). Dans notre exemple , le degré de liberté est \\(2\\times1 = 2\\) D’après le tableau de la loi de Student , la valeur critique pour un test avec 5% de chances de se tromper est un degré de liberté de 2 vaut 4.303. Si la valeur calculée du \\(\\chi^2\\) est supérieure à la valeur critique, on rejette \\(H_0\\). Pour notre exemple: On rejette \\(H_0\\) : les deux variables ne sont pas indépendantes, car \\(\\chi ^2 \\approx 26 &gt; 4.303\\) Interprétation: «la forme est liée à la couleur dans cette population, nous pouvons l’affirmer avec un risque d’erreur d’au moins 5%» 5.7.4 Résumé des étapes du \\(\\chi ^2\\) faire un tableau de contingence calculer les sommes marginales et diviser par l’effectif on obtient les fréquences observées et les fréquences marginales on obtient les fréquences théoriques par produit des fréquences marginales calcul du tableau d’effectifs théoriques calcul de la valeur du test comparaison avec les valeurs de la table de Student 5.7.5 Fréquences théoriques et fréquences marginales À partir des fréquences marginales , on obtient pour chaque couple de modalités, la probabilité théorique, celle qui suppose \\(H_0\\), par un simple produit. Cela découle du fait que la probabilité conjointe de deux évènements \\(A\\) et \\(B\\) indépendants est donné par leur produit. \\(P(A \\cap B) = P(A) \\times P(B)\\) pour générer la population théorique, on se place dans le cas où \\(H_0\\) est vrai, les deux variables sont indépendantes, et donc la probabilité théorique d’avoir un individu caractérisé par une certaine combinaison de modalités est obtenu par le produit des fréquences marginales des modalités concernées. Exemple : Si \\(H_0\\) est vraie, la probabilité d’observer un triangle noir est donnée par: \\(P(triangle \\cap noir) = P(triangle) \\times P(noir)\\) \\(P(triangle \\cap noir) =0.447619 \\times 0.2952381 = 0.1321542\\) La probabilité théorique d’observer un triangle noir est de 13,2% 5.8 Lien entre une variable qualitative et une variable quantitative. Il n’existe malheureusement pas de moyen simple de calculer le lien entre une variable qualitative et une variable quantitative. Des possibilités sont offertes par les techniques suivantes, qui ne sont pas au programme de ce cours : corrélation de rang régression logistique analyse de la variance (ANOVA) On peut tout de même qualifier le lien entre une variable quantitative et une variable qualitative. La variable qualitative sert de catégorie, et on fait varier la représentation graphique de la variable quantitative suivant cette catégorie. Deux possibilités s’offrent à nous: plusieurs boîtes à moustaches (une par modalité) la superposition d’ histogrammes ou de densités Nous avons déjà vu les quartiles dans la section 2.6.2 5.8.1 Boxplot par catégories Par exemple, voici comment obtenir la consommation de véhicules par type (dataset mpg de R) p &lt;- ggplot(mpg, aes(class, hwy)) p + geom_boxplot(outlier.alpha = 0.2) +xlab(&quot;type de véhicule (var. qualitative)&quot;) +ylab(&quot;consommation (var. quantitative&quot;)+theme_light() 5.8.2 Superpositions de distributions Nous avons donné dans la section 4 des exemples de visualisation de distributions par modalité d’une variable qualitative. Voici comment réaliser la superposition de distribution en R avec le package ggplot . library(palmerpenguins) myplot &lt;- ggplot(penguins, aes(x=body_mass_g, group=species))+ geom_density(aes(fill=species, color=species), alpha=0.5) + theme_light() myplot L’équivalent en histogrammes, est illisible et à proscrire : library(palmerpenguins) myplot &lt;- ggplot(penguins, aes(x=body_mass_g, group=species))+ geom_histogram(aes(fill=species, color=species), alpha=0.5) + theme_light() myplot ## `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. 5.9 Références supplémentaires Cours complet sur les modèles linéaires : [https://www.math.univ-toulouse.fr/~barthe/M1modlin/poly.pdf] Interprétation des graphique de lm en R : [https://data.library.virginia.edu/diagnostic-plots/] "],["anaspat.html", "Chapitre 6 Analyse spatiale 6.1 Contenu du chapitre 6.2 Semis de points 6.3 Auto-corrélation spatiale : Moran et Geary 6.4 Les flux 6.5 Exemple 6.6 Le modèle gravitaire 6.7 L’étape d’après : modèles dynamiques", " Chapitre 6 Analyse spatiale 6.1 Contenu du chapitre Ce chapitre présente quelques notions d’analyse spatiale proprement dite. Ces notions ne sont pas au programme du module d’analyse statistique L’analyse spatiale s’intéresse aux structures récurrentes de l’espace et à leur influence sur les processus que cet espace supporte. Cette fois-ci les variables qui nous intéressent ont un rapport avec la localisation, la distance et les interactions d’objets spatialisés. Les thèmes abordés dans ce court chapitre sont : Analyse de semis de points Auto-corrélation spatiale globale Notions sur les flux Principe du modèle gravitaire Ces outils, même s’ils sont relativement basiques, sont utiles et puissants, mais ils ne peuvent pas se substituer à une représentation cartographique adéquate d’un phénomène spatialisé, ni sa description exhaustive. Les outils statistiques apportent des informations complémentaires, quantifient des phénomènes (par exemple la spatialisation des résidus d’un modèles nous indique quelles sont les zones qui résistent au pouvoir généralisateur du modèle), et sont destinés à être employés en combinaison avec une méthodologie adéquate, des représentations cartographiques et un discours précis qui lie le tout. Calculer l’auto-corrélation spatiale d’une mesure et s’en tenir là n’aurait pas de sens. (opinion personelle) Une bonne carte surpasse toute analyse statistique spatiale (statique) 6.1.1 L’information géographique L’information géographique est une information localisée en une, deux (la plupart du temps), ou trois dimensions. Ici , au contraire des chapitres précédents, ce qui nous intéresse principalement, c’est la localisation des individus de la populations. L’immense majorité des objets que nous allons aborder sont des objets qui peuvent être ramenés à des objets ponctuels, où à des graphes dont les nœuds sont des objets ponctuels: Objets : e.g. arbres de Paris, stations de Vélib’ Événements : e.g. incendies , accidents Mesures : e.g. altitude, température, pollution Statistiques : e.g. taux de chômage par quartier, votes par communes Interactions : mobilité, flux commerciaux d’après H.Commenges, Données massives spatialisées 6.1.2 Précautions dans l’emploi de projections Nous ne ferons que des calculs en coordonnées projetées. Si on a besoin de calculer des surfaces (e.g. densité) , il faut utiliser une projection équivalente. Si la distance intervient dans l’analyse, il faut utiliser une projection conforme. Cette mise en garde ne s’applique que pour les calculs. une fois qu’ils sont faits, le choix de la projection pour la cartographie est laissé au cartographe. 6.2 Semis de points Les données ponctuelles représentent des unités spatiales localisées à un point et/ou dont on peut négliger l’emprise au sol. On peut étudier la répartition de ces points avec la distance euclidienne dans l’espace qu’on suppose isotrope (invariant par direction) et homogène (invariable par translation). Par exemple avec la localisation des arbres de paris, on observe que les arbres ne sont pas localisés n’importe où : la localisation des arbres à Paris n’est pas isotrope. En pratique, les données spatiales sont rarement réparties de façon isotropes. On fait pourtant souvent l’approximation de faire des calculs avec la distance euclidienne, qui suppose en principe d’avoir un espace isotrope, pour que les distances aient un sens. 6.2.1 Point moyen et point médian Si on définit une distance dans l’espace, par exemple ici la distance euclidienne, on peut définir deux points particuliers à ce semis : Le point moyen est le point dont les coordonnées sont les moyennes des coordonnées des points du semis (en \\(x\\) et en \\(y\\)). Si on le note \\(G\\) , \\(x_G=\\bar{x}\\) et \\(y_G=\\bar{y}\\). Le point médian est le point le plus accessible du semis, celui qui minimise la somme des distances entre lui et les autres points du semis. Il sépare la population en quatre cadrants contenant chacun 25% de l’effectif. Avec la distance euclidienne, il ne peut pas se calculer directement , et s’obtient par approximations successives (il est trouvé par la résolution d’un problème d’optimisation) Ces deux indicateurs n’ont de sens que dans une référentiel euclidien. Dans un espace rectilinéaire, i.e. où on utilise la distance de Manhattan, son calcul est plus direct: 6.2.2 Dispersion et concentration d’un semis La dispersion du nuage de points est approchée par la distance type (euclidienne) : \\[\\sigma_{Dist}= \\sqrt{\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2 + (y_i-\\bar{y})^2} = \\sqrt{\\sigma_x^2 +\\sigma_y^2}\\] Ce n’est pas pour rien qu’on l’appelle la distance type , car c’est l’écart type des distances au point moyen. On reconnaît dans la formule la racine carrée de la somme des variances des coordonnées. Voici un exemple de deux semis de points, répartis dans les IRIS du département de la Seine-Saint-Denis, avec la valeur de leurs distances types. Le nombre de points est identique dans les deux cartes. Dans le semis de gauche, dans la surface de chaque IRIS, on tire 2 points aléatoirement. Dans le semis de droite, on tire aléatoirement le même nombre de points (2 x Nombre d’IRIS), mais dans toute la surface de la Seine-Saint-Denis . ## Distance type à gauche 6422.307 ## Distance type à droite 7001.788 la configuration de gauche est plus agrégée/polarisée , du fait de la différence de surface des IRIS et de leur nombre de points constants. Celle de droite est plus homogène, même si ce terme n’a pas beaucoup de sens puisqu’on n’évalue pas -encore- la concentration des points. La concentration d’un semis est mesurée par l’indice de concentration, qui décrit la régularité des positions des points. Pour l’obtenir : on discrétise l’espace en \\(C\\) mailles de formes régulière( carrée, hexagonale) on calcule le nombre de points théorique par maille (la densité théorique) : \\(D_{théo}= N/C\\) on compte le nombre de points du semis par maille (la densité observée) et on calcule la variance de cette série : \\[var(D_{obs}) = \\frac{1}{C}\\sum_c(nb_c-D_{théo})^2\\] avec: \\(nb_c\\), le nombre de points comptés dans la maille \\(c\\) \\(C\\) le nombre de maille du maillage \\(N\\) le nombre de points du semis. L’indice de concentration est le ratio entre la densité et la variance du nombre de points par maille : \\[IC= var(D_{obs})/D_{théo}\\] si IC = 1 , la distribution est aléatoire si IC &gt; 1 , la distribution est concentrée si IC &lt; 1 , la distribution est homogène Vous avez reconnu la comparaison -ici par un ratio- entre la distribution observée d’une quantité -la densité- et sa distribution théorique. De cette comparaison, on peut quantifier l’écart au modèle nul, ici une répartition aléatoire des objets. comme dans le test du \\(\\chi^2\\) , abordé en section 5.7. Voici quelques exemples de semis avec leur indice de concentration associés: 6.2.3 Le modèle nul pour un semis de points Un processus de spatial de Poisson est un processus spatialement aléatoire. Dans un espace découpé en zones, on estime la probabilité d’avoir un nombre de points)dans une zone \\(z\\) par une distribution de poisson de moyenne \\(\\lambda * surface(zone)\\). 6.3 Auto-corrélation spatiale : Moran et Geary L’auto-corrélation spatiale est une mesure statistique qui répond à la question suivante: « Les valeurs d’entités spatiales proches sont-elles plus similaires que les valeurs d’entités lointaines ?» «Est-ce que les grosses(resp. petites) valeurs sont rapprochées (resp.dispersées) ? » E.g. (au hasard) Les personnes pauvres et les personnes riches sont-elles regroupées au même endroit dans une ville ? Valeurs positives : situations ségrégées Valeurs nulles : répartition aléatoire Valeurs négatives : alternance parfaite entre valeurs 6.3.1 L’indice de Moran L’indice \\(I\\) de Moran utilise un terme \\(\\omega_{ij}\\) positif d’“interaction” pour quantifier la proximité entre deux entités \\(i\\) et \\(j\\). On peut le choisir discret (1 si voisins , 0 sinon) ou continu (inverse de la distance). \\(I=\\frac{n}{\\sum_i \\sum_j \\omega_{ij}} \\frac{\\sum_i \\sum_j \\omega_{ij}(X_i-\\bar{X})(X_j-\\bar{X})}{\\sum_i(X_i-\\bar{X})^2}\\) avec \\(n\\) le nombre d’entités spatiales , \\(\\omega\\) une matrice carrée de poids positifs de dimension \\(n\\), telle que \\(\\omega_{ij}\\) quantifie la proximité, l’influence de \\(j\\) sur \\(i\\) , \\(\\bar{X}\\) la moyenne de la variable \\(X\\) 6.3.2 L’indice de Geary Cet indice, noté \\(C\\), est dit plus “local” que l’indice de Moran \\(C=\\frac{(n-1) \\sum_i \\sum_j \\omega_{ij}(X_i-X_j)^2 }{2W\\sum_i(X_i-\\bar{X})^2}\\) \\(C\\in[0;2]\\) \\(C&lt;1\\) : auto-corrélation négative, \\(C=1\\) : pas d’autocorrelation, \\(C&gt;1\\) : autocorrélation positive avec \\(n\\) le nombre d’entités spatiales, \\(\\omega\\) une matrice carrée de poids positifs de dimension \\(n\\), de somme \\(W\\) et telle que \\(\\omega_{ij}\\) quantifie la proximité, l’influence mutuelle de \\(j\\) sur \\(i\\), \\(\\bar{X}\\) la moyenne de la variable \\(X\\) 6.3.3 Commandes R Les mesures des indices de Moran et Geary s’obtiennent avec les fonctions moran et geary du package spdep 6.3.4 Interprétation de l’indice de Moran les images suivantes montrent comment varient les valeurs de l’indice de Moran en fonction de la localisation d’une quantité dans une grille régulière. 6.4 Les flux 6.4.1 Matrice de flux Pour une partition en zones d’un espace, on appelle flux toute mesure d’intéraction spatiale qui implique des échanges, des déplacements entre deux zones, matériels ou immatériels. On range les mesures qui mesurent ces échanges dans un matrice de flux,une matrice carrée \\(F\\) dont le terme général \\(F_{ij}\\) indique la quantité échangée entre le site \\(i\\) en ligne et le site \\(j\\) en colonne . 6.4.2 Indices \\(V_i= \\sum_i F_{ij} + \\sum_j F_{ij}\\) : Volume = Départs + Arrivées \\(S_i = \\sum_i F_{ij} - \\sum_j F_{ij}\\) : Solde= Départs - Arrivées \\(A_i = \\frac{S_i}{V_i}\\) : Attractivité = ratio des soldes sur les volumes 6.4.3 Cartographie des flux On en peut pas représenter tous les flux entre toutes les zones (lisibilité). On détermine les flux dominants de la façon suivante: \\(F_{ij}\\) est dominant si: \\(i\\) envoie son flux le plus important vers \\(j\\) la somme des arrivées de \\(j\\) est plus importante que la somme des arrivées de \\(i\\) package flows de R : https://cran.r-project.org/web/packages/flows/vignettes/flows.html par les auteurs de cartography 6.5 Exemple ## Warning in wkt(obj): CRS object has no comment 6.5.1 Pour aller plus loin les flux sont modélisés par des graphes, une grande partie de la littérature de l’analyse de réseaux et des notions de théories des graphes peuvent être transposées et utilisées pour l’analyse de flux. E.g. Attributs de noeuds ou d’arcs : centralité de degré (nombres de voisins dans le graphe) centralité d’intermédiarité (fréquence d’apparition du noeud dans les plus courts chemins entre deux noeuds du graphe) Attributs de graphes : communautés (sous graphes denséments connectés) densité de liens … 6.6 Le modèle gravitaire 6.6.1 Fin XIXe : Les lois de Ravenstein (Ravenstein E., 1885 &amp; 1889, “The Laws of Migration,” Journal of Royal Statist. Society, London , d’après Noin D., 1988, Géographie de la Population, Masson, Paris, pp. 273-274, cité par C. Grasland. Le nombre de migrants diminue quand la distance augmente; la plupart ne vont pas très loin ; ceux qui se déplacent sur de grandes distances se dirigent de préférence vers les grands centres commerciaux et industriels. Le processus se fait de la façon suivante : une ville à croissance rapide attire les gens des régions environnantes ; les vides ainsi créés sont comblés par les migrants de districts plus éloignés ; la force d’attraction des grandes villes dynamiques se fait donc sentir de proche en proche en diminuant d’intensité. Le nombre de migrants de la zone d’accueil est donc proportionnel à la population de la zone d’origine et inversement proportionnel à la distance qui les sépare. Chaque courant principal de migration suscite un contre-courant compensatoire. Les citadins ont une mobilité plus faible que les ruraux Les femmes ont une mobilité plus forte que les hommes, au moins à courte distance. L’intensité des migrations augmente avec le développement du commerce, de l’industrie et des transports Les facteurs déterminant la migration sont nombreux mais le plus important est le facteur économique 6.6.2 Le modèle gravitaire Modèle simple, déterministe et symmétrique qui prose de calculer l’intensité de l’interaction entre deux entités spatiales en fonction de leurs masses et de la distance qui les sépare. Plusieurs formulations : \\(Fij = k \\frac{P_i . P_j}{D_{ij}^2}\\) avec \\(P_i\\) la masse (e.g. population, attraction, emmission, etc.) de l’entité spatiale \\(i\\), \\(D_ij\\) une fonction de coût e.g. la distance qui sépare \\(i\\) de \\(j\\) \\(Fij = k \\frac{P_i . P_j}{D_{ij}^\\alpha}\\) avec alpha le frein à la distance (distance decay) , pour modéliser des déplacements plus ou moins faciles 6.6.3 Distance et interaction D’après H.Commenges, cours d’analyse spatiale , Delhi R School Lorsque l’interaction est spatialisée, elle implique nécessairement une distance. L’interaction spatiale considère le déplacement comme un effort. L’interaction spatiale est le rapport dialogique entre mise en contact et friction de l’éloignement. L’interaction spatiale être abordée de deux façons : relations entre lieux ou attraction/influence d’un lieu sur les autres lieux. Dans le premier cas on parlera d’analyse de flux Dans le second cas on parlera d’analyse de position (cf. Accessibilité) 6.6.4 Utilisation du modèle gravitaire Exemple d’une formulation plus générale : \\(F_{ij} = \\alpha_i \\beta_j E_i A_j C_{ij}^{−n}\\) avec \\(\\alpha_i\\) et\\(\\beta_j\\) des facteurs d’équilibrage qui assurent la contrainte aux marges, \\(E_i\\) et \\(A_j\\) : valeurs d’émission et d’attraction de la zone \\(C_{ij}\\) : coût généralisé du trajet de \\(i\\) vers \\(j\\) : temps, prix , inconfort. \\(n\\) : frein à la distance, la valeur varie selon les motifs de déplacement. (plus faible vers le travail que vers des motifs “secondaires” (e.g. courses quotidiennes) On détermine les valeurs des coefficients (\\(\\alpha,\\ \\beta,\\ n\\))inconnus par calibration sur des données (problème d’optimisation classique, qu’on peut résoudre par algorithme génétique, recuit simulé, etc.) 6.6.5 Variantes du modèles gravitaires Nature des masses: population, emplois, émission , attraction , offre, demande Pondérations des masses : facteurs ou exposants Fonction de friction : puissance , exponentielle Contraintes aux marges : aucune , simple, double 6.7 L’étape d’après : modèles dynamiques Les modèles que nous avons jusqu’à présent étaient des modèles statiques: ils décrivaient ou résumaient des données par des objets mathématiques eux aussi “statiques.” Nous avons vu le modèle linéaire , qui résume une nuage de points lorsqu’il a la forme appropriée. Cet objet est fixe, il s’agit des deux coefficient d’une équation de droite \\(y=ax+b\\), et \\(a\\) et \\(b\\) une fois déterminés n’ont pas vocation à évoluer. 6.7.1 Modèles simples : les automates cellulaires Le plus célèbre des modèles simples : le jeu de la vie (Conway) Modèle de ségrégation de Schelling 6.7.2 Principe d’un modèle complexe de mobilité Modèle à 4 étapes Module de génération : quantité de déplacements attirés et émis par zones prédécoupées (caractéristiques de la population et des activités) Module de distribution : répartition géographique des flux (modèle gravitaire) Module de choix modal : par quel modalité s’effectue le transport ? Module d’affectation au réseau Modèles LUTI : Land Use Transport Integration Accessibilité et Usage du sol exercent une influence l’un sur l’autre. Simule les foyers, leurs choix de résidence , d’activité et de transports. "],["references.html", "References", " References "]]
